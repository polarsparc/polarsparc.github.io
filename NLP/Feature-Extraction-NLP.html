<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Feature Extraction for Natural Language Processing">
    <meta name="subject" content="Feature Extraction for Natural Language Processing">
    <meta name="keywords" content="python, nlp, nltk">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Feature Extraction for Natural Language Processing</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="gen-home"></span></td>
        <td valign="bottom"><span id="gen-home-a"><a id="gen-home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="gen-title-div">
      <p>Feature Extraction for Natural Language Processing</p>
    </div>
    <br/>
    <table id="gen-ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">02/05/2023</td>
        </tr>
      </tbody>
    </table>
    <hr class="gen-line-hr"/>
    <br/>
    <div id="gen-step-div">
      <p>Overview</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the article, <a href="https://polarsparc.github.io/NLP/Python-NLTK.html" target="_blank"><span class="bold">Basics of
        Natural Language Processing using NLTK</span></a>, we introduced the basic concepts in Natural Language Processing (or NLP
        for short).</p>
    </div>
    <div id="para-div">
      <p>The realm of NLP deals with the processing and analysis of the unstructured textual data. In order to extract any kind of
        meaningful information and/or insights from the unstructured text data, one will have to employ some kind of Machine Learning
        approach. But, all the Machine Learning approaches <span class="underbold">ONLY</span> deal with features that have numerical
        values.</p>
    </div>
    <div id="para-div">
      <p>For example, in order to perform any kind of document classification on a corpus (a collection of text documents), one will
        have to convert each of the text document in the corpus to a set of numerical features (numerical vector) and them perform
        the classification.</p>
      <p>The process of converting a text document to a vector of numbers is often referred to as <span class="hi-yellow">Feature
        Extraction</span> or <span class="hi-yellow">Word Embedding</span>.</p>
    </div>
    <div id="para-div">
      <p>The following are some of the basic encoding approaches used in NLP, to convert a text document into a numerical vector:</p>
      <ul id="gen-sqr-ul">
        <li><p><span class="bold">One-Hot Encoding</span></p></li>
        <li><p><span class="bold">Bag of Words</span></p></li>
        <li><p><span class="bold">Term Frequency - Inverse Document Frequency</span></p></li>
      </ul>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Installation and Setup</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Please refer to the article <a href="https://polarsparc.github.io/NLP/Python-NLTK.html" target="_blank"><span class="bold">
        Basics of Natural Language Processing using NLTK</span></a> for the environment installation and setup.</p>
    </div>
    <div id="para-div">
      <p>Open a <span class="bold">Terminal</span> window in which we will excute the various commands.</p>
    </div>
    <div id="para-div">
      <p>To launch the Jupyter Notebook, execute the following command in the Terminal:</p>
    </div>
    <div id="gen-cmd-div">
      <p>$ jupyter notebook</p>
    </div>
    <div id="para-div">
      <p>To set the correct path to the nltk data packages, run the following statement in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>nltk.data.path.append("/home/alice/nltk_data")</p>
    </div>
    <div id="para-div">
      <p>To initialize a tiny corpus containing 3 sentences for ease of understanding, run the following statements in the Jupyter
        cell:</p>
    </div>
    <div id="cmd-div">
      <p>['She likes to swim.', 'He loves to read.', 'They like to bike.']</p>
      <p>sentences</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Tiny Corpus" class="gen-img-cls" src="./images/nlp-model-01.png">
        <div class="gen-img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of a word tokenizer, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>from nltk.tokenize import WordPunctTokenizer</p>
      <p>tokenizer = WordPunctTokenizer()</p>
    </div>
    <div id="para-div">
      <p>To normalize the documents in our tiny corpus, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>for i in range(len(sentences)):</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;tokens = tokenizer.tokenize(sentences[i])</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;words = [word.lower() for word in tokens if word.isalpha()]</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;sentences[i] = ' '.join(words)</p>
      <p>sentences</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Normalize Corpus" class="gen-img-cls" src="./images/nlp-model-02.png">
        <div class="gen-img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>One of the <span class="underbold">IMPORTANT</span> steps in the feature extraction process is to extract all the unique
        words from the corpus. This typically implies only root words and getting rid of all the rest including punctuations, stop
        words, etc.</p>
      <p>Given our tiny corpus, we will <span class="underbold">NOT</span> exclude the stop words.</p>
    </div>
    <div id="para-div">
      <p>To extract all the unique words from our tiny corpus, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>all_words = []</p>
      <p>for sentence in sentences:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;tokens = tokenizer.tokenize(sentence)</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;all_words.extend(tokens)</p>
      <p>words = sorted(set(all_words), key=all_words.index)</p>
      <p>words</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Unique Words" class="gen-img-cls" src="./images/nlp-model-03.png">
        <div class="gen-img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>One-Hot Encoding</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the <span class="hi-yellow">One-Hot Encoding</span> model, a text document in the corpus is encoded as a vector of 1's
        for each of the unique words from the corpus that is present in the current document. The words not present in the current
        document are encoded as 0's.</p>
    </div>
    <div id="para-div">
      <p>For example, the third sentence from our tiny corpus is encoded as the following vector:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="One-Hot 3rd Document" class="gen-img-cls" src="./images/nlp-model-04.png">
        <div class="gen-img-cap">Figure.4</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice from Figure.4 above, the One-Hot Encoding model <span class="underbold">DOES NOT</span> maintain the order of the
        words from the document (the third sentence from out tiny corpus).</p>
    </div>
    <div id="para-div">
      <p>For a given corpus, the One-Hot Encoding model outputs a sparse matrix, where the rows represent the documents in the corpus,
        while the columns are the unique words from the corpus.</p>
    </div>
    <div id="para-div">
      <p>To create the One-Hot Encoding sparse matrix for our tiny corpus, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>from sklearn.feature_extraction.text import CountVectorizer</p>
      <p>model = CountVectorizer(binary=True, vocabulary=words)</p>
      <p>matrix = model.fit_transform(sentences).toarray()</p>
      <p>one_hot_words = pd.DataFrame(data=matrix, columns=words)</p>
      <p>one_hot_words</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="One-Hot Encoding Model" class="gen-img-cls" src="./images/nlp-model-05.png">
        <div class="gen-img-cap">Figure.5</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are some of the drawbacks of the One-Hot Encoding model:</p>
      <ul id="gen-sqr-ul">
        <li><p>For a very large corpus, the model performs poorly due to space and time complexity</p></li>
        <li><p>The model does not preserve the order of the words in the document, thereby losing semantic details</p></li>
        <li><p>The model only captures if a word is present or not in a document, but not the word frequency, losing some useful
          information</p></li>
      </ul>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Bag of Words</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="hi-yellow">Bag of Words</span> (<span class="hi-yellow">BoW</span> for short) model improves on the One-Hot
        Encoding model by capturing the frequency of each of the unique words from the corpus in the current document.</p>
    </div>
    <div id="para-div">
      <p>For example, the third sentence from our tiny corpus is encoded as the following vector:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="BoW 3rd Document" class="gen-img-cls" src="./images/nlp-model-06.png">
        <div class="gen-img-cap">Figure.6</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice from Figure.6 above, the BoW model encodes a count of 2 for the word <span class="underbold">to</span> for the third
        sentence from out tiny corpus.</p>
    </div>
    <div id="para-div">
      <p>To create the BoW sparse matrix for our tiny corpus, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>from sklearn.feature_extraction.text import CountVectorizer</p>
      <p>model = CountVectorizer(vocabulary=words)</p>
      <p>matrix = model.fit_transform(sentences).toarray()</p>
      <p>bag_of_words = pd.DataFrame(data=matrix, columns=words)</p>
      <p>bag_of_words</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="BoW Model" class="gen-img-cls" src="./images/nlp-model-07.png">
        <div class="gen-img-cap">Figure.7</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are some of the drawbacks of the BoW model:</p>
      <ul id="gen-sqr-ul">
        <li><p>Frequently occuring words in a document tend to obscure the real intent of the document</p></li>
        <li><p>For a very large corpus, the model performs poorly due to space and time complexity</p></li>
        <li><p>The model does not preserve the order of the words in the document, hence losing contextual details</p></li>
      </ul>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Term Frequency - Inverse Document Frequency</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="hi-yellow">Term Frequency - Inverse Document Frequency</span> (or <span class="hi-yellow">TF-IDF</span> for
        short) model builds on the BoW model. Instead of using a raw word frequency, it uses a statistical measure to determine the
        relevance of each of the unique words from the corpus with respect to a document in the corpus.</p>
    </div>
    <div id="para-div">
      <p>Notice that this model is made up of two very important measures - the <span class="hi-yellow">Term Frequency</span> and the
        <span class="hi-yellow">Inverse Document Frequency</span>.</p>
    </div>
    <div id="para-div">
      <p>The Term Frequency, denoted as $TF(w)$, measures the frequency of the word $f(w)$ with respect to the total number of words
        $F(d)$ in the current document $d$.</p>
      <p>In mathematical terms:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$TF(w) = \Large{\frac{f(w)}{F(d)}}$</p>
    </div>
    <div id="para-div">
      <p>The Inverse Document Frequency, denoted as $IDF(w)$, measures how common or rare the word $w$ is across the documents in the
        corpus.</p>
      <p>In other words, it is the ratio of the total number of documents in the corpus $N(d)$ to the total number of documents with
        the word $w$ in the corpus $N(w)$.</p>
      <p>In mathematical terms:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$IDF(w) = \Large{\frac{N(d)}{N(w)}}$</p>
      <p>If the number of documents in the corpus $N(d)$ is very large, then $IDF(w)$ will be large. To manage this, one typically
        uses a <span class="hi-yellow">Logarithmic</span> scale.</p>
      <p>In other words:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$IDF(w) = log_e($ $\Large{\frac{N(d)}{N(w)}}$ $)$</p>
      <p>If a word $w$ appears in all the documents of the corpus, then $IDF(w) = log_e(1) = 0$</p>
      <p>If we desire to assign a very low score versus a $0$ when a word $w$ is common across all the documents in the corpus, the
        equation for $IDF(w)$ can be modified as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$IDF(w) = log_e($ $1 + \Large{\frac{N(d)}{N(w)}}$ $)$</p>
    </div>
    <div id="para-div">
      <p>Given we have the measures for the Term Frequency $TF(w)$ and the Inverse Document Frequency $IDF(w)$, the definition for
        TD-IDF score in mathematical terms is as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\textit{TD-IDF(w)} = TF(w) * IDF(w)$</p>
    </div>
    <div id="para-div">
      <p>For a given corpus, the TF-IDF model outputs a sparse matrix, where the rows represent the documents in the corpus, while
        the columns are the unique words from the corpus.</p>
    </div>
    <div id="para-div">
      <p>To create the TF-IDF sparse matrix for our tiny corpus, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>from sklearn.feature_extraction.text import TfidfVectorizer</p>
      <p>model = TfidfVectorizer(vocabulary=words)</p>
      <p>matrix = model.fit_transform(sentences).toarray()</p>
      <p>tf_idf_words = pd.DataFrame(data=matrix, columns=words)</p>
      <p>tf_idf_words</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="TF-IDF Model" class="gen-img-cls" src="./images/nlp-model-08.png">
        <div class="gen-img-cap">Figure.8</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>As is evident from Figure.8 above, the word <span class="bold">to</span> appears in all the 3 sentences of our tiny corpus
        and hence gets a lower score.</p>
    </div>
    <div id="para-div">
      <p>There will be use-cases in which multiple words together (bigrams or trigrams) may provide a better context for the text
        in the corpus.</p>
    </div>
    <div id="para-div">
      <p>To extract all the bigrams from our tiny corpus, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>all_bigrams = []</p>
      <p>for sentence in sentences:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;all_tokens = tokenizer.tokenize(sentence)</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;bi_tokens = list(nltk.bigrams(all_tokens))</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;bi_words = [' '.join([w1, w2]) for w1, w2 in bi_tokens]</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;all_bigrams.extend(bi_words)</p>
      <p>all_bigrams = sorted(set(all_bigrams), key=all_bigrams.index)</p>
      <p>all_bigrams</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Unique Bigrams" class="gen-img-cls" src="./images/nlp-model-09.png">
        <div class="gen-img-cap">Figure.9</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create the TF-IDF sparse matrix on bigrams for our tiny corpus, run the following statements in the Jupyter cell:</p>
    </div>
    <div id="cmd-div">
      <p>from sklearn.feature_extraction.text import TfidfVectorizer</p>
      <p>model = TfidfVectorizer(ngram_range=(2,2), vocabulary=all_bigrams)</p>
      <p>matrix = model.fit_transform(sentences).toarray()</p>
      <p>tf_idf_bigrams = pd.DataFrame(data=matrix, columns=all_bigrams)</p>
      <p>tf_idf_bigrams</p>
    </div>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="TF-IDF Bigrams" class="gen-img-cls" src="./images/nlp-model-10.png">
        <div class="gen-img-cap">Figure.10</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>As is evident from Figure.10 above, the bigram <span class="bold">to swin</span> appears in all the 2 sentences of our tiny
        corpus and hence gets a lower score for the third document.</p>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>References</p>
    </div>
    <br/>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/NLP/Python-NLTK.html" target="_blank"><span class="bold">Basics of Natural Language Processing using NLTK</span></a></p>
      <p><a href="https://www.nltk.org/" target="_blank"><span class="bold">NLTK Documentation</span></a></p>
    </div>
    <br/>
    <hr class="gen-line-hr" />
    <div>
      <a id="gen-footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
