<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Introduction to Bayes Theorem">
    <meta name="subject" content="Introduction to Bayes Theorem">
    <meta name="keywords" content="math, mathematics, probability, bayes">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Introduction to Bayes Theorem</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home-3"></span></td>
        <td valign="bottom"><span id="home-a-3"><a id="home-a-3" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div-3">
      <p>Introduction to Bayes Theorem</p>
    </div>
    <br />
    <table id="ad-table-3">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">06/05/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr-3" />
    <br/>
    <div id="section-div-3">
      <p>Bayes Theorem</p>
    </div>
    <div id="para-div">
      <p>In the article, <a href="https://polarsparc.github.io/Mathematics/Probability.html" target="_blank"><span class="bold">
        Introduction to Probability</span></a>, we covered the fundamentals of probability, including the concept of <span class=
        "bold">Conditional Probability</span>.</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Bayes Theorem</span> (as referred to as the <span class="hi-yellow">Bayes Rule</span>) is a way of
        computing the conditional probability, given the observation of a new event. In other words, it is the process of updating
        the prior hypothesis (belief) when confronted with new evidence (data).</p>
    </div>
    <div id="para-div">
      <p>Bayes Rule is defined using the following equation:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(H \mid E) = \Large{\frac{P(E \mid H) . P(H)}{P(E)}}$</p>
      <p>where:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\boldsymbol{H}$ is the hypothesis</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\boldsymbol{E}$ is the evidence</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\boldsymbol{P(H)}$ is the prior probability of the hypothesis</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\boldsymbol{P(E)}$ is the prior probability of the evidence</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\boldsymbol{P(E \mid H)}$ is the probability (or likelihood) of the evidence given the hypothesis</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\boldsymbol{P(H \mid E)}$ is the posterior probability of the hypothesis given the evidence</p>
    </div>
    <div id="step-div-3">
      <p>Proof of Bayes Theorem</p>
    </div>
    <div id="para-div">
      <p>From the principles of conditional probability, we know the probability for the two dependent events $H$ and $E$ to occur
        together is defined as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(H \cap E) = P(H) . P(E \mid H)$ $\color{red} ..... (1)$</p>
      <p>Similarly,the probability for the two dependent events $E$ and $H$ to occur together is defined as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(E \cap H) = P(E) . P(H \mid E)$ $\color{red} ..... (2)$</p>
      <p>We know that the probability of ($H$ and $E$) is the same as the probability of ($E$ and $H$). This implies the equations
        $\color{red} (1)$ and $\color{red} (2)$ must be equal.</p>
      <p>That is:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(H) . P(E \mid H) = P(E) . P(H \mid E)$</p>
      <p>Re-arranging the terms, we get:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(H \mid E) = \bbox[pink,2pt]{\Large{\frac{P(E \mid H) . P(H)}{P(E)}}}$</p>
    </div>
    <div id="step-div-3">
      <p>Solved Problems</p>
    </div>
    <div id="para-div">
      <p>Let us look at an example to get an understanding of the Bayes Theorem.</p>
      <table id="row2-table-3">
        <thead>
          <tr>
            <th class="th-c1">Example-1</th>
            <th>There are two boxes with some colored balls in each. The first box contains 4 red and 6 green balls. The second box
              contains 4 red and 3 green balls. One box is picked at random and one ball from that box is drawn at random and it
              turns out to be a green ball. Find the probability that the green ball was drawn from the first box</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="2">
              <p>The following illustration shows the tree diagram of the probability outcomes:</p>
              <div id="img-outer-div-3"> <img alt="Boxes and Balls" src="./images/bayes-1.png"
                class="img-cls-3" />
                <div class="img-cap-3">Fig.1</div>
              </div>
              <br/>
              <p>We are interested in the probability of picking the first box given it is a green ball - $P(B_1 \mid G)$</p>
              <p>From Bayes Rules, we know: $P(B_1 \mid G) = \Large{\frac{P(G \mid B_1) . P(B_1)}{P(G)}}$</p>
              <p>From the probability outcomes from above, we can infer the following:</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(G \mid B_1) = 0.6$</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(B_1) = 0.5$</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(G) = P(B_1) . P(G \mid B_1) + P(B_2) . P(G \mid B_2) = (0.5)(0.6) + (0.5)(0.43) = 0.52$</p>
              <p>Therefore $P(B_1 \mid G) = \Large{\frac{(0.6)(0.5)}{0.52}}$ $= 0.57$</p>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
    <br/>
    <div id="para-div">
      <p>Let us look at another example to get a better understanding of the Bayes Rule.</p>
      <table id="row2-table-3">
        <thead>
          <tr>
            <th class="th-c1">Example-2</th>
            <th>There are 3 companies A, B, and C that manufacture dryers. Company A makes 60% of the dryers, company B A makes 30%
              of the dryers, and company C makes the remaining 20% of the dryers respectively. The dryers made by company A have a
              defect rate of 4%, company B has a defect rate of 3% , and company C has a defect rate of 4% respectively. If a dryer
              is selected randomly and found to be defective, find the probability it was manufactured by company B</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="2">
              <p>The following illustration shows the tree diagram of the probability outcomes:</p>
              <div id="img-outer-div-3"> <img alt="Dryers and Defects" src="./images/bayes-2.png"
                class="img-cls-3" />
                <div class="img-cap-3">Fig.2</div>
              </div>
              <br/>
              <p>We are interested in the probability that the dryer is made by company B given it is defective - $P(C_A \mid D)$</p>
              <p>From Bayes Rules, we know: $P(C_A \mid D) = \Large{\frac{P(D \mid C_A) . P(C_A)}{P(D)}}$</p>
              <p>From the probability outcomes from above, we can infer the following:</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(D \mid C_A) = 0.04$</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(C_A) = 0.5$</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(D) = P(C_A) . P(D \mid C_A) + P(C_B) . P(D \mid C_B) + P(C_C) . P(D \mid C_C) = (0.5)(0.04)
                + (0.3)(0.03) + (0.2)(0.04) = 0.037$</p>
              <p>Therefore $P(C_A \mid D) = \Large{\frac{(0.04)(0.5)}{0.037}}$ $= 0.54$</p>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
    <br/>
    <div id="para-div">
      <p>Let us look at one final example to firm our understanding of the Bayes Theorem.</p>
      <table id="row2-table-3">
        <thead>
          <tr>
            <th class="th-c1">Example-3</th>
            <th>There is a rare diseases that effects 1% of a population. A certain lab test can detect this rare disease. The lab
              test is positive for 95% of the people with the rare disease and positive in 2% of the people without the rare disease.
              If a person, selected at random from this population has tested positive, what is the probability that the selected
              person has the rare disease</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="2">
              <p>The following illustration shows the tree diagram of the probability outcomes:</p>
              <div id="img-outer-div-3"> <img alt="Disease and Test" src="./images/bayes-3.png"
                class="img-cls-3" />
                <div class="img-cap-3">Fig.3</div>
              </div>
              <br/>
              <p>We are interested in the probability that the person selected from the population who tested positive has the rare
                disease - $P(D \mid +)$</p>
              <p>From Bayes Rules, we know: $P(D \mid +) = \Large{\frac{P(+ \mid D) . P(D)}{P(+)}}$</p>
              <p>From the probability outcomes from above, we can infer the following:</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(+ \mid D) = 0.95$</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(D) = 0.01$</p>
              <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(+) = P(D) . P(+ \mid D) + P(\bar{D}) . P(+ \mid \bar{D}) = (0.01)(0.95) + (0.99)(0.02)
                = 0.03$</p>
              <p>Therefore $P(D \mid +) = \Large{\frac{(0.95)(0.01)}{0.03}}$ $= 0.32$</p>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
    <br/>
    <div id="step-div-3">
      <p>Generic Bayes Rule</p>
    </div>
    <div id="para-div">
      <p>From the 3 examples above, notice that the denominator was the aggregate of all the options (the two boxes from the first
        example and the three companies from the second example) involved in the problem.</p>
    </div>
    <div id="para-div">
      <p>With that intuition, one could re-write the Bayes Rule in a generic way as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(H \mid E) = \Large{\frac{P(E \mid H) . P(H)}{P(E)}}$</p>
      <p>where:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\boldsymbol{P(E)} = \sum_{i=1}^n P(E_i) . P(H \mid E_i)$</p>
      <p>with $n$ entities</p>
      <p>Therefore,</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(H \mid E) = \bbox[pink,2pt]{\Large{\frac{P(E \mid H) . P(H)}{\sum_{i=1}^n P(E_i) . P(H \mid
        E_i)}}}$</p>
    </div>
    <br/>
    <div id="section-div-3">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/Mathematics/Probability.html" target="_blank"><span class="bold">Introduction to
        Probability</span></a></p>
    </div>
    <hr class="line-hr-3" />
    <div>
      <a id="footer-a-3" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
