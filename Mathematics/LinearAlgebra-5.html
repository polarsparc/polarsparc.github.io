<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Introduction to Linear Algebra - Part 5">
    <meta name="subject" content="Introduction to Linear Algebra - Part 5">
    <meta name="keywords" content="math, mathematics, linear algebra">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Introduction to Linear Algebra - Part 5</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home-3"></span></td>
        <td valign="bottom"><span id="home-a-3"><a id="home-a-3" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div-3">
      <p>Introduction to Linear Algebra - Part 5</p>
    </div>
    <br />
    <table id="ad-table-3">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">03/04/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr-3" />
    <br/>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-4.html">Part 4</a> of this series, we explored the concept
        on Eigen decomposition and the Power of a matrix. In this <span class="underbold">FINAL</span> part, we will explore the
        concept of <span class="hi-yellow">Rank of a Matrix</span> and <span class="hi-yellow">Singular Value Decomposition</span>.</p>
    </div>
    <div id="section-div-3">
      <p>Rank of a Matrix</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-yellow">Rank of a Matrix</span> is a single number associate with a matrix that indicates the maximum
        number of <span class="underbold">Linearly Independent</span> column or row vectors of the matrix.</p>
      <p>If $r$ is the rank of a matrix $\textbf{A}$ of dimesnsion M x N, then $0 \le r \le min(M, N)$.</p>
      <p>The rank of the matrix $\begin{bmatrix} 1 & 0 & 2 \\ 2 & 1 & 0 \\ 3 & 2 & 1 \end{bmatrix}$ is 3 since each of the column
        (or row) vectors is linearly independent, meaning, they cannot be created using a linear combination of other column (or
        row) vectors.</p>
      <p>The rank of the matrix $\begin{bmatrix} 1 & 3 & 2 \\ 2 & 6 & 0 \\ 4 & 12 & 1 \end{bmatrix}$ is 2, since the second column
        vector is a linear combination of the first column vector.</p>
      <p>The rank of the matrix $\begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ is 1.</p>
      <p>The rank of the matrix $\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ is 0.</p>
    </div>
    <div id="section-div-3">
      <p>Singular Value Decomposition</p>
    </div>
    <div id="para-div">
      <p>Eigen decomposition only works for a square matrix, but in the real-world not all matrices are square; they are rectangular.
        This is where <span class="hi-yellow">Singular Value Decomposition</span> comes in handy.</p>
      <p>The purpose of <span class="hi-yellow">Singular Value Decomposition</span> (often referred to as <span class="hi-vanila">
        SVD</span>) is to decompose a matrix $\textbf{A}$ of dimension M x N into three matrices $\textbf{U}$, $\Sigma$, and $\textbf{V}^T$
        such that the following condition is satisfied:</p>
      <p>$\textbf{A} = \textbf{U} \Sigma \textbf{V}^T$</p>
      <p>where</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;the matrix $\textbf{U}$, referred to as the <span class="hi-blue">Left Singular</span> vectors
        matrix, is an <span class="underbold">ORTHOGONAL</span> matrix of dimension M x M (corresponding to the M rows of
        $\textbf{A}$),</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;the diagonal matrix of scalars $\Sigma$, referred to as the <span class="hi-vanila">Singular</span>
        values matrix, is of dimension M x N,</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;and the matrix $\textbf{V}^T$, referred to as the <span class="hi-green">Right Singular</span>
        vectors matrix, is an <span class="underbold">ORTHOGONAL</span> matrix of dimension N x N (corresponding to the N columns of
        $\textbf{A}$).</p>
    </div>
    <div id="para-div">
      <p>The following is a pictorial illustration of SVD:</p>
    </div>
    <div id="img-outer-div-3"> <img alt="SVD" src="./images/linalgebra-6.png"
      class="img-cls-3" />
      <div class="img-cap-3">Fig.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Before we proceed with the process of computing SVD of a matrix, the following are some properties of <span class="bold">
        Matrices</span>:</p>
      <ul id="green-sqr-ul">
        <li><p>For any matrix $\textbf{A}$, the transpose of its transpose, is itself. In other words, $(\textbf{A}^T)^T =
          \textbf{A}$</p></li>
        <li><p>The transpose of a <span class="underbold">DIAGONAL</span> matrix $\textbf{D}$ is itself. That is $\textbf{D}^T =
          \textbf{D}$</p></li>
        <li><p>The transpose of an <span class="underbold">ORTHOGONAL</span> matrix $\textbf{A}$ is equal to its inverse. That is
          $\textbf{A}^T = \textbf{A}^{-1}$</p></li>
        <li><p>For any matrix $\textbf{A}$, the matrix multiplication with its transpose $\textbf{A}\textbf{A}^T$ or $\textbf{A}^T
          \textbf{A}$ is a <span class="underbold">SQUARE</span> matrix</p></li>
        <li><p>Given matrices $\textbf{A}$, $\textbf{B}$, and $\textbf{C}$, the transpose of their product is equal to the product
          of the transpose of each of the individual matrices in the <span class="underbold">REVERSE</span> order. In other words,
          $(\textbf{A}\textbf{B}\textbf{C})^T = \textbf{C}^T \textbf{B}^T \textbf{A}^T$</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>Given $\textbf{A} = \textbf{U} \Sigma \textbf{V}^T$.</p>
      <p>Then, $\textbf{A}^T\textbf{A} = (\textbf{U} \Sigma \textbf{V}^T)^T \textbf{U} \Sigma \textbf{V}^T$</p>
      <p>That is, $\textbf{A}^T\textbf{A} = (\textbf{V}^T)^T \Sigma^T \textbf{U}^T \textbf{U} \Sigma \textbf{V}^T = \textbf{V}
        \Sigma \textbf{U}^T \textbf{U} \Sigma \textbf{V}^T$</p>
      <p>Since the matrix $\textbf{U}$ is orthogonal, $\textbf{U}^T\textbf{U} = \textbf{I}$</p>
      <p>That is, $\textbf{A}^T\textbf{A} = \textbf{V} \Sigma \textbf{I} \Sigma \textbf{V}^T$</p>
      <p>Or, $\textbf{A}^T\textbf{A} = \textbf{V} \Sigma^2 \textbf{V}^T$</p>
      <p>Notice that the right-hand size of the above equation is that of eigen decomposition.</p>
      <p>Therefore, the matrix $\Sigma^2$ is the diagonal matrix of <span class="underbold">SQUARED</span> eigenvalues of the matrix
        $\textbf{A}^T\textbf{A}$ and the matrix $\textbf{V}$ is the matrix of the eigenvectors of the matrix $\textbf{A}^T\textbf{A}$.</p>
      <p>We now have the matrices $\textbf{V}$ and $\Sigma$. The only missing piece is the matrix $\textbf{U}$.</p>
      <p>Next, $\textbf{A}\textbf{A}^T = \textbf{U} \Sigma \textbf{V}^T (\textbf{U} \Sigma \textbf{V}^T)^T$</p>
      <p>That is, $\textbf{A}\textbf{A}^T = \textbf{U} \Sigma \textbf{V}^T (\textbf{V}^T)^T \Sigma^T \textbf{U}^T = \textbf{U} \Sigma
        \textbf{V}^T \textbf{V} \Sigma \textbf{U}^T$</p>
      <p>Since the matrix $\textbf{V}$ is orthogonal, $\textbf{V}^T\textbf{V} = \textbf{I}$</p>
      <p>That is, $\textbf{A}\textbf{A}^T = \textbf{U} \Sigma \textbf{I} \Sigma \textbf{U}^T$</p>
      <p>Or, $\textbf{A}\textbf{A}^T = \textbf{U} \Sigma^2 \textbf{U}^T$</p>
      <p>Notice that the right-hand size of the above equation is that of eigen decomposition. The matrix $\textbf{U}$ is the matrix
        of the eigenvectors of the matrix $\textbf{A}\textbf{A}^T$.</p>
      <p>With this, we now have all the 3 matrices of SVD - the left singular vectors matrix $\textbf{U}$, the singular values
        matrix $\Sigma$, and the right singular vectors matrix $\textbf{V}^T$.</p>
    </div>
    <div id="para-div">
      <p>Let us look at an example to understand singular value decomposition.</p>
      <table id="row2-table-3">
        <thead>
          <tr>
            <th class="th-c1">Example-1</th>
            <th>Find the three SVD matrices $\textbf{U}$, $\Sigma$, and $\textbf{V}^T$ for the matrix: $\textbf{A} = \begin{bmatrix}
              1 & -1 & 3 \\ 3 & 1 & 1 \end{bmatrix}$</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="2">
              <p>Given $\textbf{A} = \begin{bmatrix} 1 & -1 & 3 \\ 3 & 1 & 1 \end{bmatrix}$.</p>
              <p>Then, $\textbf{A}^T = \begin{bmatrix} 1 & 3 \\ -1 & 1 \\ 3 & 1 \end{bmatrix}$</p>
              <p>Next, $\textbf{A}^T\textbf{A} = \begin{bmatrix} 1 & 3 \\ -1 & 1 \\ 3 & 1 \end{bmatrix} \begin{bmatrix} 1 & -1 & 3
                \\ 3 & 1 & 1 \end{bmatrix}$</p>
              <p>That is, $\textbf{A}^T\textbf{A} = \begin{bmatrix} 1+9 & -1+3 & 3+3 \\ -1+3 & 1+1 & -3+1 \\ 3+3 & -3+1 & 9+1
                \end{bmatrix} = \begin{bmatrix} 10 & 2 & 6 \\ 2 & 2 & -2 \\ 6 & -2 & 10 \end{bmatrix}$</p>
              <p>To find the eigenvalues for the matrix $\textbf{A}^T\textbf{A}$, we need to solve for $\lvert \textbf{A}^T\textbf{A}
                - \lambda\textbf{I}\rvert = 0$.</p>
              <p>That is, $\begin{vmatrix} \begin{bmatrix} 10 & 2 & 6 \\ 2 & 2 & -2 \\ 6 & -2 & 10 \end{bmatrix} - \lambda
                \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \end{vmatrix} = 0$</p>
              <p>Or, $\begin{vmatrix} \begin{bmatrix} 10 & 2 & 6 \\ 2 & 2 & -2 \\ 6 & -2 & 10 \end{bmatrix} - \begin{bmatrix} \lambda
                & 0 & 0 \\ 0 & \lambda & 0 \\ 0 & 0 & \lambda \end{bmatrix} \end{vmatrix} = 0$</p>
              <p>Or, $\begin{vmatrix}  10-\lambda & 2 & 6 \\ 2 & 2-\lambda & -2 \\ 6 & -2 & 10-\lambda \end{vmatrix} = 0$</p>
              <p>We know the determinant of $\begin{vmatrix}  a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = a \begin{vmatrix}
                e & f \\ h & i \end{vmatrix} - b \begin{vmatrix} d & f \\ g & i \end{vmatrix} + c \begin{vmatrix} d & e \\ g & h
                \end{vmatrix} = a (ei - fh) - b (di - fg) + c (dh - eg)$</p>
              <p>That is, $(10 - \lambda)[(2-\lambda)(10-\lambda)-4] - 2[2(10-\lambda)+12] + 6[-4-6(2-\lambda)] = 0$</p>
              <p>Or, $(10 - \lambda)[(16-12\lambda+{\lambda}^2] - 2[-2\lambda+32] + 6[6\lambda-16] = 0$</p>
              <p>Or, ${\lambda}^3 - 22{\lambda}^2 = 96\lambda = 0$</p>
              <p>The above cubic equation can be written as $\lambda({\lambda}^2 - 22\lambda + 96) = 0$</p>
              <p>That is, $\lambda(\lambda - 6)(\lambda - 16) = 0$</p>
              <p>Hence, the eigenvalues for the matrix $\textbf{A}^T\textbf{A}$ are $\lambda = 16$, $\lambda = 6$, and $\lambda =
                0$ in the descending order.</p>
              <p>Note that the eigenvalues of the matrix $\textbf{A}^T\textbf{A}$ are <span class="underbold">SQUARED</span> values.
                Hence, to find the diagonal matrix of the singular values $\Sigma$, we need to find their respective square roots.</p>
              <p>In other words, $\Sigma = \begin{bmatrix} \sqrt{16} & 0 & 0 \\ 0 & \sqrt{6} & 0 \end{bmatrix}$</p>
              <div id="warn-div">
                <h4>** FACT **</h4>
                <pre>The number of non-zero singular values equals the rank of the matrix. Hence the rank of the matrix A is 2.</pre>
              </div>
              <p>Therefore, the diagonal matrix $\Sigma$ for the SVD is $\begin{bmatrix} 4 & 0 & 0 \\ 0 & 0.2449 & 0 \end{bmatrix}$</p>
              <p>We also know the relationship between eigenvalues and eigenvectors: $\textbf{A}\vec{v} = \lambda\vec{v}$</p>
              <p>For $\lambda = 16$, $\begin{bmatrix} 10 & 2 & 6 \\ 2 & 2 & -2 \\ 6 & -2 & 10 \end{bmatrix} \begin{bmatrix} x \\ y
                \\ z \end{bmatrix} = 16\begin{bmatrix} x \\ y \\ z \end{bmatrix}$</p>
              <p>That is, $\begin{align*} & 10x + 2y + 6z = 16x \\ & 2x + 2y - 2z = 16y \\ & 6x - 2y + 10z = 16z \end{align*}$</p>
              <p>Rearranging terms, $\begin{align*} & 3x - y - 3z = 0 \\ & x - 7y - z = 0 \\ & 3x - y - 3z = 0 \end{align*}$</p>
              <p>Solving for equations, we get $y = 0$</p>
              <p>By substitution, we get $x = z$. Choosing $z = 1$, we get $x = 1$, and therefore the eigenvector would be $\vec{v}
                = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}$</p>
              <p>Converting the eigenvector to a unit vector, we get the eigenvector would be $\vec{v} = \begin{bmatrix} 0.7071 \\
                0 \\ 0.7071 \end{bmatrix}$</p>
              <p>Therefore, for the eigenvalue $\lambda = 16$, the corresponding eigenvector is $\begin{bmatrix} 0.7071 \\ 0
                \\ 0.7071 \end{bmatrix}$</p>
              <p>For $\lambda = 6$, $\begin{bmatrix} 10 & 2 & 6 \\ 2 & 2 & -2 \\ 6 & -2 & 10 \end{bmatrix} \begin{bmatrix} x \\ y
                \\ z \end{bmatrix} = 6\begin{bmatrix} x \\ y \\ z \end{bmatrix}$</p>
              <p>That is, $\begin{align*} & 10x + 2y + 6z = 6x \\ & 2x + 2y - 2z = 6y \\ & 6x - 2y + 10z = 6z \end{align*}$</p>
              <p>Rearranging terms, $\begin{align*} & 2x + y + 3z = 0 \\ & x - 2y - z = 0 \\ & 3x - y + 2z = 0 \end{align*}$</p>
              <p>Solving for equations, we get $x = y$</p>
              <p>Choosing $y = 1$, we get $x = 1$, and by substitution, we get $z = -1$, and therefore the eigenvector would be
                $\vec{v} = \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}$</p>
              <p>Converting the eigenvector to a unit vector, we get the eigenvector would be $\vec{v} = \begin{bmatrix} 0.5774 \\
                0.5774 \\ -0.5774 \end{bmatrix}$</p>
              <p>Therefore, for the eigenvalue $\lambda = 6$, the corresponding eigenvector is $\begin{bmatrix} 0.5774 \\ 0.5774
                \\ -0.5774 \end{bmatrix}$</p>
              <p>Finally, for $\lambda = 0$, $\begin{bmatrix} 10 & 2 & 6 \\ 2 & 2 & -2 \\ 6 & -2 & 10 \end{bmatrix} \begin{bmatrix}
                x \\ y \\ z \end{bmatrix} = 0\begin{bmatrix} x \\ y \\ z \end{bmatrix}$</p>
              <p>That is, $\begin{align*} & 10x + 2y + 6z = 0 \\ & 2x + 2y - 2z = 0 \\ & 6x - 2y + 10z = 0 \end{align*}$</p>
              <p>Rearranging terms, $\begin{align*} & 5x + y + 3z = 0 \\ & x + y - z = 0 \\ & 3x - y + 5z = 0 \end{align*}$</p>
              <p>Solving for equations, we get $x = -\frac{1}{2}y$</p>
              <p>Choosing $y = -2$, we get $x = 1$, and by substitution, we get $z = -1$, and therefore the eigenvector would be
                $\vec{v} = \begin{bmatrix} 1 \\ -2 \\ -1 \end{bmatrix}$</p>
              <p>Converting the eigenvector to a unit vector, we get the eigenvector would be $\vec{v} = \begin{bmatrix} 0.4082 \\
                -0.8165 \\ -0.4082 \end{bmatrix}$</p>
              <p>Therefore, for the eigenvalue $\lambda = 0$, the corresponding eigenvector is $\begin{bmatrix} 0.4082 \\ -0.8165
                \\ -0.4082 \end{bmatrix}$</p>
              <p>The matrix $\textbf{V}$ for the SVD of $\textbf{A}$ is the concatenation of the eigenvectors of $\textbf{A}^T
                \textbf{A}$ = $\begin{bmatrix} 0.7071 & 0.5774 & 0.4082 \\ 0 & 0.5774 & -0.8165 \\ 0.7071 & -0.5774 & -0.4082
                \end{bmatrix}$</p>
              <p>Therefore, The matrix $\textbf{V}^T$ for the SVD is $\begin{bmatrix} 0.7071 & 0 & 0.7071 \\ 0.5774 & 0.5774 &
                -0.5774 \\ 0.4082 & -0.8165 & -0.4082 \end{bmatrix}$</p>
              <p>Next, $\textbf{A}\textbf{A}^T = \begin{bmatrix} 1 & -1 & 3 \\ 3 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 3 \\
                -1 & 1 \\ 3 & 1 \end{bmatrix}$</p>
              <p>That is, $\textbf{A}\textbf{A}^T = \begin{bmatrix} 1+1+9 & 3-1+3 \\ 3-1+3 & 9+1+1 \end{bmatrix} = \begin{bmatrix}
                11 & 5 \\ 5 & 11 \end{bmatrix}$</p>
              <p>To find the eigenvalues for the matrix $\textbf{A}\textbf{A}^T$, we need to solve for $\lvert \textbf{A}\textbf{A}^T
                - \lambda\textbf{I}\rvert = 0$.</p>
              <p>That is, $\begin{vmatrix} \begin{bmatrix} 11 & 5 \\ 5 & 11 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0 \\ 0 & 1
                \end{bmatrix} \end{vmatrix} = 0$</p>
              <p>Or, $\begin{vmatrix} \begin{bmatrix} 11 & 5 \\ 5 & 11 \end{bmatrix} - \begin{bmatrix} \lambda & 0 \\ 0 & \lambda
                \end{bmatrix} \end{vmatrix} = 0$</p>
              <p>Or, $\begin{vmatrix}  11-\lambda & 5 \\ 5 & 11-\lambda \end{vmatrix} = 0$</p>
              <p>We know the determinant of $\begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc$</p>
              <p>That is, $(11 - \lambda)(11 - \lambda) - 25 = 0$</p>
              <p>Or, $121 -22\lambda + {\lambda}^2 - 25 = 0$</p>
              <p>Or, ${\lambda}^2 - 22\lambda + 96 = 0$</p>
              <p>The above quadratic equation can be written as $(\lambda - 6)(\lambda - 16) = 0$</p>
              <p>Hence, the eigenvalues are $\lambda = 16$ and $\lambda = 6$ in the descending order.</p>
              <p>We also know the relationship between eigenvalues and eigenvectors: $\textbf{A}\vec{v} = \lambda\vec{v}$</p>
              <p>For $\lambda = 16$, $\begin{bmatrix} 11 & 5 \\ 5 & 11 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} =
                16\begin{bmatrix} x \\ y \end{bmatrix}$</p>
              <p>That is, $\begin{align*} & 11x + 5y = 16x \\ & 5x + 11y = 16y \end{align*}$</p>
              <p>Or, $x = y$</p>
              <p>Choosing $y = 1$, we get $x = 1$, and therefore the eigenvector would be $\vec{v} = \begin{bmatrix} 1 \\ 1
                \end{bmatrix}$</p>
              <p>Converting the eigenvector to a unit vector, we get the eigenvector would be $\vec{v} = \begin{bmatrix} 0.7071 \\
                0.7071 \end{bmatrix}$</p>
              <p>Therefore, for the eigenvalue $\lambda = 16$, the corresponding eigenvector is $\begin{bmatrix} 0.7071 \\ 0.7071
                \end{bmatrix}$</p>
              <p>Similarly, for $\lambda = 6$, $\begin{bmatrix} 11 & 5 \\ 5 & 11 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
                = 6\begin{bmatrix} x \\ y \end{bmatrix}$</p>
              <p>That is, $\begin{align*} & 11x + 5y = 6x \\ & 5x + 11y = 6y \end{align*}$</p>
              <p>Or, $x = -y$</p>
              <p>Choosing $y = -1$, we get $x = 1$, and therefore the eigenvector would be $\vec{v} = \begin{bmatrix} 1 \\ -1
                \end{bmatrix}$</p>
              <p>Converting the eigenvector to a unit vector, we get the eigenvector would be $\vec{v} = \begin{bmatrix} 0.7071 \\
                -0.7071 \end{bmatrix}$</p>
              <p>Therefore, for the eigenvalue $\lambda = 4$, the corresponding eigenvector is $\begin{bmatrix} 0.7071 \\ -0.7071
                \end{bmatrix}$</p>
              <p>The matrix $\textbf{U}$ for the SVD of $\textbf{A}$ is the concatenation of the eigenvectors of $\textbf{A}
                \textbf{A}^T = \begin{bmatrix} 0.7071 & 0.7071 \\ 0.7071 & -0.7071 \end{bmatrix}$</p>
              <p><span class="underbold">SOLUTION</span>: The 3 matrices for the SVD of matrix $\textbf{A}$ are as follows:</p>
              <p>$\textbf{U} = \begin{bmatrix} 0.7071 & 0.7071 \\ 0.7071 & -0.7071 \end{bmatrix}$</p>
              <p>$\Sigma = \begin{bmatrix} 4 & 0 & 0 \\ 0 & 0.2449 & 0 \end{bmatrix}$</p>
              <p>$\textbf{V}^T = \begin{bmatrix} 0.7071 & 0 & 0.7071 \\ 0.5774 & 0.5774 & -0.5774 \\ 0.4082 & -0.8165 & -0.4082
                \end{bmatrix}$</p>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
    <br/>
    <div id="para-div">
      <p>To get a better understanding of SVD, let us break down $\textbf{A} = \textbf{U} \Sigma \textbf{V}^T$.</p>
      <p>One can visualize the matrix $\textbf{U}$ of dimension M x M as a concatenation of N column vectors $\vec{u}$. In other words
        $\textbf{U} = \begin{bmatrix} \vec{u}_1 \: \vert \: \vec{u}_2 \: \vert \: ... \: \vert \: \vec{u}_N \end{bmatrix}$.</p>
      <p>Similarly, the matrix $\textbf{V}^T$ of dimension N x N as a concatenation of N column vectors $\vec{v}^T$. In other words,
        $\textbf{V}^T = \begin{bmatrix} \vec{v}^T{_1} \: \vert \: \vec{v}^T{_2} \: \vert \: ... \: \vert \: \vec{v}^T{_M}
        \end{bmatrix}$.</p>
      <p>The matrix $\Sigma$ is of the same dimension as $\textbf{A}$ M x N and is the diagonal matrix of singular values. In other
        words, $\Sigma = \begin{bmatrix} \sigma_1 & 0 & ... & 0 \\ 0 & \sigma_2 & ... & 0 \\ 0 & 0 & \sigma_N & 0 \end{bmatrix}.$</p>
      <p>Then, $\textbf{A} = \textbf{U} \Sigma \textbf{V}^T = \begin{bmatrix} \vec{u}_1 \: \vert \: \vec{u}_2 \: \vert \: ... \:
        \vert \: \vec{u}_N \end{bmatrix} \begin{bmatrix} \sigma_1 & 0 & ... & 0 \\ 0 & \sigma_2 & ... & 0 \\ 0 & 0 & \sigma_N & 0
        \end{bmatrix} \begin{bmatrix} \vec{v}^T{_1} \: \vert \: \vec{v}^T{_2} \: \vert \: ... \: \vert \: \vec{v}^T{_M} \end{bmatrix}
        = \vec{u}_1\sigma_1\vec{v}^T{_1} + \vec{u}_2\sigma_2\vec{v}^T{_2} + ... + \vec{u}_N\sigma_N\vec{v}^T{_N}$.</p>
      <p>Or, $\textbf{A} = \sum_{i=1}^{N} \vec{u}_i\sigma_i\vec{v}^T{_i}$.</p>
      <p>Intuitively, one can think of the matrix $\textbf{A}$ as a combination of a series of smaller atomic matrices $\vec{u}_i
        \sigma_i \vec{v}^T{_i}$.</p>
      <p>In the real-world use-cases, what this means is that for a very large dimension matrix, if its rank is $r$, then we will
        only need upto the $r$ number of those smaller atomic vectors to recreate the original matrix. This is akin to compression
        (reduction in dimension). For example, one could use SVD for compression of media files such as audio, images, video, etc.</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Here is the link to the <a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-Hands-On.html" target="_blank">
        <span class="bold">Jupyter Notebook</span></a> that provides an hands-on demo for the entire series on Linear Algebra.</p>
    </div>
    <br/>
    <div id="section-div-3">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-4.html" target="_blank"><span class="bold">Introduction to Linear Algebra - Part 4</span></a></p>
      <p><a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-3.html" target="_blank"><span class="bold">Introduction to Linear Algebra - Part 3</span></a></p>
      <p><a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-2.html" target="_blank"><span class="bold">Introduction to Linear Algebra - Part 2</span></a></p>
      <p><a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-1.html" target="_blank"><span class="bold">Introduction to Linear Algebra - Part 1</span></a></p>
    </div>
    <hr class="line-hr-3" />
    <div>
      <a id="footer-a-3" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
