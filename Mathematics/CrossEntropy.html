<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Understanding Cross Entropy">
    <meta name="subject" content="Understanding Cross Entropy">
    <meta name="keywords" content="ai, mathematics, cross_entropy">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Understanding Cross Entropy</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home-3"></span></td>
        <td valign="bottom"><span id="home-a-3"><a id="home-a-3" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div-3">
      <p>Understanding Cross Entropy</p>
    </div>
    <br/>
    <table id="ad-table-3">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">07/12/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr-3" />
    <br/>
    <div id="para-div">
      <p>For a <span class="hi-grey">Classification</span> model that predicts one of the $M$ classes (or target labels), given an
        input, the model predicts the probability distribution for the $M$ classes.</p>
      <p>For the classification model training, we need a loss function that compares two probability distributions - one the ground
        truth (real) probability distribution and the other the predicted probability distribution, and penalize the probabilities
        from the predicted distribution that are confidently wrong.</p>
      <p>This is where the <span class="hi-yellow">Cross Entropy</span> loss function comes into play !!!</p>
      <p>The term <span class="hi-yellow">Entropy</span> is for measuring the uncertainity in the prediction and term <span class=
        "hi-yellow">Cross</span> is for comparing two distributions - the ground truth vs the prediction.</p>
      <p>In mathematical terms, the Cross Entropy loss function is expressed as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$H(P, Q_{\theta}) = - \sum_i^N P(y_i|x_i) . \log_e(Q_{\theta}(\hat{y_i}|x_i))$ $...\color{red}(1)$</p>
      <p>where</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(y_i|x_i)$ is the ground truth probability distribution with real probabilities $y_i$ given the
        input $x_i$ (represented as $y_i|x_i$)</p>
      <p>and</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$Q_{\theta}(\hat{y_i}|x_i)$ is the predicted probability distribution with probabilities $\hat{y_i}$
        given the input $x_i$ (represented as $\hat{y_i}|x_i$) with $\theta$ being the model parameters</p>
    </div>
    <div id="para-div">
      <p>Let us try to unpack the Cross Entropy loss equation shown in the equation $\color{red}(1)$.</p>
      <p>One way to compare two quantities is using <span class="underbold">Subtraction</span>. if the result is zero, then the two
      quantities are equal else not.</p>
      <p>But, how does one perform a subtraction on two probability distributions ???</p>
      <p>One approach would be to find the <span class="underbold">Mean</span> (or <span class="bold">average</span>) for each of the
        probability distributions.</p>
      <p>The next challange - how does one find the mean of a probability distribution ???</p>
      <p>This is where we can leverage the <span class="underbold">Expected Value</span>, which approximate to the average for very
        large sample sizes (law of large numbers).</p>
      <p>Mathematically, the Expected Value can be expressed as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\mathbb{E}(X) = \sum_i^N x_i . p(x_i)$ $...\color{red}(2)$</p>
      <p>where</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$X$ is the random variable representing the sample space and $x_i$ is the $i^{th}$ sample from the
        sample space,</p>
      <p>and</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$p(x_i)$ is the probability for the sample $x_i$</p>
    </div>
    <div id="para-div">
      <p>We indicated the intent of the Cross Entropy is to <span class="underbold">penalize</span> the model for assigning higher
        probabilities to incorrect target classes. In other words, Cross Entropy is about the measure of <span class="underbold">
        surprise</span>. For example, when an incoming email is classified as SPAM when it is indeed SPAM (high predicted probability),
        we are less surprised (or low Entropy value). However, if the incoming email is NOT classified as SPAM when it is indeed a
        SPAM (low predicted probability), we are more surprised (or high Entropy value). This implies there is an inverse relation
        between the predicted probability and the Entropy. This is where one can leverage a <span class="underbold">Logarithm</span>
        scale for Entropy (the inverse behavior).</p>
      <p>For example, if the predicted probability is $0.8$, then $log_e(0.8) \approx -0.22$.</p>
      <p>Similarly, if the predicted probability is $0.1$, then $log_e(0.1) \approx -2.30$.</p>
      <p>One can compute the Expected Value (equation $\color{red}(2)$ from above) for probability distributions as well.</p>
      <p>Mathematically, the Expected Value for a probability distribution can be expressed as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\mathbb{E}(p(X)) = - \sum_i^M p(x_i) . log_e(p(x_i))$ $...\color{red}(3)$</p>
      <p>where</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$p(x_i)$ is the probability for the sample $x_i$</p>
      <p>and</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$log_e(p(x_i))$ is a way to measure the element of surprise</p>
      <p>Notice the use of the <span class="underbold">minus</span> sign, because $log_e()$ will return a <span class="underbold">
        negative</span> value for probability values between $0.0$ and $1.0$, and hence need to compensate for it.</p>
    </div>
    <div id="para-div">
      <p>Let us test the above equation $\color{red}(3)$ with an example to see if it works.</p>
      <p>Assume a classification model that predicts one of the $3$ target labels. Let the ground truth be the probabilities $\{0.98
        ,001, 0.01\}$.</p>
      <p>If the predicted probabilities are $\{0.8, 0.1, 0.1\}$, then the Expected Value for the predicted distribution will be $-(
        0.98 * log_e(0.8) + 0.01 * log_e(0.1) + 0.01 * log_e(0.1)) \approx 0.26$.</p>
      <p>However, if the predicted probabilities are $\{0.1, 0.8, 0.1\}$, then the Expected Value for the predicted distribution will
        be $-(0.98 * log_e(0.1) + 0.01 * log_e(0.1) + 0.01 * log_e(0.1)) \approx 4.56$.</p>
      <p>Notice the value $4.56$ is much higher (penalizing for wrong prediction) than the value $0.26$.</p>
    </div>
    <div id="para-div">
      <p>Given that we have all the ingredients, we can compare two probability distributions $P$ and $Q_{\theta}$ by using equation
        $\color{red}(3)$ and the subtraction operation as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\sum_i^N P(y_i|x_i) . log_e(P(y_i|x_i)) - \sum_i^N P(y_i|x_i) . log_e(Q_{\theta}(\hat{y_i}|x_i))$
        $...\color{red}(4)$</p>
      <p>Simplifying the equation $\color{red}(4)$ from above, we get:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\sum_i^N P(y_i|x_i) . [log_e(P(y_i|x_i)) - log_e(Q_{\theta}(\hat{y_i}|x_i))]$</p>
      <p>Or:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\bbox[lightgreen,2pt]{\sum_i^N P(y_i|x_i) . log_e\Large{(\frac{P(y_i|x_i)}{Q_{\theta}(\hat{y_i}|
        x_i)})}}$ $...\color{red}(5)$</p>
      <p>The equation $\color{red}(5)$ above is referred to as the <span class="hi-yellow">Kullback-Leibler Divergence</span> or
        <span class="hi-yellow">KL Divergence</span> for short, and is often used for comparing two probability distributions.</p>
      <p>When we train a classification model, we use <span class="underbold">Gradient Descent</span> (derivatives from calculus)
        during <span class="underbold">Backpropagation</span> to optimize the model weights $\theta$.</p>
      <p>Given the first term in the equation $\color{red}(4)$ above is independent of $\theta$, it can be dropped for parameter
        optimization.</p>
      <p>Hence, we end up with the simplified equation:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\bbox[pink,2pt] {- \sum_i^N P(y_i|x_i) . log_e(Q_{\theta}(\hat{y_i}|x_i))]}$ $...\color{red}(6)$</p>
      <p>Compare the above equation $\color{red}(6)$ to the equation of Cross Entropy in $\color{red}(1)$ !!!</p>
    </div>
    <div id="para-div">
      <p>Hope this article provides the intuition into why the Cross Entropy loss function for classification models !!!</p>
    </div>
    <br/>
    <div id="section-div-3">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank"><span class="bold">
        Cross Entropy - PyTorch</span></a></p>
    </div>
    <hr class="line-hr-3" />
    <div>
      <a id="footer-a-3" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
