<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Deep Learning - Gated Recurrent Unit">
    <meta name="subject" content="Deep Learning - Gated Recurrent Unit">
    <meta name="keywords" content="artificial-intelligence, deep-learning, neural-network, pytorch, gru">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Deep Learning - Gated Recurrent Unit</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="gen-home"></span></td>
        <td valign="bottom"><span id="gen-home-a"><a id="gen-home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="gen-title-div">
      <p>Deep Learning - Gated Recurrent Unit</p>
    </div>
    <br />
    <table id="gen-ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">11/03/2023</td>
        </tr>
      </tbody>
    </table>
    <hr class="gen-line-hr" />
    <br/>
    <div id="section-div">
      <p>Introduction</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the previous article <a href="https://polarsparc.github.io/DeepLearning/DL-LongShortTerm.html" target="_blank"><span class
        ="bold">Long Short Term Memory</span></a> of this series, we provided an explanation of the inner workings of the <span class=
        "bold">LSTM</span> model and how it mitigates the <a href="https://polarsparc.github.io/DeepLearning/DL-VanishingGradient.html"
        target="_blank"><span class="bold">Vanishing Gradient</span></a> issue.</p>
      <p>In 2014, the <span class="hi-yellow">Gated Recurrent Unit</span> model (or <span class="hi-yellow">GRU</span> for short) was
        introduced as a more efficient alternative than the <span class="bold">LSTM</span> model.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Gated Recurrent Unit</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="bold">GRU</span> is a more recent model that does away with the long term memory (the <span class="bold">Cell
        state</span>) and using fewer parameters than the <span class="bold">LSTM</span> model, and yet with an ability to remember
        the longer term historical context.</p>
      <p>One can basically think of the <span class="bold">GRU</span> model as an enhanced version of the <span class="bold">RNN</span>
        model that is not susceptible to the <span class="bold">Vanishing Gradient</span> problem.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the high-level abstraction of a <span class="bold">Gated Recurrent Unit</span> cell:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="GRU Cell" class="gen-img-cls" src="./images/gated-recurrent-1.png">
        <div class="gen-img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The output from the <span class="bold">GRU</span> model is the <span class="underbold">hidden state</span> $h$ from the
        current time step, which also captures the long term context from the sequence of inputs that has been processed until the
        current time step. $x$ is the next input into the model.</p>
      <p>Notice that the <span class="bold">GRU Cell</span> does not show any <span class="bold">weight</span> parameters and that
        is intentional as the computations in the <span class="bold">GRU Cell</span> are more complex than the <span class="bold">
        RNN Cell</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the <span class="bold">Gated Recurrent Unit</span> network unfolded over time for a sequence
        of $3$ inputs:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Unfolded GRU" class="gen-img-cls" src="./images/gated-recurrent-2.png">
        <div class="gen-img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice that the unfolded network in <span class="bold">Figure.2</span> above looks very similar to that of an <span class=
        "bold">RNN</span> network. The difference is inside the <span class="bold">GRU Cell</span>.</p>
      <p>Now, for the question on what magic happens inside the <span class="bold">GRU Cell</span> with the next input $x$ in the
        sequence and the previous value of the <span class="bold">hidden state</span> $h$ to generate the output ???</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the computation graph inside the <span class="bold">GRU Cell</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="GRU Computations" class="gen-img-cls" src="./images/gated-recurrent-3.png">
        <div class="gen-img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The computational graph may appear complicated, but in the following paragraphs, we will unpack and explain each of the blocks,
        so it becomes more clear.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-yellow">Reset Gate</span> block in <span class="bold">Figure.3</span> controls what percentage of the
        information from the previous <span class="bold">hidden state</span> needs to be <span class="underbold">forgotten</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration focuses on the <span class="bold">Reset Gate</span> block:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Reset Gate" class="gen-img-cls" src="./images/gated-recurrent-4.png">
        <div class="gen-img-cap">Figure.4</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">Reset Gate</span> uses the input $x_t$ from the current time step along with the output $h_{t-1}$
        from the previous time step and applies the <span class="hi-green">Sigmoid</span> activation function to generate a numeric
        value between $0.0$ and $1.0$, which acts like the percentage knob to control how much information from the previous time
        step needs to be forgotten and what portion carried forward.</p>
      <p>In mathematical terms, the computation that happen inside the first section of the <span class="bold">Reset Gate</span> is
        as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$r_t = sigmoid(W_{rx} * x_t + W_{rh} * h_{t-1} + b_r)$</p>
      <p>where $W_{rx}$ and $W_{rh}$ are the <span class="bold">weight</span>s associated with the input $x_t$ and the previous
        output $h_{t-1}$ respectively, while $b_r$ is the <span class="bold">bias</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration focuses on the section to the right of the <span class="bold">Reset Gate</span> block in <span
        class="bold">Figure.3</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Proposed State" class="gen-img-cls" src="./images/gated-recurrent-5.png">
        <div class="gen-img-cap">Figure.5</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>This section combines the output from the <span class="bold">Reset Gate</span> with the output $h_{t-1}$ from the previous
        time step using the <span class="underbold">element-wise</span> (or <span class="underbold">hadamard</span>) product and then
        adds it to the weighted input $x_t$ from the current time step. Finally, it applies the <span class="hi-purple">Tanh</span>
        activation function to generate a numeric value between $-1.0$ and $1.0$, which determines how much of information from the
        <span class="bold">Reset Gate</span> $r_t$ and the input $x_t$ from the current time step, taken together, needs to be removed
        or added. In other words, the output from this section is the proposed <span class="underbold">candidate</span> <span class=
        "bold">hidden state</span> that is to be carried forward to the next time step.</p>
      <p>In mathematical terms, the computation that happen inside this section is as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$C_t = tanh(W_{cx} * x_t + r_t \odot h_{t-1} + b_c)$</p>
      <p>where $\odot$ is the element-wise vector multiplication.</p>
      <p>where $W_{cx}$ is the <span class="bold">weight</span> associated with the input $x_t$ and $b_c$ is the <span class="bold">
        bias</span>.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-yellow">Update Gate</span> in <span class="bold">Figure.3</span> above controls what percentage of the
        information from the previous <span class="bold">hidden state</span> and the candidate <span class="bold">hidden state</span>
        needs to be retained and passed along.</p>
    </div>
    <div id="para-div">
      <p>The following illustration focuses on the <span class="bold">Update Gate</span> block:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Update Gate" class="gen-img-cls" src="./images/gated-recurrent-6.png">
        <div class="gen-img-cap">Figure.6</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">Update Gate</span> uses the input $x_t$ from the current time step along with the output $h_{t-1}$
        from the previous time step and applies the <span class="hi-green">Sigmoid</span> activation function to generate a numeric
        value between $0.0$ and $1.0$, which acts like the percentage knob to control how much of information from the previous
        time step and the proposed <span class="bold">hidden state</span> needs to be retained and passed along to the next time
        step.</p>
      <p>In mathematical terms, the computation that happen inside the <span class="bold">Update Gate</span> is as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$z_t = sigmoid(W_{zx} * x_t + W_{zh} * h_{t-1} + b_z)$</p>
      <p>where $W_{zx}$ and $W_{zh}$ are the <span class="bold">weight</span>s associated with the input $x_t$ and the previous
        output $h_{t-1}$ respectively, while $b_z$ is the <span class="bold">bias</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration focuses on the computation of the final <span class="bold">hidden state</span>, which will be
        passed to the next time step:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Final State" class="gen-img-cls" src="./images/gated-recurrent-7.png">
        <div class="gen-img-cap">Figure.7</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>If the output $z_t$ from the <span class="bold">Update Gate</span> close to $1$, then a major portion of information from
        the previous time step is retained and information from the current time step is discarded (because of $1 - z_t$) and vice
        versa.</p>
      <p>In other words, the output $z_t$ from the <span class="bold">Update Gate</span> acts like the percentage knob to control
        how much information from the previous and current time steps need to be carried forward.</p>
      <p>In mathematical terms, the computation of the final <span class="bold">hidden state</span> $h_t$ for the current time step
        is as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$h_t = z_t \odot h_{t-1} + (1 - z_t) \odot C_t$</p>
      <p>where $\odot$ is the element-wise vector multiplication.</p>
    </div>
    <div id="para-div">
      <p>Hopefully the unpacking of each of the blocks helped clarify on what is going inside the <span class="bold">GRU Cell</span>.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on GRU Using PyTorch</p>
    </div>
    <br/>
    <div id="para-div">
      <p>We will demonstrate how one could leverage the <span class="bold">GRU</span> model for predicting the <span class="underbold">
        Next Word</span> following a sequence using a toy corpus.</p>
    </div>
    <div id="para-div">
      <p>To predict the <span class="bold">next word</span> following a sequence of input words, we will train the <span class="bold">
        LSTM</span> model using the popular <span class="bold">Aesop's Fable</span> found here <a href="https://read.gov/aesop/091.html"
        target="_blank"><span class="bold">The Goose &amp; the Golden Egg</span></a>.</p>
    </div>
    <div id="para-div">
      <p>To import the necessary <span class="bold">Python</span> module(s), execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>import numpy as np
import random
import torch
from nltk.tokenize import WordPunctTokenizer
from torch import nn</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In order to ensure reproducibility, we need to set the <span class="bold">seed</span> to a constant value by executing the
        following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>seed_value = 101

torch.manual_seed(seed_value)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Copy the contents of the <span class="bold">The Goose &amp; the Golden Egg</span> fable into a variable by executing the
        following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>corpus_text = '''There was once a Countryman who possessed the most wonderful Goose you can imagine, for every day when he visited the nest, the Goose had laid a beautiful, glittering, golden egg. The Countryman took the eggs to market and soon began to get rich. But
it was not long before he grew impatient with the Goose because she gave him only a single golden egg a day. He was not getting rich fast enough.
Then one day, after he had finished counting his money, the idea came to him that he could get all the golden eggs at once by killing the Goose and cutting it open. But when the deed was done, not a single golden egg did he find, and his precious Goose was dead.'''</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To display the length of the corpus, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>len(corpus_text)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>659</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To define a function to extract all the word tokens from the corpus, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>def extract_corpus_words(corpus):
  word_tokenizer = WordPunctTokenizer()
  tokens = word_tokenizer.tokenize(corpus)
  all_tokens = [word.lower() for word in tokens if word.isalpha()]
  return all_tokens</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To extract all the word tokens from the given corpus and display the first 10 words, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>corpus_tokens = extract_corpus_words(corpus_text)
corpus_tokens[:10]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>['there',
 'was',
 'once',
 'a',
 'countryman',
 'who',
 'possessed',
 'the',
 'most',
 'wonderful']</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To define a function to extract all the unique words (vocabulary) from the corpus, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>def extract_corpus_vocab(all_words):
  vocab_words = list(set(all_words))
  vocab_words.sort()
  return vocab_words</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To extract all the unique words (vocabulary) from the given corpus and display the first 10 words, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>vocab_words_list = extract_corpus_vocab(corpus_tokens)
vocab_words_list[:10]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>['a',
 'after',
 'all',
 'and',
 'at',
 'beautiful',
 'because',
 'before',
 'began',
 'but']</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To display the number of unique words (vocabulary), execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>len(vocab_words_list)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>76</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>We know that <span class="bold">neural network</span> models only deal with numbers. To define a function to assign a numeric
        value to each of the unique words (vocabulary) from the corpus, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>def assign_word_number(words):
  words_index = {}
  for idx, word in enumerate(words):
    words_index[word] = idx
  return words_index</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To assign a numeric value to each of the unique words (vocabulary) from the given corpus, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>word_to_nums = assign_word_number(vocab_words_list)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>For our training set, we will consider 3-grams, meaning a sequence of three words from the corpus. To define a function to
        generate the training set of 3-grams from the corpus, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>ngram_size = 3

def create_ngram_sequences(tokens):
  ngrams_list = []
  for i in range(ngram_size, len(tokens) + 1):
    ngrams = tokens[i - ngram_size:i]
    ngrams_list.append(ngrams)
  return ngrams_list</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To generate the training set of 3-grams from the corpus and display the first 10 sequences, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>ngram_sequences = create_ngram_sequences(corpus_tokens)
ngram_sequences[:10]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>[['there', 'was', 'once'],
 ['was', 'once', 'a'],
 ['once', 'a', 'countryman'],
 ['a', 'countryman', 'who'],
 ['countryman', 'who', 'possessed'],
 ['who', 'possessed', 'the'],
 ['possessed', 'the', 'most'],
 ['the', 'most', 'wonderful'],
 ['most', 'wonderful', 'goose'],
 ['wonderful', 'goose', 'you']]</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To display the number of training sequences, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>len(ngram_sequences)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>125</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To define a function to convert the sequence of words to the sequence of their corresponding assigned numeric values, execute
        the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>def ngram_sequences_to_numbers(seq_list):
  ngram_numbers_list = []
  for ngrams in seq_list:
    num_seq = []
    for word in ngrams:
      num_seq.append(word_to_nums.get(word))
    ngram_numbers_list.append(num_seq)
  return ngram_numbers_list</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To convert the sequence of words from the training set to the corresponding sequence of numbers and display the first 10 set
        of numerical sequences, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>ngram_seq_nums = ngram_sequences_to_numbers(ngram_sequences)
ngram_seq_nums[:10]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>[[66, 70, 53],
 [70, 53, 0],
 [53, 0, 15],
 [0, 15, 72],
 [15, 72, 57],
 [72, 57, 64],
 [57, 64, 50],
 [64, 50, 74],
 [50, 74, 35],
 [74, 35, 75]]</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>For training the <span class="bold">neural network</span> model, one should do so on a <span class="bold">GPU</span> device
        for efficiency. To select the <span class="bold">GPU</span> device if available, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>device = 'cuda' if torch.cuda.is_available() else 'cpu'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create a tensor object from the numeric sequences and display the first 10 items, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>ngram_nums = torch.tensor(ngram_seq_nums, device=device)
ngram_nums[:10]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>tensor([[66, 70, 53],
       [70, 53,  0],
       [53,  0, 15],
       [ 0, 15, 72],
       [15, 72, 57],
       [72, 57, 64],
       [57, 64, 50],
       [64, 50, 74],
       [50, 74, 35],
       [74, 35, 75]], device='cuda:0')</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To create the feature and target tensors from the training set, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>X_train = ngram_nums[:, :-1]
y_target = ngram_nums[:, -1]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To define a function to convert the target numeric values to one-hot representation, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>one_hot_size = len(vocab_words_list)

def word_num_to_onehot(target):
  one_hot_list = []
  for idx in target:
    one_hot = np.zeros(one_hot_size, dtype=np.float32)
    one_hot[idx] = 1.0
    one_hot_list.append(one_hot)
  return one_hot_list</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To convert the target tensor object to equivaled one-hot representation and display the first 5 items, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>y_one_hot = word_num_to_onehot(y_target)
y_one_hot_target = torch.tensor(np.asarray(y_one_hot), device=device)
y_one_hot_target[:5]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.9</h4>
      <pre>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0.]], device='cuda:0')</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To create the <span class="bold">GRU</span> model to predict the next word given a sequence of two input words, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_features = X_train.shape[1]
vocab_size = one_hot_size
embed_size = 128
hidden_size = 128
output_size = one_hot_size
dropout_rate = 0.2

class NextWordGRU(nn.Module):
  def __init__(self, vocab_sz, embed_sz, hidden_sz, output_sz):
    super(NextWordGRU, self).__init__()
    self.embed = nn.Embedding(vocab_sz, embed_sz)
    self.lstm = nn.GRU(input_size=embed_sz, hidden_size=hidden_sz)
    self.dropout = nn.Dropout(dropout_rate)
    self.linear = nn.Linear(hidden_size*num_features, output_sz)

  def forward(self, x_in: torch.Tensor):
    embedded = self.embed(x_in)
    output, _ = self.lstm(embedded)
    output = output.view(output.size(0), -1)
    output = self.dropout(output)
    output = self.linear(output)
    return output</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the <span class="hi-yellow">NextWordGRU</span> model, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>nw_model = NextWordGRU(vocab_size, embed_size, hidden_size, output_size)
nw_model.to(device)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Since the next word can be anyone from the vocabulary, we will create an instance of the <span class="bold">Cross Entropy
        </span> loss function by executing the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>criterion = nn.CrossEntropyLoss()
criterion.to(device)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the <span class="bold">gradient descent</span> function, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>learning_rate = 0.01

optimizer = torch.optim.Adam(nw_model.parameters(), lr=learning_rate)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To implement the iterative training loop for the forward pass to predict, compute the loss, and execute the backward pass
        to adjust the parameters, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_epochs = 51

for epoch in range(1, num_epochs):
  nw_model.train()
  optimizer.zero_grad()
  y_predict = nw_model(X_train)
  loss = criterion(y_predict, y_one_hot_target)
  if epoch % 5 == 0:
    print(f'Next Word Model GRU -> Epoch: {epoch}, Loss: {loss}')
  loss.backward()
  optimizer.step()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>Next Word Model GRU -> Epoch: 5, Loss: 1.2131516933441162
Next Word Model GRU -> Epoch: 10, Loss: 0.0475066713988781
Next Word Model GRU -> Epoch: 15, Loss: 0.006809725426137447
Next Word Model GRU -> Epoch: 20, Loss: 0.001894962857477367
Next Word Model GRU -> Epoch: 25, Loss: 0.0005625554476864636
Next Word Model GRU -> Epoch: 30, Loss: 0.0005398059147410095
Next Word Model GRU -> Epoch: 35, Loss: 0.0002399681106908247
Next Word Model GRU -> Epoch: 40, Loss: 0.00017658475553616881
Next Word Model GRU -> Epoch: 45, Loss: 0.00018268678104504943
Next Word Model GRU -> Epoch: 50, Loss: 0.00012366866576485336</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Given that we have completed the model training, we can bring the model back to the <span class="bold">CPU</span> device by
        executing the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>device_cpu = 'cpu'

nw_model.to(device_cpu)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To define a function to randomly pick a sequence from the training set, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>def pick_ngram_sub_sequence(seq_list):
  idx = random.randint(0, len(seq_list))
  return seq_list[idx], seq_list[idx][:-1]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To evaluate the trained model, we will run about $10$ trials. In each trial, we randomly pick a word sequence, convert the
        selected word sequence to numeric sequence, create a tensor object for the converted numeric sequence, and then predict the
        next word for the selected numeric sequence. To run the trials, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_trials = 10

for i in range(1, num_trials+1):
  ngram_seq_pick, ngram_sub_seq = pick_ngram_sub_sequence(ngram_sequences)
  print(f'Trial #{i} - Picked sequence: {ngram_seq_pick}, Test sub-sequence: {ngram_sub_seq}')
  ngram_sub_seq_num = ngram_sequences_to_numbers([ngram_sub_seq])
  X_test = torch.tensor(ngram_sub_seq_num)
  print(f'Trial #{i} - X_test: {X_test}')
  nw_model.eval()
  with torch.no_grad():
    y_predict = nw_model(X_test)
    idx = torch.argmax(y_predict)
    print(f'Trial #{i} - idx: {idx}, next word: {vocab_words_list[idx]}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>Trial #1 - Picked sequence: ['rich', 'fast', 'enough'], Test sub-sequence: ['rich', 'fast']
Trial #1 - X_test: tensor([[59, 26]])
Trial #1 - idx: 24, next word: enough
Trial #2 - Picked sequence: ['long', 'before', 'he'], Test sub-sequence: ['long', 'before']
Trial #2 - X_test: tensor([[47,  7]])
Trial #2 - idx: 38, next word: he
Trial #3 - Picked sequence: ['day', 'when', 'he'], Test sub-sequence: ['day', 'when']
Trial #3 - X_test: tensor([[17, 71]])
Trial #3 - idx: 38, next word: he
Trial #4 - Picked sequence: ['it', 'open', 'but'], Test sub-sequence: ['it', 'open']
Trial #4 - X_test: tensor([[44, 56]])
Trial #4 - idx: 9, next word: but
Trial #5 - Picked sequence: ['to', 'him', 'that'], Test sub-sequence: ['to', 'him']
Trial #5 - X_test: tensor([[67, 39]])
Trial #5 - idx: 63, next word: that
Trial #6 - Picked sequence: ['began', 'to', 'get'], Test sub-sequence: ['began', 'to']
Trial #6 - X_test: tensor([[ 8, 67]])
Trial #6 - idx: 31, next word: get
Trial #7 - Picked sequence: ['there', 'was', 'once'], Test sub-sequence: ['there', 'was']
Trial #7 - X_test: tensor([[66, 70]])
Trial #7 - idx: 53, next word: once
Trial #8 - Picked sequence: ['to', 'market', 'and'], Test sub-sequence: ['to', 'market']
Trial #8 - X_test: tensor([[67, 48]])
Trial #8 - idx: 3, next word: and
Trial #9 - Picked sequence: ['did', 'he', 'find'], Test sub-sequence: ['did', 'he']
Trial #9 - X_test: tensor([[20, 38]])
Trial #9 - idx: 27, next word: find
Trial #10 - Picked sequence: ['for', 'every', 'day'], Test sub-sequence: ['for', 'every']
Trial #10 - X_test: tensor([[29, 25]])
Trial #10 - idx: 17, next word: day</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p><span class="bold">WOW</span> !!! As can be inferred from the <span class="bold">Output.11</span> above, the success rate
        is $100\%$.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://pytorch.org/docs/stable/index.html" target="_blank"><span class="bold">PyTorch Documentation</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DL-LongShortTerm.html" target="_blank"><span class="bold">Deep Learning - Long Short Term Memory</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DL-RecurrentNN.html" target="_blank"><span class="bold">Deep Learning - Recurrent Neural Network</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DL-VanishingGradient.html" target="_blank"><span class="bold">Deep Learning - The Vanishing Gradient</span></a></p>
    </div>
    <hr class="gen-line-hr" />
    <div>
      <a id="gen-footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
