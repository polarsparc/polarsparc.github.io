<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Deep Learning - The Vanishing Gradient">
    <meta name="subject" content="Deep Learning - The Vanishing Gradient">
    <meta name="keywords" content="artificial-intelligence, deep-learning, neural-network">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Deep Learning - The Vanishing Gradient</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="gen-home"></span></td>
        <td valign="bottom"><span id="gen-home-a"><a id="gen-home-a" href="https://polarsparc.github.io">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="gen-title-div">
      <p>Deep Learning - The Vanishing Gradient</p>
    </div>
    <br />
    <table id="gen-ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">08/06/2023</td>
        </tr>
      </tbody>
    </table>
    <hr class="gen-line-hr" />
    <br/>
    <div id="section-div">
      <p>The Vanishing Gradient</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In using a <span class="bold">Neural Network</span> for a complex use-case, one may simply assume that adding more number
        of <span class="bold">hidden layer</span>s would solve the problem. But, there is an interesting challenge with this naive
        approach - the problem of the <span class="hi-yellow">Vanishing Gradient</span>.</p>
    </div>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/DeepLearning/DeepLearning-1.html" target="_blank"><span class="bold">Part 1</span>
        </a> of the deep learning series, we introduced the concept of the <span class="bold">activation function</span> and in particular
        the <span class="bold">Sigmoid</span> function.</p>
      <p>A <span class="bold">Sigmoid</span> function is a non-linear <span class="bold">activation function</span> that squishes
        its output value to be in the range $[0, 1]$, for any given input value.</p>
      <p>In mathematical terms:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\sigma(x) = \Large{\frac{1}{1 + e^{-x}}}$</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the graph of a <span class="bold">sigmoid activation function</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Sigmoid Function" class="gen-img-cls" src="./images/deep-learning-12.png">
        <div class="gen-img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/DeepLearning/DeepLearning-3.html" target="_blank"><span class="bold">Part 3</span>
        </a> of the deep learning series, we found the <span class="bold">derivative</span> of a <span class="bold">Sigmoid</span>
        function to be:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{d\sigma}{dx}}$ $= \Large{\frac{d}{dx}}$ $\Large{(\frac{1}{1 + e^{-x}})}$ $=\Large{
        \frac{d}{dx}}$ $(1 + e^{-x})^{-1}$ $= \sigma(x).(1 - \sigma(x))$</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the graph of the <span class="bold">derivative</span> of a <span class="bold">sigmoid
        activation function</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Sigmoid Derivative" class="gen-img-cls" src="./images/deep-learning-46.png">
        <div class="gen-img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice from the illustration in Figure.2 above, that the <span class="underbold">MAXIMUM</span> value of the <span class=
        "bold">derivative</span> of a <span class="bold">sigmoid activation function</span> is $0.25$.</p>
    </div>
    <div id="para-div">
      <p>To keep things simple, let us refer to the following illustration of the <span class="bold">neural network</span> with an
        <span class="bold">input layer</span> with two inputs, two <span class="bold">hidden layer</span>s each consisting of two
        <span class="bold">neuron</span>s, and an <span class="bold">output layer</span> with two outputs:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Simple Network" class="gen-img-cls" src="./images/deep-learning-27.png">
        <div class="gen-img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/DeepLearning/DeepLearning-3.html" target="_blank"><span class="bold">Part 3</span>
      </a> of the deep learning series, we went into the gory details of computing the gradients of the <span class="bold">bias</span>es
        and <span class="bold">weight</span>s with respect to the <span class="bold">loss</span> in order to optimize the model.</p>
      <p>Let us consider the gradient of the <span class="bold">weight</span> $W_{1,1}^1$ with respect to the <span class="bold">
        loss</span> $L$.</p>
    </div>
    <div id="para-div">
      <p>In order to adjust the weight partameter $W_{1,1}^1$ during <span class="bold">backpropagation</span>, we need to compute
        the gradient for $\Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$.</p>
      <p>Once we have the gradient for $\Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$, we can adjust the weight $W_{1,1}^1$ as
        follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$W_{1,1}^1 = W_{1,1}^1 -$ $\eta * \Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$ $..... \color
        {red}\textbf{(0)}$</p>
    </div>
    <div id="para-div">
      <p>We can compute the gradient for $\Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$ using the <span class="bold">chain rule
        </span> from <span class="bold">derivative</span>s as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$ $= \Large{\frac{\partial{Z_1^1}}{\partial{W_{1,1}
        ^1}}}$ $.\Large{\frac{\partial{a_1^1}}{\partial{Z_1^1}}}$ $.\Large{\frac{\partial{Z_1^2}}{\partial{a_1^1}}}$ $.\Large{\frac
        {\partial{a_1^2}}{\partial{Z_1^2}}}$ $.\Large{\frac{\partial{Z_1^3}}{\partial{a_1^2}}}$ $.\Large{\frac{\partial{a_1^3}}
        {\partial{Z_1^3}}}$ $.\Large{\frac{\partial{L}}{\partial{a_1^2}}}$ $..... \color{red}\textbf{(1)}$</p>
      <p>The <span class="bold">partial derivative</span> for $\Large{\frac{\partial{a_1^1}}{\partial{Z_1^1}}}$ is as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp; $\Large{\frac{\partial{a_1^1}}{\partial{Z_1^1}}}$ $=\sigma(Z_1^1).(1 - \sigma(Z_1^1))$ $.....
        \color{red}\textbf{(2)}$</p>
      <p>Similarly, we can find the <span class="bold">partial derivative</span>s for $\Large{\frac{\partial{a_1^2}}{\partial{Z_1^2}}}$
        and $\Large{\frac{\partial{a_1^3}}{\partial{Z_1^3}}}$ is as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp; $\Large{\frac{\partial{a_1^2}}{\partial{Z_1^2}}}$ $=\sigma(Z_1^2).(1 - \sigma(Z_1^2))$ $.....
        \color{red}\textbf{(3)}$</p>
      <p>and</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp; $\Large{\frac{\partial{a_1^3}}{\partial{Z_1^3}}}$ $=\sigma(Z_1^3).(1 - \sigma(Z_1^3))$ $.....
        \color{red}\textbf{(4)}$</p>
      <p>Substituting the equations $\color{red}\textbf{(2)}$, $\color{red}\textbf{(3)}$ and $\color{red}\textbf{(4)}$ into equation
        $\color{red}\textbf{(1)}$ and re-arranging, we get the equation:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$ $=\color{blue}[\sigma(Z_1^1).(1-\sigma(Z_1^1))
        .\sigma(Z_1^2).(1-\sigma(Z_1^2)).\sigma(Z_1^3).(1-\sigma(Z_1^3))]$ $.\Large{\frac{\partial{Z_1^1}}{\partial{W_{1,1}^1}}}$
        $.\Large{\frac{\partial{Z_1^2}}{\partial{a_1^1}}}$ $.\Large{\frac{\partial{Z_1^3}}{\partial{a_1^2}}}$ $.\Large{\frac{\partial
        {L}}{\partial{a_1^2}}}$ $..... \color{red}\textbf{(5)}$</p>
    </div>
    <div id="para-div">
      <p>From the Figure.2 above, we know the <span class="bold">maximum</span> value for the <span class="bold">derivative</span>
        of a <span class="bold">sigmoid function</span> is $0.25$.</p>
      <p>Substituting the value of $0.25$ for each of the <span class="bold">derivative</span> terms in the $\color{blue}blue$
        portion of the equation $\color{red}\textbf{(5)}$ above, we arrive at the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$ $=\color{blue}[(0.25).(0.25).(0.25)]$ $.\Large{
        \frac{\partial{Z_1^1}}{\partial{W_{1,1}^1}}}$ $.\Large{\frac{\partial{Z_1^2}}{\partial{a_1^1}}}$ $.\Large{\frac{\partial{
        Z_1^3}}{\partial{a_1^2}}}$ $.\Large{\frac{\partial{L}}{\partial{a_1^2}}}$</p>
      <p>That is:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{\partial{L}}{\partial{W_{1,1}^1}}}$ $=\color{blue}[0.0156]$ $.\Large{\frac{\partial
        {Z_1^1}}{\partial{W_{1,1}^1}}}$ $.\Large{\frac{\partial{Z_1^2}}{\partial{a_1^1}}}$ $.\Large{\frac{\partial{Z_1^3}}{\partial
        {a_1^2}}}$ $.\Large{\frac{\partial{L}}{\partial{a_1^2}}}$</p>
    </div>
    <div id="para-div">
      <p>Notice from the above equation that the gradient is multiplied by <span class="underbold">0.0156</span> for a <span class=
      "bold">neural network</span> with $2$ <span class="bold">hidden layer</span>s and $1$ <span class="bold">output layer</span>.
      If we added $8$ more <span class="bold">hidden layer</span>s, the gradient would be multiplied by a much smaller factor of
      <span class="underbold">0.000000238</span>. In other words, the factor is fast approaching <span class="underbold">ZERO</span>.
      This in effect means the initial <span class="bold">hidden layer</span>s are learning <span class="underbold">NOTHING</span>
      (because the <span class="bold">weight</span>s don't change from equation $\color{red}\textbf{(0)}$) and hence this issue is
      referred as the <span class="underbold">VANISHING GRADIENT</span> problem.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-3.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 3</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-2.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 2</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-1.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 1</span></a></p>
    </div>
    <hr class="gen-line-hr" />
    <div>
      <a id="gen-footer-a" href="https://polarsparc.github.io">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
