<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Deep Learning - Recurrent Neural Network">
    <meta name="subject" content="Deep Learning - Recurrent Neural Network">
    <meta name="keywords" content="artificial-intelligence, deep-learning, neural-network, pytorch, rnn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Deep Learning - Recurrent Neural Network</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="gen-home"></span></td>
        <td valign="bottom"><span id="gen-home-a"><a id="gen-home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="gen-title-div">
      <p>Deep Learning - Recurrent Neural Network</p>
    </div>
    <br />
    <table id="gen-ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">08/13/2023</td>
        </tr>
      </tbody>
    </table>
    <hr class="gen-line-hr" />
    <br/>
    <div id="section-div">
      <p>Introduction</p>
    </div>
    <br/>
    <div id="para-div">
      <p>There are use-cases in the natural language processing domain, such as, predicting the next word in a sentence, or predicting
        the sentiment of a sentence, etc. For these cases, notice that the size of the input is varying and more importantly the order
        of the words is crucial. Hence, the regular <span class="bold">neural network</span> would not work for these cases.</p>
      <p>To process a variable number of ordered sequence of data, we need a different type of <span class="bold">neural network</span>
        and this is where the <span class="hi-yellow">Recurrent Neural Network</span> (or <span class="hi-yellow">RNN</span> for short)
        comes in handy.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Recurrent Neural Network</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="bold">RNN</span> is a <span class="bold">neural network</span> that processes a sequence of inputs $x_1, x_2,
        x_3,...,x_{t-1}, x_t$ at each time step to produce some output.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the high-level abstraction of a <span class="bold">Recurrent Neural Network</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="RNN Cell" class="gen-img-cls" src="./images/recurrent-nn-1.png">
        <div class="gen-img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>$y$ is the output from the <span class="bold">RNN</span> model with $W_y$ as its <span class="bold">weight</span> parameter.
        $x$ is the next input into the model with $W_x$ as its <span class="bold">weight</span> parameter. The parameter $h$ is the
        <span class="hi-yellow">hidden state</span> which captures the historical sequence of inputs that has been processed until
        the current time step with $W_h$ as its <span class="bold">weight</span> parameter. One can think of $h$ as the <span class=
        "bold">memory</span> of the <span class="bold">neural network</span> model.</p>
      <p>The basic <span class="bold">RNN</span> model consists of a single <span class="bold">hidden layer</span> and the whole
        network is encapsulated into a single unit, which often referred to as an <span class="hi-yellow">RNN Cell</span>.</p>
      <p>Notice that the <span class="bold">RNN Cell</span> is also fed the <span class="bold">hidden state</span> in addition to
        the input. This may seem a bit confusing and hence better visualized when the model is unfolded for the entire input sequence.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the <span class="bold">Recurrent Neural Network</span> unfolded over time for a sequence
        of $3$ inputs:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Unfolded RNN" class="gen-img-cls" src="./images/recurrent-nn-2.png">
        <div class="gen-img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The initial <span class="bold">hidden state</span> $h_0$ at the first time step is set to ZERO. At the second time step, the
        <span class="bold">hidden state</span> $h_1$ from the previous time step is used along with the input $x_2$ and so on. This
        pattern repeats itself for the entire input sequence. Hence, the word <span class="hi-yellow">Recurrent</span> to indicate
        the pattern occurs repeatedly.</p>
      <p>Notice that the <span class="bold">weight</span> parameters $W_x$, $W_h$, and $W_y$ are the same for each time step of the
        <span class="bold">neural network</span> model.</p>
    </div>
    <div id="para-div">
      <p>Now, for the question on what magic happens inside the <span class="bold">RNN Cell</span> with the next input $x$ in the
        sequence and the previous value of the <span class="bold">hidden state</span> $h$ to generate the output $y$ ???</p>
      <p>Two <span class="bold">activation function</span>s are used inside the <span class="bold">RNN Cell</span> - the first is
        the <span class="hi-yellow">Hyperbolic Tan</span> (or <span class="hi-yellow">tanh</span>) function and the second is the
        <span class="hi-yellow">Softmax</span> function.</p>
      <p>With that, let us unravel the mathematical computations that happen inside the <span class="bold">RNN Cell</span>:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$h_t = tan(x_t * W_x + h_{t-1} * W_h + b_x)$</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$y_t = softmax(h_t * W_y + b_h)$</p>
    </div>
    <div id="para-div">
      <p>where $t$ is the current time step, $t-1$ is the previous time step, $b_x$ is the <span class="bold">bias</span> for the
        input, and $b_h$ is the <span class="bold">bias</span> for the <span class="bold">hidden state</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the computation graph inside the <span class="bold">RNN Cell</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="RNN Computations" class="gen-img-cls" src="./images/recurrent-nn-3.png">
        <div class="gen-img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">RNN Cell</span> we discussed above is referred to as the <span class="hi-yellow">One-to-One</span>
        network model. In other words, for each input there is one output. This type of a model can be used for the Image
        Classification use-case.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts a <span class="hi-yellow">One-to-Many</span> <span class="bold">RNN</span> network model:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="RNN One-to-Many" class="gen-img-cls" src="./images/recurrent-nn-4.png">
        <div class="gen-img-cap">Figure.4</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In the <span class="bold">One-to-Many</span> <span class="bold">RNN</span> network model, for each input there are more than
        one output. This type of a model can be used for the Image Captioning use-case.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts a <span class="hi-yellow">Many-to-One</span> <span class="bold">RNN</span> network model:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="RNN Many-to-One" class="gen-img-cls" src="./images/recurrent-nn-5.png">
        <div class="gen-img-cap">Figure.5</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In the <span class="bold">Many-to-One</span> <span class="bold">RNN</span> network model, for a sequence of inputs there is
        one output. This type of a model can be used for the Sentiment Classification use-case.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts a <span class="hi-yellow">Many-to-Many</span> <span class="bold">RNN</span> network model:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="RNN Many-to-Many" class="gen-img-cls" src="./images/recurrent-nn-6.png">
        <div class="gen-img-cap">Figure.6</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In the <span class="bold">Many-to-Many</span> <span class="bold">RNN</span> network model, for a sequence of inputs there
        are more than one output. This type of a model can be used for the Language Translation use-case.</p>
    </div>
    <div id="para-div">
      <p>As indicated in the very beginning, the basic <span class="bold">RNN</span> network model uses a single <span class="bold">
        hidden layer</span>. There is nothing preventing one from stacking an <span class="bold">RNN Cell</span> on top of one
        another to create multiple <span class="bold">hidden layer</span>s.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts an <span class="bold">RNN</span> network model with two <span class="bold">hidden layer
        </span>s:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="RNN Multi Layer" class="gen-img-cls" src="./images/recurrent-nn-7.png">
        <div class="gen-img-cap">Figure.7</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the <span class="bold">hidden state</span> $h^1_t$ is associated with the first layer, $h^2_t$ with the second
        layer and so on.</p>
      <p>For a long sequence of inputs, the <span class="bold">Recurrent Neural Network</span> model is susceptible to the dreaded
        <a href="https://polarsparc.github.io/DeepLearning/DL-VanishingGradient.html" target="_blank"><span class="bold">Vanishing
          Gradient</span></a> problem since the unfolding results in a deep network.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on RNN Using PyTorch</p>
    </div>
    <br/>
    <div id="para-div">
      <p>To perform sentiment analysis using the <span class="bold">RNN</span> model, we will be leveraging the
        <a href="https://www.kaggle.com/datasets/d4rklucif3r/restaurant-reviews" target="_blank"><span class="bold">Restaurant
        Reviews</span></a> data set from Kaggle.</p>
    </div>
    <div id="para-div">
      <p>To import the necessary <span class="bold">Python</span> module(s), execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>import numpy as np
import pandas as pd
import nltk
import torch
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import WordPunctTokenizer
from sklearn.model_selection import train_test_split
from torch import nn
from torchmetrics import Accuracy</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Assuming the logged in user is <span class="bold">alice</span>, to set the correct path to the nltk data packages, execute
        the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>nltk.data.path.append("/home/alice/nltk_data")</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Download the <span class="bold">Kaggle Restaurant Reviews</span> data set to the directory /home/alice/txt_data.</p>
    </div>
    <div id="para-div">
      <p>To load the tab-separated restaurant reviews data set into pandas and display the first few rows, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>reviews_df = pd.read_csv('./txt_data/Restaurant_Reviews.tsv', sep='\t')
reviews_df.head()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Reviews Dataframe" class="gen-img-cls" src="./images/recurrent-nn-8.png">
        <div class="gen-img-cap">Figure.8</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the stop words, the word tokenizer, and the lemmatizer, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>stop_words = stopwords.words('english')
word_tokenizer = WordPunctTokenizer()
word_lemmatizer = nltk.WordNetLemmatizer()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To extract all the text reviews as a list of sentences (corpus), execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>reviews_txt = reviews_df.Review.values.tolist()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To cleanse the sentences from the corpus by removing the punctuations, stop words, two-letter words, converting words to
        their roots, collecting all the unique words from the reviews corpus, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>vocabulary_counter = Counter()
cleansed_review_txt = []
for review in reviews_txt:
  tokens = word_tokenizer.tokenize(review)
  alpha_words = [word.lower() for word in tokens if word.isalpha() and len(word) > 2 and word not in stop_words]
  final_words = [word_lemmatizer.lemmatize(word) for word in alpha_words]
  vocabulary_counter.update(final_words)
  cleansed_review = ' '.join(final_words)
  cleansed_review_txt.append(cleansed_review)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To update the original reviews in the reviews pandas dataframe with the cleansed restaurant reviews display the first few
        rows, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>reviews_df['Review'] = cleansed_review_txt
reviews_df.head()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Cleansed Dataframe" class="gen-img-cls" src="./images/recurrent-nn-9.png">
        <div class="gen-img-cap">Figure.9</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>We need an index position for each word in the corpus. For this demonstration, we will use $500$ of the most common words.
        To create a word to index dictionary for the most common words, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>one_hot_size = 500
common_vocabulary = vocabulary_counter.most_common(one_hot_size)
word_to_index = {word:idx for idx, (word, count) in enumerate(common_vocabulary)}</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>We will use the word to index dictionary from above to convert each of the restaurant reviews (in text form) to a one-hot
        encoded vector (of numbers - ones for word present or zeros for absent). To create a list of one-hot encoded vector for each
        of the reviews, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>clean_reviews_txt = reviews_df.Review.values.tolist()
clean_reviews_labels = reviews_df.Liked.values.tolist()
one_hot_reviews_list = []
for review in clean_reviews_txt:
  tokens = word_tokenizer.tokenize(review)
  one_hot_review = np.zeros((one_hot_size), dtype=np.float32)
  for word in tokens:
    if word in word_to_index:
      one_hot_review[word_to_index[word]] = 1
  one_hot_reviews_list.append(one_hot_review)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create the tensor objects for the input and the corresponding labels, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>X = torch.tensor(np.array(one_hot_reviews_list), dtype=torch.float)
y = torch.tensor(np.array(clean_reviews_labels), dtype=torch.float).unsqueeze(dim=1)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create the training and testing data sets, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize variables for the number of input features, the size of the <span class="bold">hidden state</span>, and the
        number of outputs, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>input_size = one_hot_size
hidden_size = 32
no_layers = 1
output_size = 1</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an <span class="bold">RNN</span> model for the reviews sentiment analysis using a single hidden layer, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>class SentimentalRNN(nn.Module):
  def __init__(self, input_sz, hidden_sz, output_sz):
    super(SentimentalRNN, self).__init__()
    self.rnn = nn.RNN(input_size=input_sz, hidden_size=hidden_sz, num_layers=no_layers)
    self.linear = nn.Linear(hidden_size, output_sz)

  def forward(self, x_in: torch.Tensor):
    output, _ = self.rnn(x_in)
    output = self.linear(output)
    return output</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the <span class="hi-yellow">SentimentalRNN</span> model, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>snt_model = SentimentalRNN(input_size, hidden_size, output_size)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Since the sentiments can either be positive or negative (binary), we will create an instance of the <span class="bold">
        Binary Cross Entropy</span> loss function by executing the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>criterion = nn.BCEWithLogitsLoss()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the <span class="hi-yellow">BCEWithLogitsLoss</span> loss function combines both the <span class="bold">sigmoid
        activation</span> function and the <span class="bold">binary cross entropy</span> loss function into a single function.</p>
    </div>
    <div id="para-div">
      <p>To create an instance of the <span class="bold">gradient descent</span> function, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>optimizer = torch.optim.SGD(snt_model.parameters(), lr=0.05)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To implement the iterative training loop for the forward pass to predict, compute the loss, and execute the backward pass
        to adjust the parameters, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_epochs = 1001
for epoch in range(1, num_epochs):
  snt_model.train()
  optimizer.zero_grad()
  y_predict = snt_model(X_train)
  loss = criterion(y_predict, y_train)
  if epoch % 100 == 0:
    print(f'Sentiment Model -> Epoch: {epoch}, Loss: {loss}')
  loss.backward()
  optimizer.step()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>Sentiment Model -> Epoch: 100, Loss: 0.6863620281219482
Sentiment Model -> Epoch: 200, Loss: 0.6723452210426331
Sentiment Model -> Epoch: 300, Loss: 0.653495192527771
Sentiment Model -> Epoch: 400, Loss: 0.6263264417648315
Sentiment Model -> Epoch: 500, Loss: 0.5884767770767212
Sentiment Model -> Epoch: 600, Loss: 0.5385892391204834
Sentiment Model -> Epoch: 700, Loss: 0.4754273295402527
Sentiment Model -> Epoch: 800, Loss: 0.3975992202758789
Sentiment Model -> Epoch: 900, Loss: 0.3084442615509033
Sentiment Model -> Epoch: 1000, Loss: 0.21621127426624298</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To predict the target values using the trained model, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>snt_model.eval()
with torch.no_grad():
  y_predict, _ = snt_model(X_test)
  y_predict = torch.round(y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To display the model prediction accuracy, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>accuracy = Accuracy(task='binary', num_classes=2)

print(f'Sentiment Model -> Accuracy: {accuracy(y_predict, y_test)}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>Sentiment Model -> Accuracy: 0.6399999856948853</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>This concludes the explanation and demonstration of the <span class="bold">Recurrent Neural Network</span> model.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://pytorch.org/docs/stable/index.html" target="_blank"><span class="bold">PyTorch Documentation</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DL-VanishingGradient.html" target="_blank"><span class="bold">Deep Learning - The Vanishing Gradient</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-7.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 7</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-6.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 6</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-5.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 5</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-4.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 4</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-3.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 3</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-2.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 2</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-1.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 1</span></a></p>
    </div>
    <hr class="gen-line-hr" />
    <div>
      <a id="gen-footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
