<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Introduction to Deep Learning - Part 5">
    <meta name="subject" content="Introduction to Deep Learning - Part 5">
    <meta name="keywords" content="artificial-intelligence, deep-learning, neural-network, pytorch">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Introduction to Deep Learning - Part 5</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="gen-home"></span></td>
        <td valign="bottom"><span id="gen-home-a"><a id="gen-home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="gen-title-div">
      <p>Introduction to Deep Learning - Part 5</p>
    </div>
    <br />
    <table id="gen-ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">07/15/2023</td>
        </tr>
      </tbody>
    </table>
    <hr class="gen-line-hr" />
    <br/>
    <div id="section-div">
      <p>Introduction</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/DeepLearning/DeepLearning-4.html" target="_blank"><span class="bold">Introduction
        to Deep Learning - Part 4</span></a> of this series, we started to get our hands dirty in using <span class="bold">PyTorch
        </span>, a popular open source <span class="bold">deep learning</span> framework called <a href="https://pytorch.org" target
        ="_blank"><span class="bold">PyTorch</span></a>.</p>
      <p>In this article, we will continue the journey in further exploring some concepts in <span class="bold">PyTorch</span>.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on PyTorch</p>
    </div>
    <br/>
    <div id="para-div">
      <p>We will now move onto the next section on <span class="bold">Tensor</span>s related to shape manipulation.</p>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Tensor Shape Manipulation</p>
    </div>
    <br/>
    <div id="para-div">
      <p>To change a vector of shape <span class="bold">10</span> into a matrix of shape 2x5, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>vector1 = torch.arange(1, 51, step=5, dtype=torch.float32)
vector1.reshape(2, 5)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>tensor([[ 1.,  6., 11., 16., 21.],
   [26., 31., 36., 41., 46.]])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice that the number of elements from the input tensor <span class="underbold">MUST</span> match that of the reshaped target
        tensor. In other words, the input tensor had <span class="bold">10</span> elements. The reshaped target tensor is of shape 2x5,
        which also equals <span class="bold">10</span> elements.</p>
    </div>
    <div id="para-div">
      <p>To remove all the dimensions from the input tensor that are of dimension <span class="bold">1</span>, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>tensor1 = torch.rand(2, 3, 1)
print(f'tensor1 = {tensor1}, \n torch.squeeze(tensor1) = {torch.squeeze(tensor1)}'
  f'\n dimension = {torch.squeeze(tensor1).shape}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>tensor1 = tensor([[[0.5029],
   [0.2406],
   [0.9118]],

  [[0.9401],
   [0.3970],
   [0.5867]]]), 
torch.squeeze(tensor1) = tensor([[0.5029, 0.2406, 0.9118],
   [0.9401, 0.3970, 0.5867]])
dimension = torch.Size([2, 3])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice the reduction in dimension of the <span class="bold">squeezed</span> input tensor - the last dimension of <span class=
        "bold">1</span> has been removed.</p>
    </div>
    <div id="para-div">
      <p>Now, let us try the <span class="bold">squeeze</span> operation on the input tensor with shape 3x2x3, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>tensor2 = torch.tensor([
  [
    [10, 11, 12],
    [13, 14, 15]
  ],
  [
    [20, 21, 22],
    [23, 24, 25]
  ],
  [
    [30, 31, 32],
    [33, 34, 35]
  ]
], dtype=torch.float)
torch.squeeze(tensor2)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>tensor([[[10., 11., 12.],
   [13., 14., 15.]],

  [[20., 21., 22.],
   [23., 24., 25.]],

  [[30., 31., 32.],
   [33., 34., 35.]]])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice there is <span class="underbold">NO</span> change in the dimension of the <span class="bold">squeezed</span> input
        tensor as there are no dimensions of <span class="bold">1</span>.</p>
    </div>
    <div id="para-div">
      <p>To add a dimension to the specified tensor at the location <span class="bold">0</span>, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>torch.unsqueeze(vector1, dim=0)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>tensor([[ 1.,  6., 11., 16., 21., 25., 31., 36., 41., 46.]])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice the additional dimension in the output tensor.</p>
    </div>
    <div id="para-div">
      <p>To add a dimension to the specified tensor at the location <span class="bold">1</span>, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>torch.unsqueeze(vector1, dim=1)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>tensor([[ 1.],
 [ 6.],
 [11.],
 [16.],
 [21.],
 [25.],
 [31.],
 [36.],
 [41.],
 [46.]])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice the additional dimension added to each of the elements from the input tensor.</p>
    </div>
    <div id="para-div">
      <p>We will now move onto the next section on <span class="bold">Tensor</span>s related to automatic differentiation.</p>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Tensor Autograd</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="hi-yellow">autograd</span> feature in <span class="bold">PyTorch</span> provides support for an easy and
        efficient computation of <span class="bold">derivatives</span> (or <span class="bold">gradients</span>) over a complex
        computational graph. This is very <span class="underbold">CRUCIAL</span> for the implementation of the <span class="bold">
        backpropagation</span> algorithm in a <span class="bold">Neural Network</span>, where one has to deal with the computation
        of the <span class="bold">partial derivatives</span> using the <span class="bold">chain rule</span> and propagating the
        <span class="bold">gradients</span> backwards to adjust the various parameters. The <span class="bold">gradients</span> are
        automatically computed by <span class="bold">autograd</span> in <span class="bold">PyTorch</span>, relieving the users from
        that responsibility.</p>
    </div>
    <div id="para-div">
      <p>Let us look at a very simple example of computing the <span class="bold">derivative</span> $\Large{\frac{dz}{dx}}$ for the
        mathematical equation shown below:</p>
    </div>
    <div id="para-div">
        <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = x^2$</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;$z = y + 2$</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the computational graph for the above indicated mathematical equation:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Computational Graph" class="gen-img-cls" src="./images/deep-learning-35.png">
        <div class="gen-img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To compute the <span class="bold">derivative</span> $\Large{\frac{dz}{dx}}$, one needs to use the <span class="bold">chain
        rule</span> as shown below:</p>
    </div>
    <div id="para-div">
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{dz}{dx}}$ $= \Large{\frac{dz}{dy}}$ $. \Large{\frac{dy}{dx}}$</p>
      <p>We know:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{dz}{dy}}$ $= \Large{\frac{d}{dy}}$ $(y + 2) = 1$</p>
      <p>and:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{dy}{dx}}$ $= \Large{\frac{d}{dx}}$ $x^2 = 2.x$</p>
      <p>Therefore:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{dz}{dx}}$ $= \Large{\frac{dz}{dy}}$ $. \Large{\frac{dy}{dx}}$ $= 2.x$</p>
      <p>When $x = 2$:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{dz}{dx}}$ $= 2.x = 4$</p>
    </div>
    <div id="para-div">
      <p>To create the required tensors to represent the above mathematical equation, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
z = y + 2</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="hi-blue">requires_grad</span> option that is specified when creating the tensor $x$ in the code snippet
        above indicates that we desired the computation of the <span class="bold">gradient</span> for $x$.</p>
    </div>
    <div id="para-div">
      <p>To initiate the backward pass execution (<span class="bold">backpropagation</span>) and automatically compute and set the
        <span class="bold">gradient</span> for tensor $x$, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>z.backward()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To display the details about the tensor $x$, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>print(f'x value = {x.data}, x gradient = {x.grad}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>x value = 2.0, x gradient = 4.0</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The tensor property <span class="hi-blue">data</span> allows one to access the underlying data once the tensor is created,
        while the tensor property <span class="hi-blue">grad</span> allows access to the computed <span class="bold">gradient</span>
        after the backward pass execution.</p>
    </div>
    <div id="para-div">
      <p>To access the <span class="bold">gradient</span> for the tensor $y$, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>print(f'TAKE-1 :: y value = {y.data}, y gradient = {y.grad}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>TAKE-1 :: y value = 4.0, y gradient = None</pre>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="bold">Hmm</span> !!! Why is the <span class="bold">gradient</span> for the tensor $y$ ??? By default, tensors
        created without the <span class="bold">requires_grad</span> option is considered a non-leaf tensor and its <span class="bold">
        gradient</span> is not preserved during the backward pass execution. To change this behavior for debugging purpose, one can
        invoke the <span class="hi-green">retain_grad()</span> method on the non-leaf tensor.</p>
    </div>
    <div id="para-div">
      <p>To re-create the required tensors for the above mathematical equation and execute the backward pass, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
y.retain_grad()
z = y + 2
z.backward()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, to access the <span class="bold">gradient</span> for the tensor $y$, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>print(f'TAKE-2 :: y value = {y.data}, y gradient = {y.grad}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>TAKE-2 :: y value = 4.0, y gradient = 1.0</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>We will now move onto the final section on <span class="bold">PyTorch</span> model building basics.</p>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>PyTorch Model Basics</p>
    </div>
    <br/>
    <div id="para-div">
      <p>All the ingredients needed for building a <span class="bold">neural network</span> can be found in the <span class="bold">
        PyTorch</span> namespace <span class="hi-yellow">torch.nn</span>. To build a <span class="bold">neural network</span> model
        in <span class="bold">PyTorch</span>, one *<span class="underbold">MUST</span>* create a model subclass, which inherits from
        the <span class="bold">PyTorch</span> base class <span class="hi-blue">nn.Module</span>.</p>
    </div>
    <div id="para-div">
      <p>Let us consider a very simple linear equation $y = m.X + c$, where $X$ is the input feature, $m$ is the slope and $c$ is
        the intercept. This is basically a simple <span class="bold">Linear Regression</span> problem.</p>
    </div>
    <div id="para-div">
      <p>To import the necessary <span class="bold">Python</span> modules, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import torch
from torch import nn</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Let the slope $m = 0.3$ and let the intercept $c = 0.5$.</p>
    </div>
    <div id="para-div">
      <p>To create a sample dataset with the input $X$ and target $y$, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>X = torch.arange(0, 1, 0.03).unsqueeze(dim=1)
y = 0.3 * X + 0.5</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create the training and testing samples, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the plot for the training set:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Line Training" class="gen-img-cls" src="./images/deep-learning-36.png">
        <div class="gen-img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create a <span class="bold">Linear Regression</span> for our simple use-case, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_features = 1
num_target = 1

class LinearRegressionModel(nn.Module):
  def __init__(self):
    super(LinearRegressionModel, self).__init__()
    self.linear_layer = nn.Linear(num_features, num_target)
  
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.linear_layer(x)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The layer <span class="hi-yellow">nn.Linear</span> performs a linear transformation $y = W^T.x + b$ on the incoming data.
        It takes in as input the number of input features and the number of targets. Based on the specified values, it automatically
        creates the associated <span class="bold">weights</span> and <span class="bold">bias</span> for the layer.</p>
      <p>The method <span class="hi-blue">forward(self, x)</span> defined in the base class <span class="bold">nn.Module</span> is
        *<span class="underbold">CRITICAL</span>* and *<span class="underbold">MUST</span>* be implemented - it defines the forward
        pass of the <span class="bold">neural network</span>.</p>
    </div>
    <div id="para-div">
      <p>To create an instance of the <span class="hi-yellow">LinearRegressionModel</span> and display its internal state, execute
        the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>lr_model = LinearRegressionModel()
lr_model.state_dict()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.9</h4>
      <pre>OrderedDict([('linear_layer.weight', tensor([[-0.6039]])),
('linear_layer.bias', tensor([-0.0994]))])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the <span class="bold">loss</span> function, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>criterion = nn.L1Loss()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The class <span class="hi-blue">nn.L1Loss</span> computes the mean absolute error $\vert predicted - actual \vert$.</p>
    </div>
    <div id="para-div">
      <p>To create an instance of the <span class="bold">gradient descent</span> function, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>optimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The class <span class="hi-blue">torch.optim.SGD(parameters, learning_rate)</span> implements the necessary logic to adjust
        the parameters (<span class="bold">weights</span> and <span class="bold">bias</span>) adjustment using the <span class="bold">
        gradient descent</span> algorithm.</p>
    </div>
    <div id="para-div">
      <p>To implement the iterative training loop (<span class="bold">epoch</span>) in order to execute the forward pass to predict,
        compute the loss, and execute the backward pass to adjust the parameters, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_epochs = 351

for epoch in range(1, num_epochs):
  lr_model.train()
  optimizer.zero_grad()
  y_predict = lr_model(X_train)
  loss = criterion(y_predict, y_train)
  if epoch % 50 == 0:
    print(f'Epoch: {epoch}, Loss: {loss}')
  loss.backward()
  optimizer.step()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>Epoch: 50, Loss: 0.4174691438674927
Epoch: 100, Loss: 0.12142442911863327
Epoch: 150, Loss: 0.0914655402302742
Epoch: 200, Loss: 0.06548042595386505
Epoch: 250, Loss: 0.039495326578617096
Epoch: 300, Loss: 0.013510189950466156
Epoch: 350, Loss: 0.004773879423737526</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice the call to <span class="hi-grey">optimizer.zero_grad()</span> - this is to zero the gradients computed from the
        earlier iteration.</p>
      <p>Also, the to <span class="hi-grey">optimizer.step()</span> is what performs the adjustments to the parameters using the
        <span class="bold">gradient descent</span> algorithm.</p>
    </div>
    <div id="para-div">
      <p>To display the values of the model parameters (the internal state of the model), execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>lr_model.state_dict()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>OrderedDict([('linear_layer.weight', tensor([[0.2925]])),
('linear_layer.bias', tensor([0.4961]))])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice how close the model parameters are to our original values of $m$ and $c$.</p>
    </div>
    <div id="para-div">
      <p>Before we wrap up, let us look at a simple <span class="bold">Binary Classification</span> problem. We will create and use
        synthetic data for this use-case. In addition, we will perform all the operations using the <span class="bold">GPU</span>.</p>
    </div>
    <div id="para-div">
      <p>To import the necessary <span class="bold">Python</span> module to create the synthetic data, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>from sklearn.datasets import make_blobs
from sklearn.metrics import accuracy_score</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create the synthetic data for the <span class="bold">Binary Classification</span> with two input features, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_samples = 500

np_Xc, np_yc = make_blobs(num_samples, n_features=2, centers=2, cluster_std=2.5, random_state=101)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To use the compute resource in a device agnostic way, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>device = 'cuda' if torch.cuda.is_available() else 'cpu'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>If a <span class="bold">CUDA</span> based <span class="bold">GPU</span> is available, it will be leveraged. Else, it will
        default to the system <span class="bold">CPU</span>.</p>
    </div>
    <div id="para-div">
      <p>To create the tensor dataset on the <span class="bold">GPU</span> device, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>Xc = torch.tensor(np_Xc, dtype=torch.float, device=device)
yc = torch.tensor(np_yc, dtype=torch.float, device=device).unsqueeze(dim=1)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create the training and testing samples, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=101)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the plot for the training set:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Binary Training" class="gen-img-cls" src="./images/deep-learning-37.png">
        <div class="gen-img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create a <span class="bold">Binary Classification</span> for our simple use-case, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_features_2 = 2
num_target_2 = 1

class BinaryClassificationModel(nn.Module):
  def __init__(self):
    super(BinaryClassificationModel, self).__init__()
    self.hidden_layer = nn.Sequential(
      nn.Linear(num_features_2, num_target_2),
      nn.Sigmoid()
    )
  
  def forward(self, cx: torch.Tensor) -> torch.Tensor:
    return self.hidden_layer(cx)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The object <span class="hi-yellow">nn.Sequential</span> represents a container for layers in a <span class="bold">neural
        network</span>. The incoming data is forwarded thourgh each of the layers in this container. In this example, the incoming
        data is passed through a <span class="bold">nn.Linear</span> layer, followed by an activation layer <span class="hi-yellow">
        nn.Sigmoid</span>.</p>
    </div>
    <div id="para-div">
      <p>To create an instance of the <span class="hi-yellow">BinaryClassificationModel</span> on the <span class="bold">GPU</span>
        device and display its internal state, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>bc_model = BinaryClassificationModel()
bc_model.to(device)
bc_model.state_dict()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.12</h4>
      <pre>OrderedDict([('hidden_layer.0.weight',
  tensor([[-0.5786,  0.5476]], device='cuda:0')),
  ('hidden_layer.0.bias', tensor([-0.2978], device='cuda:0'))])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the <span class="bold">loss</span> function on the <span class="bold">GPU</span> device, execute
        the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>criterion_2 = nn.BCELoss()
criterion_2.to(device)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The class <span class="hi-blue">nn.BCELoss</span> computes the binary cross entropy error.</p>
    </div>
    <div id="para-div">
      <p>To create an instance of the <span class="bold">gradient descent</span> function, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>optimizer_2 = torch.optim.SGD(bc_model.parameters(), lr=0.05)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To implement the iterative training loop (<span class="bold">epoch</span>) in order to execute the forward pass to predict,
        compute the loss, and execute the backward pass to adjust the parameters, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>num_epochs_2 = 1001
for epoch in range(1, num_epochs_2):
  bc_model.train()
  optimizer_2.zero_grad()
  yc_predict = bc_model(Xc_train)
  loss = criterion_2(yc_predict, yc_train)
  if epoch % 50 == 0:
    print(f'Epoch: {epoch}, Loss: {loss}')
  loss.backward()
  optimizer_2.step()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.13</h4>
      <pre>Epoch: 100, Loss: 0.1626521497964859
Epoch: 150, Loss: 0.14181235432624817
Epoch: 200, Loss: 0.12650831043720245
Epoch: 250, Loss: 0.11473483592271805
Epoch: 300, Loss: 0.10540398955345154
Epoch: 350, Loss: 0.09782855957746506
Epoch: 400, Loss: 0.09155341982841492
Epoch: 450, Loss: 0.08626670390367508
Epoch: 500, Loss: 0.08174819499254227
Epoch: 550, Loss: 0.07783830165863037
Epoch: 600, Loss: 0.0744188204407692
Epoch: 650, Loss: 0.07140025496482849
Epoch: 700, Loss: 0.0687137320637703
Epoch: 750, Loss: 0.06630539149045944
Epoch: 800, Loss: 0.06413248926401138
Epoch: 850, Loss: 0.062160711735486984
Epoch: 900, Loss: 0.06036214530467987
Epoch: 950, Loss: 0.058713868260383606
Epoch: 1000, Loss: 0.05719691514968872</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To display the values of the model parameters (the internal state of the model), execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>bc_model.state_dict()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.14</h4>
      <pre>OrderedDict([('hidden_layer.0.weight',
  tensor([[-0.5084, -0.4961]], device='cuda:0')),
  ('hidden_layer.0.bias', tensor([-2.9599], device='cuda:0'))])</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>To predict the target values using the trained <span class="bold">Binary Classification</span> model, execute the following
        code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>bc_model.eval()
with torch.no_grad():
  y_predict_2 = bc_model(Xc_test)
  y_predict_2 = torch.round(y_predict_2)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that in order to use the model for prediction, the model needs to be in an evaluation mode. This is achieved by invoking
        the method <span class="hi-blue">bc_model.eval()</span> followed by the use of the context <span class="hi-blue">with
        torch.no_grad()</span>.</p>
    </div>
    <div id="para-div">
      <p>To display the model prediction accuracy, execute the following code snippet:</p>
    </div>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>print(f'Accuracy: {accuracy_score(y_predict_2.cpu(), yc_test.cpu())}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.15</h4>
      <pre>Accuracy: 0.99</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>Note that the <span class="bold">PyTorch</span> models demonstrated above were simple linear models.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://pytorch.org/docs/stable/index.html" target="_blank"><span class="bold">PyTorch Documentation</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-4.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 4</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-3.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 3</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-2.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 2</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DeepLearning-1.html" target="_blank"><span class="bold">Introduction to Deep Learning - Part 1</span></a></p>
    </div>
    <hr class="gen-line-hr" />
    <div>
      <a id="gen-footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
