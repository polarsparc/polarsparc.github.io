<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Deep Learning - Understanding the Transformer Models">
    <meta name="subject" content="Deep Learning - Understanding the Transformer Models">
    <meta name="keywords" content="artificial-intelligence, deep-learning, llm, neural-network, transformers">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Deep Learning - Understanding the Transformer Models</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Deep Learning - Understanding the Transformer Models</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">12/10/2023</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Introduction</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Of late, there is a lot of <span class="bold">BUZZ</span> around <span class="hi-yellow">Large Language Model</span>s (or
        <span class="hi-vanila">LLM</span>s for short) with the release of <span class="hi-vanila">ChatGPT</span> from <span class=
        "bold">OpenAI</span>. In short, <span class="bold">ChatGPT</span> is an advanced form of a <span class="bold">ChatBot</span>
        that engages in any type of conversation with the users (referred to as <span class="hi-grey">Conversational AI</span>).</p>
      <p>In the article <a href="https://polarsparc.github.io/DeepLearning/DL-Seq2Seq.html" target="_blank"><span class="bold">Deep
        Learning - Sequence-to-Sequence Model</span></a>, we introduced the concept of the <span class="bold">Encoder-Decoder</span>
        model for language translation. The same model can also be used to accomplish the task of a <span class="bold">ChatBot</span>.
        So what is all the excitement around the <span class="bold">LLM</span> models (aka <span class="bold">ChatGPT</span>) ???</p>
      <p>Given that the amount of data from the <span class="bold">Internet</span> is in <span class="underbold">ZETTABYTES</span>,
        there are two challenges with the <span class="bold">Seq2Seq</span> model:</p>
      <ul id="gen-sqr-ul">
        <li><p>It will not be able to retain all the contextual information for such vast amounts of data</p></li>
        <li><p>It will be super <span class="underbold">SLOW</span> to train, given it deals with one word at a time</p></li>
      </ul>
      <p>To address these challenges, Google published the <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank"><span class
        ="bold">Attention Is All You Need</span></a> paper in <span class="bold">2017</span>, which led to the creation of the next
        breed of language model called the <span class="hi-green">Transformer</span> model.</p>
      <p>In this article, we will unravel the mystery behind <span class="bold">Transformer</span> models, so we have a better
        intuition of how they work.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Inside the Transformer Model</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the architecture of a <span class="bold">Transformer</span> model from the paper:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Transformer Model" class="img-cls" src="./images/transformer-01.png">
        <div class="img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>A <span class="bold">Transformer</span> model has the <span class="bold">Encoder</span> and the <span class="bold">Decoder
        </span> blocks similar to the <span class="bold">Seq2Seq</span> model, with the ability to process all the words in a given
        sentence in parallel (versus sequentially one at a time). However, the most <span class="underbold">CRUCIAL</span> block is
        the one highlighted in purple in <span class="bold">Figure.1</span> above - <span class="hi-purple">Attention</span> (also
        referred to as <span class="hi-purple">Self-Attention</span>).</p>
    </div>
    <div id="para-div">
      <p>For better understanding, we will simplify the architecture shown in <span class="bold">Figure.1</span> above to the one as
        shown below:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Simple Transformer" class="img-cls" src="./images/transformer-02.png">
        <div class="img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To get an intuition on <span class="bold">Attention</span>, let us consider the following two sentences:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Attention Sentences" class="img-cls" src="./images/transformer-03.png">
        <div class="img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>A human brain can look at the two sentences in <span class="bold">Figure.3</span> above and quickly fill in the word <span
        class="hi-blue">dress</span> for sentence <span class="bold">1</span> and the word <span class="hi-orange">Lemonade</span>
        for sentence <span class="bold">2</span>.</p>
      <p>How is the human brain able to do this ???</p>
      <p>For sentence <span class="bold">1</span>, the human brain focussed or paid <span class="underbold">attention</span> to the
        word <span class="hi-blue">wore</span> to guess the missing word as <span class="hi-blue">dress</span>.</p>
      <p>Similarly, for sentence <span class="bold">2</span>, the human brain paid <span class="underbold">attention</span> to the
        word <span class="hi-orange">drink</span> to guess the missing word as <span class="hi-orange">Lemonade</span>.</p>
      <p>In other words, the human brain is able to selectively <span class="underbold">focus</span> on some words in a sentence to
        figure the context of the sentence and fill in the blanks. Intuitively, this is how the human <span class="bold">Attention
        </span> mechanism works.</p>
    </div>
    <div id="para-div">
      <p>Let us look at another longer sentence as shown below:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Longer Sentence" class="img-cls" src="./images/transformer-04.png">
        <div class="img-cap">Figure.4</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>There are two <span class="underbold">IT</span> in the long sentence above. What are they referring to ???</p>
      <p>The human brain very quickly will be able to associate the first <span class="bold">IT</span> with the word <span class=
        "hi-green">grass</span> and the second <span class="bold">IT</span> with the word <span class="hi-blue">mower</span>.</p>
      <p>Given the longer sentence, the human brain pays <span class="underbold">attention</span> to only few words to determine
        the context and relationships between words.</p>
      <p>In the <span class="bold">Seq2Seq</span> model, every word is processed irrespective of its importance. This is one of the
        reasons, why it performed poorly with large text corpus.</p>
      <p>But, how do we mimic the <span class="underbold">attention</span> mechanism like the human brain in a neural network ???</p>
    </div>
    <div id="para-div">
      <p>From the paper, how <span class="bold">Attention</span> can be computed is shown using the following illustration:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Attention Paper" class="img-cls" src="./images/transformer-05.png">
        <div class="img-cap">Figure.5</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>For clarity, we will redraw the <span class="bold">Attention</span> block shown in <span class="bold">Figure.5</span> above
        with the following simplified illustration:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Attention Simplified" class="img-cls" src="./images/transformer-06.png">
        <div class="img-cap">Figure.6</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In mathematical terms, the <span class="bold">Attention</span> is computed as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$Attention(Q, K, V) = softmax(\Large{\frac{Q.K^T}{\sqrt{d_k}}})\normalsize{.V}$</p>
    </div>
    <div id="para-div">
      <p>Let us unravel and understand the <span class="bold">Attention</span> mechanism computation from <span class="bold">Figure.6
        </span> above.</p>
      <p>As hinted above, the human brain tries to find the relationships and similarities between words and determine the context
        of a sentence based on <span class="underbold">STRONG</span> similarities. This is where the aid of <span class="hi-grey">Word
        Embeddings</span> and <span class="bold">Word Similarities</span> come in handy.</p>
      <p>In the article <a href="https://polarsparc.github.io/xhtml/DL-Word2Vec.html" target="_blank"><span class="bold">Deep Learning
        - Word Embeddings with Word2Vec</span></a>, we introduced the concept of <span class="bold">Word Embeddings</span> and the
        use of the <span class="hi-grey">Vector Dot Product</span> to determine the closeness or similarity between words. In short,
        when two words are similar to each other in meaning or relationship, the <span class="underbold">dot product</span> of their
        <span class="bold">embeddings</span> will be greater than zero.</p>
      <p>The question one may have at this point - how does the <span class="underbold">dot product</span> of the <span class="bold">
        embeddings</span> mimic <span class="underbold">attention</span> mechanism ???</p>
    </div>
    <div id="para-div">
      <p>To explain the computation of <span class="bold">Attention</span>, let us consider the following simple sentence:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Simple Sentence" class="img-cls" src="./images/transformer-07.png">
        <div class="img-cap">Figure.7</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The matrices $Q$, $K$, and $V$ are all the <span class="underbold">SAME</span> and nothing more than the <span class="bold">
        word embedding vector</span> for each of the words in the sentence stacked as rows, as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Q and K" class="img-cls" src="./images/transformer-08.png">
        <div class="img-cap">Figure.8</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that we have arbitrarily chose a <span class="bold">word embedding</span> of size $4$ and arbitrarily assigned the
        values for the corresponding <span class="bold">word embedding vector</span>.</p>
    </div>
    <div id="para-div">
      <p>The first step in finding <span class="bold">Attention</span> is to compute the <span class="underbold">dot product</span>
        $Q.K^T$. What this is trying to determine is how similar are each of the words to the other words in the sentence.</p>
      <p>In other words, for each word $q_i$ from the matrix $Q$ (represents all the words of the sentence), we are finding the
        <span class="bold">word similarity</span> with each of the words $k_j$ from the matrix $K$ (representation of all the words
        from the sentence). Rather than performing the operations in this sequential manner, we are using the matrix representation
        $Q.K^T$ for efficiency (vectorized operation).</p>
      <p>One way to think of this - it is similar to making a <span class="bold">QUERY</span> (hence represented as $Q$) with all
        the <span class="bold">KEYS</span> (hence represented as $K$) to determine how similar each of the words are and returning a
        vector of scores. The result of this operation is what is referred to as the <span class="hi-yellow">Similarity Scores</span>
        (or <span class="hi-purple">Attention Scores</span>).</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the outcome of the <span class="underbold">dot product</span> $Q.K^T$.:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Dot Product" class="img-cls" src="./images/transformer-09.png">
        <div class="img-cap">Figure.9</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the higher values indicate more similarity between words and hence depicted in darker shades.</p>
    </div>
    <div id="para-div">
      <p>The second step is to normalize or scale the <span class="bold">Attention Scores</span>. Note that a typical <span class=
        "bold">word embedding vector</span> is at a minimum of size $64$ elements and hence the <span class="underbold">dot product
        </span> values tend be large. During the model training, large values have a negative impact on backpropagation (gradient
        computations) and hence need to be normalized or scaled. Per the paper, the scaling must be done by a factor, which is the
        $SquareRoot(vector\ size)$. In our example, the vector size is $4$ and hence the scaling factor of $2$.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the outcome of the <span class="underbold">scaled dot product</span> $\Large{\frac{Q.K^T}
        {\sqrt{d_k}}}$:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Scaled Dot Product" class="img-cls" src="./images/transformer-10.png">
        <div class="img-cap">Figure.10</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The third step is to normalize the <span class="bold">Attention Scores</span> to be all positive numbers in the range of
        $[0, 1]$. Note that we did not pick any negative numbers for our arbitrary <span class="bold">word embedding vector</span>s.
        In the real world, however, the <span class="bold">embedding vector</span>s do include negative numbers as well. In order
        to normalize all the positive/negative number to be positive in the range of $[0, 1]$, we use the $SoftMax$ function.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the outcome of the <span class="underbold">softmax</span> $softmax(\Large{\frac{Q.K^T}
        {\sqrt{d_k}}})$:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="SoftMax Matrix" class="img-cls" src="./images/transformer-11.png">
        <div class="img-cap">Figure.11</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The end result after the third step above, is the normalized and scaled <span class="bold">Attention Scores</span>.</p>
      <p>The final computation is the <span class="underbold">dot product</span> of the matrix $V$ (represents all the words of the
        sentence) with the just determined <span class="bold">Attention Scores</span>. The effect of this operation is like finding
        the weighted sum of all the words with respect to the other words in the matrix $V$. In essense, it is making the words of
        the sentence <span class="underbold">CONTEXT AWARE</span> using some kind of weights aka the <span class="bold">Attention
        Scores</span>.</p>
      <p>In effect, the <span class="bold">Attention</span> mechanism is <span class="underbold">AMPLIFYING</span> words that are
        related and <span class="underbold">FILTERING</span> out words that add no value.</p>
    </div>
    <div id="para-div">
      <p>The observant readers may be thinking - there are no parameters (or <span class="underbold">Weights</span>) in the <span
        class="bold">Attention</span> block of <span class="bold">Figure.6</span> above that could be adjusted during the model
        training ???</p>
      <p>What we illustrated in the <span class="bold">Figure.6</span> above was for simplicity and better understanding. In order
        to make the <span class="bold">Attention</span> block learn during the model training, we introduce linear neural network
        layers to the <span class="bold">Attention</span> block of <span class="bold">Figure.6</span> above, which results in the
        following illustration:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Attention Learning" class="img-cls" src="./images/transformer-12.png">
        <div class="img-cap">Figure.12</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Hope this clarifies the crux of the idea behind the concept of <span class="underbold">Self-Attention</span> mechanism !!!</p>
    </div>
    <div id="para-div">
      <p>Moving along, the next part of the paper to unravel is the <span class="hi-red">Multi-Head Attention</span> from the block
        in <span class="bold">Figure.5</span> above.</p>
      <p>To determine the <span class="bold">Attention Scores</span>, we started with the <span class="bold">embedding vector</span>s
        of the words in the sentence and fed them to the <span class="bold">Attention</span> block of <span class="bold">Figure.12
        </span> above. What if we fed the <span class="bold">embedding vector</span>s of the words in the sentence to many instances
        of the <span class="bold">Attention</span> block of <span class="bold">Figure.12</span> above, with the thought that each of
        the instances will learn quite different patterns ???</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts three instances of the <span class="bold">Attention</span> block of <span class="bold">
        Figure.12</span> above, each processing the <span class="bold">word embedding vector</span>s, with the belief that they will
        each learn different set of patterns:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Multi-Head Attention" class="img-cls" src="./images/transformer-13.png">
        <div class="img-cap">Figure.13</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">Attention Scores</span> from the three instances can then be combined and fed into a linear neural
        network layer to produce the final <span class="bold">Attention Scores</span>. The idea is that the linear neural network
        will learn from multiple perspectives to produce more refined final scores.</p>
    </div>
    <div id="para-div">
      <p>The final puzzle piece from the <span class="bold">Transformer</span> model in <span class="bold">Figure.2</span> above is
        the <span class="hi-blue">Positional Encoding</span>.</p>
      <p>In the <span class="bold">Seq2Seq</span> model, the input tokens (words) from a sentence are processed in sequential order
        and hence did not have the need to indicate the position of the words. However, in the <span class="bold">Transformer</span>
        model, the input tokens (words) from a sentence are processed in parallel. This poses an interesting challenge.</p>
    </div>
    <div id="para-div">
      <p>To understand the situation, consider the following two sentences:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Identical Words" class="img-cls" src="./images/transformer-14.png">
        <div class="img-cap">Figure.14</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To two sentences indicated above have identical words, except that they have totally different meaning.</p>
      <p>In order to ensure the position of the words in a sentence are preserved during parallel processing, it somehow needs to
        be encoded into the <span class="bold">word embedding vector</span>s.</p>
      <p>One naive approach could be to use the index position of each word, converting it to a vector and adding it to the <span
        class="bold">word embedding vector</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows this naive approach:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Index Position" class="img-cls" src="./images/transformer-15.png">
        <div class="img-cap">Figure.15</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>This naive approach completely distorts the <span class="bold">embedding vector</span> and may break the similarity between
        words. In fact, look at the <span class="bold">embedding</span> for the word <span class="underbold">the</span> in the above
        illustration - it has been completely changed.</p>
      <p>Clearly, this naive approach will <span class="underbold">NOT</span> work !!!</p>
      <p>We need a way to keep the position values bounded, such that, they do not significantly change the <span class="bold">word
        embedding</span>, but at the same time have infinite values (to support very long sentences). This is why the researcher of
        the paper chose to use the <span class="underbold">sine</span> and <span class="underbold">cosine</span> functions. These
        functions are bounded in the range $[-1, 1]$ and extend to infinite position.</p>
      <p>One challenge with these two <span class="bold">Trigonometric</span> functions - they repeat at regular frequencies. To fix
        the issue, two things can be done - first change the <span class="bold">frequency</span> to a lower value and second alternate
        between the two <span class="bold">Trigonometric</span> functions.</p>
      <p>In mathematical terms, the <span class="underbold">sine</span> function can be written as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = a.sin(b.x)$</p>
      <p>and</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = a.cos(b.x)$</p>
      <p>where the variable $a$ controls the <span class="underbold">height</span> (or <span class="bold">amplitude</span>) and the
        variable $b$ controls the <span class="underbold">horizontal stretch</span> (or <span class="bold">frequency</span>). The
        lower the value of $b$, the farther it is stretched.</p>
    </div>
    <div id="para-div">
      <p>The following graphs illustrate the effect of variable $a$ and $b$:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Sine and Cosine" class="img-cls" src="./images/transformer-16.png">
        <div class="img-cap">Figure.16</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To ensure the encoded values do not repeat the variable $b$ is set as $\Large{\frac{pos}{10000^{2i/d}}}$</p>
      <p>where:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$pos$ is the position of the word starting at $1$ for the first word, $2$ for the second and so on</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$i$ is the array index of the element within the <span class="bold">embedding vector</span>. If the
        <span class="bold">embedding vector</span> is of size $4$, then it has $4$ array elements and the index $i$ ranges from $0$
        to $3$</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$d$ is the size of the <span class="bold">embedding vector</span></p>
    </div>
    <div id="para-div">
      <p>The <span class="bold">Positional Encoding</span> vector for each word will have the same size as the <span class="bold">
        word embedding vector</span> with the even array index positions encoded using the <span class="underbold">sine</span>
        function and the odd array index positions encoded using the <span class="underbold">cosine</span> function.</p>
      <p>In mathematical terms:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;For even array index positions: $sin(\Large{\frac{pos}{10000^{2i/d}}}\normalsize{)}$</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;For odd array index positions: $cos(\Large{\frac{pos}{10000^{2i/d}}}\normalsize{)}$</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows how the array elements of the <span class="bold">Positional Encoding</span> vector are
        encoded:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="img-outer-div">
        <img alt="Position Encoding" class="img-cls" src="./images/transformer-17.png">
        <div class="img-cap">Figure.17</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>With this, we have all the blocks from <span class="bold">Figure.2</span> unpacked and revealed.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank"><span class="bold">Attention Is All You Need</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DL-Seq2Seq.html" target="_blank"><span class="bold">Deep Learning - Sequence-to-Sequence Model</span></a></p>
      <p><a href="https://polarsparc.github.io/DeepLearning/DL-Word2Vec.html" target="_blank"><span class="bold">Deep Learning - Word Embeddings with Word2Vec</span></a></p>
    </div>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
