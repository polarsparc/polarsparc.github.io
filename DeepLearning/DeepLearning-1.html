<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Introduction to Deep Learning - Part 1">
    <meta name="subject" content="Introduction to Deep Learning - Part 1">
    <meta name="keywords" content="artificial-intelligence, deep-learning, neural-network">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Introduction to Deep Learning - Part 1</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="gen-home"></span></td>
        <td valign="bottom"><span id="gen-home-a"><a id="gen-home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="gen-title-div">
      <p>Introduction to Deep Learning - Part 1</p>
    </div>
    <br />
    <table id="gen-ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">05/07/2023</td>
        </tr>
      </tbody>
    </table>
    <hr class="gen-line-hr" />
    <br/>
    <div id="gen-step-div">
      <p>Introduction</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="hi-yellow">Artificial Intelligence</span> has progressed by leaps and bounds over the past few years and hence
        a lot of buzz around <span class="hi-yellow">Deep Learning</span> (aka <span class="hi-yellow">Deep Neural Networks</span>).</p>
      <p><span class="bold">Deep Learning</span> is a subset of <span class="bold">Machine Learning</span>, which tries to mimic the
        way the <span class="hi-yellow">Neurons</span> are connected and process information in the human brain.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the structure of a <span class="bold">neuron</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Neuron Structure" class="gen-img-cls" src="./images/deep-learning-01.png">
        <div class="gen-img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The human brain is a highly interconnected network of about <span class="underbold">100 Billion</span> <span class="bold">
        neuron</span>s, which have about <span class="underbold">100 Trillion</span> connections between them.</p>
      <p>A <span class="bold">neuron</span> is the basic building block of <span class="underbold">LEARNING</span> in the human brain
        and it consists of a single cell called the <span class="hi-yellow">Nucleus</span>, which is connected to a tree-like nerve
        extensions referred to as the <span class="hi-yellow">Dendrites</span>.</p>
      <p><span class="bold">Dendrites</span> receives signals from other connected <span class="bold">neuron</span>s, which in-turn
        receive signals through the different sensory channels - sight, smell, sound, taste, touch, etc.</p>
      <p>The <span class="bold">nucleus</span> behaves like a simple processor, acting on the input signal and generating an output
        signal. The output signal is sent to other connected <span class="bold">neuron</span>s via the transmission channel referred
        to as the <span class="hi-yellow">Axon</span>.</p>
      <p>The <span class="bold">axon</span> terminates at the <span class="hi-yellow">Synatic Terminals</span>, which in-turn connect
        to other <span class="bold">neuron</span>s.</p>
      <p>The <span class="bold">synatic terminals</span> act like the <span class="bold">memory</span> in the human brain.</p>
      <p>In other words, a <span class="bold">neuron</span> acts like a simple computational unit and a billion of these connected
        <span class="bold">neuron</span>s is what creates the <span class="underbold">AMAZING</span> human brain.</p>
    </div>
    <div id="para-div">
      <p>The idea of <span class="bold">deep neural networks</span> is to mimic the <span class="bold">neuron network</span> of the
        human brain.</p>
      <p>In the following sections, we will start to put together the components that will lead to a <span class="bold">Deep Neural
        Network</span> - an artificial human brain.</p>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Perceptron</p>
    </div>
    <br/>
    <div id="para-div">
      <p>A <span class="hi-yellow">Perceptron</span> is an artificial equivalent of a <span class="bold">neuron</span>. At a very
        high-level, it consumes multiple inputs and generates a binary (<span class="bold">0 or 1</span>) output.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the structure of a <span class="bold">perceptron</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Perceptron Structure" class="gen-img-cls" src="./images/deep-learning-02.png">
        <div class="gen-img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the illustration in <span class="bold">Figure.2</span> above, we see the <span class="bold">perceptron</span> taking
        inputs <span class="bold">perceptron</span> $x_1$ through $x_n$. Each of the inputs $x_i$ have an associated <span class=
        "underbold">WEIGHT</span> $w_i$. Think of a weight as the importance (or influence) the associated input has on the output.
        The single adjustment term $b$ is the <span class="underbold">BIAS</span>, similar to the intercept term in an equation for
        a line. The term $\Large{\Sigma}$ is the symbol for summation. The output from the summation $\Sigma$ is passed through a
        function $\sigma$, referred to as the <span class="hi-yellow">Activation Function</span>, to produce the final output $y$.</p>
      <p>In other words, the <span class="bold">perceptron</span> takes the sum of weighted inputs along with the bias $b$ and sends
        it through a transformation function $\sigma$:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = \Large{\sigma}$$(\sum_{i=1}^n w_i.x_i + b)$$..... \color{red}\textbf{(1)}$</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;OR, using the matrix notation:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = \Large{\sigma}$$(W.x^T + b)$$..... \color{red}\textbf{(2)}$</p>
      <p>to produce the final binary output $y = [0 | 1]$.</p>
      <p>If the value of $\sigma(x) \le 0$, the output is $0$ <span class="bold">AND</span> if the value of $\sigma(x) \gt 0$, the
        output is $1$. By default, the <span class="bold">activation function</span> $\sigma$ from a <span class="bold">perceptron
        </span> produces a binary value of $0 | 1$. Therefore, the <span class="bold">activation function</span> is often referred
        to as the <span class="hi-yellow">Step Function</span> (hence a step symbol inside $f$ in the illustration above).</p>
    </div>
    <div id="para-div">
      <p>In the illustration <span class="bold">Figure.2</span> above, we show the two functions - the summation $\Sigma$ and the
        <span class="bold">activation function</span> $\sigma$ for clarity. In practice, the <span class="bold">bias</span> $b$,
        the summation operation $\Sigma$ and the <span class="bold">activation function</span> $\sigma$ are all represented as a
        single empty circle as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Perceptron Structure" class="gen-img-cls" src="./images/deep-learning-03.png">
        <div class="gen-img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above paragraph(s), one can clearly deduce that a <span class="bold">perceptron</span> simply behaves like a
        <span class="hi-yellow">Linear Classifier</span>.</p>
    </div>
    <div id="para-div">
      <p>To understand the behavior of a single <span class="bold">perceptron</span>, let us look at a simple binary classification
        dataset with two features $X1$ and $X2$. The following illustration depicts the scatter plot of the simple dataset with two
        classes:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Simple Dataset" class="gen-img-cls" src="./images/deep-learning-04.png">
        <div class="gen-img-cap">Figure.4</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Using a single <span class="bold">perceptron</span> with inputs $X1$ and $X2$, one can train the model using the simple
        dataset and predict the target class on future data values.</p>
    </div>
    <div id="para-div">
      <p>After the training, the single <span class="bold">perceptron</span> would assume the following values for the two weights
        $W1$ and $W2$ associated with the input and the <span class="bold">bias</span> $b$:</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;$W1 = -0.46$</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;$W2 = 0.49$</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;$b = 0.0$</p>
        <p>&nbsp;&nbsp;&nbsp;&nbsp;The mathematical equation for the <span class="bold">perceptron</span> would be: $-0.46.X1 +
          0.49.X2$</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the scatter plot of our simple binary classification dataset, along with the decision
        boundary of the <span class="bold">perceptron</span> that demarcates the two classes:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Single Perceptron" class="gen-img-cls" src="./images/deep-learning-05.png">
        <div class="gen-img-cap">Figure.5</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>When $X1$ is smaller and $X2$ is larger, the <span class="bold">activation function</span> $\sigma$ outputs a $1$, tagging
        the data as a <span class="hi-red">RED</span> point. Similarly, when $X1$ is larger and $X2$ is smaller, the <span class="bold">
        activation function</span> $\sigma$ outputs a $0$, tagging the data as a <span class="hi-green">GREEN</span> point.</p>
    </div>
    <div id="para-div">
      <p>Once again, from the above example, one can confirm that a <span class="bold">perceptron</span> behaves like a simple linear
        classifier.</p>
    </div>
    <div id="para-div">
      <p>What if there are more than two inputs to the <span class="bold">perceptron</span> ???</p>
    </div>
    <div id="para-div">
      <p>From the mathematical equation $\color{red}\textbf{(2)}$ above, it is clear <span class="underbold">NO MATTER</span> how
        many inputs a single <span class="bold">perceptron</span> takes, it will always behave like a simple linear classifier.</p>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Neural Network</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Let us now look at another simple dataset with multple classes using two features $X1$ and $X2$. The following illustration
        depicts the scatter plot of the multi-class dataset:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Multi Class Dataset" class="gen-img-cls" src="./images/deep-learning-06.png">
        <div class="gen-img-cap">Figure.6</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Given that a single <span class="bold">perceptron</span> behaves like a simple linear classifier, it would not be able to
        properly classify the above dataset. This implies we will need multiple <span class="bold">perceptron</span>s to handle the
        above dataset.</p>
    </div>
    <div id="para-div">
      <p>As indicated earlier, a <span class="bold">perceptron</span> is an artificial equivalent of a <span class="bold">neuron
        </span> in the human brain. We know the human brain is a complex network of <span class="bold">neuron</span>s. So, how does
        one create an equvalent network of <span class="bold">perceptron</span>s ???</p>
    </div>
    <div id="para-div">
      <p>One could connect a series of <span class="bold">perceptron</span>s (in layers) starting from the input till the output as
        shown in the illustration below:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Neural Network" class="gen-img-cls" src="./images/deep-learning-07.png">
        <div class="gen-img-cap">Figure.7</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The network of <span class="bold">perceptron</span>s depicted in the illustration above is what is referred to as the
        <span class="hi-yellow">Deep Neural Network</span> (or simply a <span class="hi-yellow">Neural Network</span>).</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the core aspects of a <span class="bold">Neural Network</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Layers of Neural Network" class="gen-img-cls" src="./images/deep-learning-08.png">
        <div class="gen-img-cap">Figure.8</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Each column of <span class="bold">perceptron</span>s is referred to as a <span class="hi-yellow">Layer</span>.</p>
      <p>The first layer receives the input and is known as the <span class="hi-yellow">Input Layer</span>. The received input will
        then pass through a series of one or more layers for further processing, which are referred to as the <span class="hi-yellow">
        Hidden Layer</span>(s). The last layer produces the final output and is known as the <span class="hi-yellow">Output Layer
        </span>.</p>
      <p>Each of the input lines (connections) into a <span class="bold">perceptron</span> has an associated <span class="bold">
        weight</span> and each of the <span class="bold">perceptron</span>s have an associated <span class="bold">bias</span> value.
        The individual <span class="bold">weight</span>s and the <span class="bold">bias</span> of the <span class="bold">perceptron
        </span>s is often referred to as the <span class="hi-yellow">Parameter</span>s.</p>
    </div>
    <div id="para-div">
      <p>To accurately segregate multi-class dataset shown above, we will need a <span class="bold">neural network</span> with one
        <span class="bold">hidden layer</span> consisting of five <span class="bold">perceptron</span>s as shown in the illustration
        below:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Multi-Class Neural Network" class="gen-img-cls" src="./images/deep-learning-09.png">
        <div class="gen-img-cap">Figure.9</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>For input $X1$, notice the associated weights from $W_{1,1}$ through $W_{1,5}$ for each of the five <span class="bold">
        perceptron</span>s respectively. Similarly, for input $X2$, the associated weights are from $W_{2,1}$ through $W_{2,5}$.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the scatter plot of our simple multi-class dataset, along with the decision boundary as
        identified by the <span class="bold">neural network</span>:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Neural Network" class="gen-img-cls" src="./images/deep-learning-10.png">
        <div class="gen-img-cap">Figure.10</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the decision boundary illustration above, once again we observe the network of <span class="bold">perceptron</span>s
        behave like a simple linear classifier.</p>
    </div>
    <br/>
    <div id="gen-step-div">
      <p>Activation Function</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the human brain, based on the situation and context, certain neurons get <span class="underbold">ACTIVATED</span> and
        fire signals. Similary, the <span class="bold">activation function</span> in a <span class="bold">neural network</span> tries
        to emulate that behavior. As an example, when one encounters a frog walking along a trail, the human brain just focus on the
        frog, ignoring other things in the surrounding.</p>
    </div>
    <div id="para-div">
      <p>The default <span class="bold">activation function</span> $\sigma$ associated with a <span class="bold">perceptron</span>
        is a binary <span class="bold">step function</span>, which outputs discrete values - a $0$ if $\sigma(x) \le 0$ and a $1$
        if $\sigma(x) \gt 0$. This forces the <span class="bold">perceptron</span> to behave like a binary classifier.</p>
      <p>In other words, a <span class="bold">perceptron</span>, by default, would ouput either a $0$ or a $1$. This is like saying
        lighter shades of grey are same as white and the darker shades of grey are black as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Binary Step Function" class="gen-img-cls" src="./images/deep-learning-11.png">
        <div class="gen-img-cap">Figure.11</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The binary <span class="bold">step function</span> works well for linear binary classification use-cases, but many other
        real-world cases demonstrate a non-linear behavior, which implies continuous values (just like the various shades of grey).</p>
      <p>In order to handle many of the real-world use-cases, one can change the default <span class="bold">activation function</span>
        to the other functions as listed below:</p>
    </div>
    <div id="para-div">
      <ul id="gen-sqr-ul">
        <li>
          <p><span class="hi-yellow">Identify Function</span> :: a linear <span class="bold">activation function</span> that produces
            an output $x$ which is identical to the input $x$.</p>
          <p>In mathematical terms:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = \sigma(x) = x$</p>
        </li>
        <li>
          <p><span class="hi-yellow">Sigmoid Function</span> :: a non-linear <span class="bold">activation function</span>, also
            referred to as the <span class="hi-yellow">Logistic Function</span>, produces an output in the range of $0$ to $1$ for
            a given input.</p>
          <p>In mathematical terms:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = \sigma(x) = \Large{\frac{1}{1 + e^{-x}}}$</p>
          <p>The following illustration shows the graph of a <span class="bold">sigmoid activation function</span>:</p>
          <br/>
          <div id="para-div">
            <div id="gen-img-outer-div">
              <img alt="Sigmoid Function" class="gen-img-cls" src="./images/deep-learning-12.png">
              <div class="gen-img-cap">Figure.12</div>
            </div>
          </div>
          <br/>
        </li>
        <li>
          <p><span class="hi-yellow">Tanh Function</span> :: a non-linear <span class="bold">activation function</span>, also
            referred to as the <span class="hi-yellow">Hyperbolic Tangent Function</span>, produces an output in the range of $-1$ to
            $1$ for a given input.</p>
          <p>In mathematical terms:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = \sigma(x) = \Large{\frac{e^x - e^{-x}}{e^x + e^{-x}}}$</p>
          <p>The following illustration shows the graph of a <span class="bold">tanh activation function</span>:</p>
          <br/>
          <div id="para-div">
            <div id="gen-img-outer-div">
              <img alt="Tanh Function" class="gen-img-cls" src="./images/deep-learning-13.png">
              <div class="gen-img-cap">Figure.13</div>
            </div>
          </div>
          <br/>
        </li>
        <li>
          <p><span class="hi-yellow">Rectified Linear Unit Function</span> :: a linear <span class="bold">activation function</span>,
            also referred to as the <span class="hi-yellow">ReLU Function</span>, produces an output of $max(0, x)$ for a given input
            x.</p>
          <p>In mathematical terms:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = \sigma(x) = max(0, x)$</p>
          <p>The following illustration shows the graph of a <span class="bold">ReLU activation function</span>:</p>
          <br/>
          <div id="para-div">
            <div id="gen-img-outer-div">
              <img alt="ReLU Function" class="gen-img-cls" src="./images/deep-learning-14.png">
              <div class="gen-img-cap">Figure.14</div>
            </div>
          </div>
          <br/>
        </li>
      </ul>
    </div>
    <br/>
    <hr class="gen-line-hr" />
    <div>
      <a id="gen-footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
