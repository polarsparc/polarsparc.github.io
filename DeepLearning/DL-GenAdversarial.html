<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Deep Learning - Generative Adversarial Network">
    <meta name="subject" content="Deep Learning - Generative Adversarial Network">
    <meta name="keywords" content="artificial-intelligence, deep-learning, neural-network, pytorch, gan">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Deep Learning - Generative Adversarial Network</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-chtml.js"></script>
    <!-- script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script -->
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="gen-home"></span></td>
        <td valign="bottom"><span id="gen-home-a"><a id="gen-home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="gen-title-div">
      <p>Deep Learning - Generative Adversarial Network</p>
    </div>
    <br />
    <table id="gen-ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">05/25/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="gen-line-hr" />
    <br/>
    <div id="section-div">
      <p>Generative Adversarial Network</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="hi-yellow">Generative Adversarial Network</span> (or <span class="hi-vanila">GAN</span> for short) is a pair
        of neural network models that are engaged in an adversarial competition with each other just like a counterfeiter and a cop.
        One model is referred to as the <span class="hi-red">Generator</span> and the other model is referred to as the <span class
        ="hi-green">Discriminator</span>. The Generator is trying to generate synthetic data samples that are indistiguisable from
        the real data samples, while the Discriminator is trying to distinguish between the synthetic samples and the real samples.</p>
    </div>
    <div id="para-div">
      <p>The Generator model is reponsible for generating new data samples from a given domain space by taking as input some random
        noise from a Gaussian (or Normal) distribution and producing synthetic data sample with the goal that the generated sample
        is as close as possible to a sample from the real domain space.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the high-level abstraction of the Generator model:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Generator Model" class="gen-img-cls" src="./images/gen-adversarial-1.png">
        <div class="gen-img-cap">Figure.1</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The Discriminator model, on the other hand, takes as input the generated sample as well as the real samples from the domain
        space with the goal of distinguishing between the two samples (generated versus real).</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the high-level abstraction of the Discriminator model:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Discriminator Model" class="gen-img-cls" src="./images/gen-adversarial-2.png">
        <div class="gen-img-cap">Figure.2</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In short, the Generator model and the Discriminator model are engaged in an adversarial cat and mouse game, such that the
        Generator tries to improve its ability to generate realistic synthetic data samples, while the Discriminator model tries to
        improve its ability to distinguish between the synthetic samples and the real samples.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the high-level abstraction of GAN with both the Generator and the Discriminator models:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="GAN Model" class="gen-img-cls" src="./images/gen-adversarial-3.png">
        <div class="gen-img-cap">Figure.3</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the synthetic data samples generated by GAN could apply to any domain such as audio, image, or text. However,
        GAN models are predominantly used for image generation.</p>
      <p>As an example, every image of a person from this site <a href="https://thispersondoesnotexist.com/" target="_blank">
        <span class="bold">Person Does Not Exist</span></a> is generated by GAN.</p>
      <p>Images are typically generated and classified using the <a href="https://polarsparc.github.io/DeepLearning/DL-Convolutional.html"
        target="_blank"><span class="bold">Convolutional Neural Network</span></a>, and hence it is sometimes referred to as the
        <span class="hi-vanila">Deep Convolutional GAN</span>.</p>
    </div>
    <div id="para-div">
      <p>Let $G$ represent the Generator model and $D$ the Discrimator model. Also, let represent $z$ the random input generated
        from the Gaussian (or Normal) distribution $p_z$ and $x$ the real data sample from the distribution $p_{data}$.</p>
      <p>According to the <a href="https://arxiv.org/pdf/1406.2661v1" target="_blank"><span class="bold">GAN Paper</span></a>, the
        Generator model $G$ and the Discriminator model $D$ are engaged in a min-max game over their value function $V(G, D)$ which
        is represented as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$min_G \space max_D \space \space V(G, D) = \mathbb{E}_{x \sim p_{data}}[log(D(x))] + \mathbb{E}_
        {z \sim p_z}[log(1 - D(G(z)))]$ $..... \color{red}\textbf{(1)}$</p>
      <p>where $\mathbb{E}$ is the expected value and $log$ is the natural logarithm.</p>
      <p>In other words, the goal of the Generator model $G$ is to minimize the value function $V(G, D)$, while the objective of the
        Discriminator model $D$ is to maximize the value function $V(G, D)$.</p>
    </div>
    <div id="para-div">
      <p>Let us unpack the above equation $\color{red}\textbf{(1)}$ for a better understanding.</p>
      <p>The first part of the above equation $\color{red}\textbf{(1)}$ is the term $\bbox[YellowGreen,2pt]{\mathbb{E}_{x \sim p_{data}}
        [log(D(x))]}$, which represents the expected value of the Discrimator model $D$ predicting the real data sample $D(x)$ as a
        real data sample with high confidence.</p>
      <p>Note that this term does <span class="underbold">NOT</span> involve the Generator model $G$. The Discriminator model $D$
        wants the value of this term (the expected value or weighted average predictions) to be <span class="underbold">HIGH</span>.</p>
    </div>
    <div id="para-div">
      <p>The second part of the above equation $\color{red}\textbf{(1)}$ is the term $\bbox[Salmon,2pt]{\mathbb{E}_{z \sim p_z}[log
        (1 - D(G(z)))]}$, which represents the expected value of the Discrimator model $D$ predicting the synthetic data sample
        $D(G(z))$ as a fake data sample with high confidence.</p>
      <p>Note that this term <span class="underbold">DOES</span> involve the Generator model $G$. The Discriminator model $D$ wants
        the value of this term (the expected value or weighted average predictions) to be as <span class="underbold">LOW</span> as
        possible (predicting it is a fake data sample), while the Generator model $G$ wants the value of this term to be as <span
        class="underbold">HIGH</span> as possible (to fool the Discriminator model into believing that the data samples are real).</p>
    </div>
    <div id="para-div">
      <p>In short, putting the two terms together, as represented in the above equation $\color{red}\textbf{(1)}$, the Discriminator
        model $D$ wants to maximize the value of the above equation $\color{red}\textbf{(1)}$, while the Generator model $G$ wants
        to minimize the value of the above equation $\color{red}\textbf{(1)}$.</p>
    </div>
    <div id="para-div">
      <p>Taking a step back, we can observe that the Discriminator model $D$ is nothing more than a <span class="hi-vanila">Binary
        Classifier</span> outputting a label of <span class="hi-red">0</span> for a fake synthetic data samples and a label of <span
        class="hi-green">1</span> real data samples.</p>
      <p>Given that there are two classes (fake or real) for this classification problem, one can use the <span class="hi-vanila">
        Binary Cross Entropy</span> (or <span class="hi-vanila">BCE</span> for short) loss function for training the GAN model.</p>
      <p>The BCE loss function is defined as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = - \frac{1}{N} \sum (y_i.log(\hat{y_i}) + (1 - y_i).log(1 - \hat{y_i}))$ $..... \color{red}
        \textbf{(2)}$</p>
      <p>where $y$ is the true label (ground truth) and $\hat{y}$ is the predicted label.</p>
      <p>For better understanding, let us consider just one data sample and simplify the above equation $\color{red}\textbf{(2)}$
        as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = y.log(\hat{y}) + (1 - y).log(1 - \hat{y})$ $..... \color{red}\textbf{(3)}$</p>
      <p>For real data sample, $y = 1$, $D(x) = \hat{y}$, and the loss function $L$ in equation $\color{red}\textbf{(3)}$ reduces
        to the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = log(\hat{y}) = log(D(x))$ $..... \color{red}\textbf{(4)}$</p>
      <p>For fake synthetic data sample, $y = 0$, $D(G(z)) = \hat{y}$, and the loss function $L$ in equation $\color{red}\textbf{(3)}$
        reduces to the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = log(1- \hat{y}) = log(1 - D(G(z)))$ $..... \color{red}\textbf{(5)}$</p>
      <p>Combining equations $\color{red}\textbf{(4)}$ and $\color{red}\textbf{(5)}$ from above, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = log(D(x)) + log(1 - D(G(z)))$ $..... \color{red}\textbf{(6)}$</p>
      <p>Training is never done on a single data sample, but rather on a batch of data samples. Also, note that the data samples
        come from a data sample distribution - $p_{data}$ for the real data samples and $p_z$ for the synthetic data samples.</p>
      <p>For training over a batch of real data samples, the loss function $L$ in equation $\color{red}\textbf{(4)}$ becomes the
        average loss and can be represented as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = \bbox[YellowGreen,2pt]{\sum p_{data}(x).log(D(x))}$ $..... \color{red}\textbf{(7)}$</p>
      <p>Similarly, for training over a batch of random data samples, the loss function $L$ in equation $\color{red}\textbf{(5)}$
        becomes the average loss and can be represented as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = \bbox[Salmon,2pt]{\sum p_z(z).log(1 - D(G(z)))}$ $..... \color{red}\textbf{(8)}$</p>
      <p>Combining equations $\color{red}\textbf{(7)}$ and $\color{red}\textbf{(8)}$ from above, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L = \bbox[YellowGreen,2pt]{\sum p_{data}(x).log(D(x))} + \bbox[Salmon,2pt]{\sum p_z(z).log(1 -
        D(G(z)))}$ $..... \color{red}\textbf{(9)}$</p>
      <p>Notice that the loss function $L$ in equation $\color{red}\textbf{(9)}$ above is similar to the value function $V(G, D)$
        in the equation $\color{red}\textbf{(1)}$ way above.</p>
    </div>
    <div id="para-div">
      <p>Now, that we have an intuition of how the GAN model works, the training of the GAN model results in the following steps:</p>
      <ul id="blue-disc-ul">
        <li><p>Sample a batch of $m$ random data samples from the distribution $p_z$</p></li>
        <li><p>Sample a batch of $m$ real data samples from the distribution $p_{data}$</p></li>
        <li><p>Update the model weights of the Discriminator model $\theta_D$ by ascending the stochastic gradient (maximize) the
          loss function $L$ in equation $\color{red}\textbf{(6)}$ with respect to $D$. That is: $\partial D / \partial \theta_D \:
          (\frac{1}{m} \sum_{i=1}^{m} [log(D(x_i)) + log(1 - D(G(z_i)))])$</p></li>
        <li><p>Update the model weights of the Generator model $\theta_G$ by descending the stochastic gradient (minimize) the
          loss function $L$ in equation $\color{red}\textbf{(6)}$ with respect to $G$. That is: $\partial G / \partial \theta_G \:
          (\frac{1}{m} \sum_{i=1}^{m} [log(1 - D(G(z_i)))])$. Note the first term becomes $0$ since it does not depend on $G$</p></li>
      </ul>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on GAN Using PyTorch</p>
    </div>
    <br/>
    <div id="para-div">
      <p>To import the necessary <span class="bold">Python</span> module(s), execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>import torch
import numpy as np
import matplotlib.pyplot as plt
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch import nn
from torch import optim
from torch.utils.data import DataLoader</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>In order to ensure reproducibility, we need to set the <span class="bold">seed</span> to a constant value by executing the
        following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>seed_value = 3
torch.manual_seed(seed_value)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To download the MNIST data to the directory <span class="hi-blue">./data</span>, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>data_dir = './data'
img_sz = 64
batch_sz = 64

compose = transforms.Compose([
    transforms.Resize(img_sz),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

mnist_dataset = datasets.MNIST(root=data_dir, download=True, train=True, transform=compose)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The above code snippet will <span class="underbold">ONLY</span> download the MNIST dataset once.</p>
    </div>
    <div id="para-div">
      <p>To create an instance of the dataset loader and access the samples in batches, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>mnist_loader = DataLoader(dataset=mnist_dataset, batch_size=batch_sz, shuffle=True)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize the variable(s) common to both the Discriminator and Generator models, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre># Common to Generator and Discriminator
c_bias = False</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize the variables for our Discriminator model hyperparameters, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre># For Discriminator - 5 Convolutional Layers
d_leaky_relu_slope = 0.2
d_filter_sz = 64

# Discriminator Layer 1 Convolution
d_l1_in_ch = 1 # Layer 1 Convolution No of channels
d_l1_c_num_k = d_filter_sz # Layer 1 Convolution No of Kernels
d_l1_kernel_sz = 4 # Layer 1 Kernel Size
d_l1_stride = 2 # Layer 1 Convolution Stride
d_l1_padding = 1 # Layer 1 Convolution Padding

# Discriminator Layer 2 Convolution
d_l2_in_ch = d_l1_c_num_k # Layer 2 Convolution No of channels
d_l2_c_num_k = d_filter_sz * 2 # Layer 2 Convolution No of Kernels
d_l2_kernel_sz = 4 # Layer 2 Kernel Size
d_l2_stride = 2 # Layer 2 Convolution Stride
d_l2_padding = 1 # Layer 2 Convolution Padding

# Discriminator Layer 3 Convolution
d_l3_in_ch = d_l2_c_num_k # Layer 3 Convolution No of channels
d_l3_c_num_k = d_filter_sz * 4 # Layer 3 Convolution No of Kernels
d_l3_kernel_sz = 4 # Layer 3 Kernel Size
d_l3_stride = 2 # Layer 3 Convolution Stride
d_l3_padding = 1 # Layer 3 Convolution Padding

# Discriminator Layer 4 Convolution
d_l4_in_ch = d_l3_c_num_k # Layer 4 Convolution No of channels
d_l4_c_num_k = d_filter_sz * 8 # Layer 4 Convolution No of Kernels
d_l4_kernel_sz = 4 # Layer 4 Kernel Size
d_l4_stride = 2 # Layer 4 Convolution Stride
d_l4_padding = 1 # Layer 4 Convolution Padding

# Discriminator Layer 5 Convolution
d_l5_in_ch = d_l4_c_num_k # Layer 5 Convolution No of channels
d_l5_c_num_k = 1 # Layer 5 Convolution No of Kernels
d_l5_kernel_sz = 4 # Layer 5 Kernel Size
d_l5_stride = 1 # Layer 5 Convolution Stride
d_l5_padding = 0 # Layer 5 Convolution Padding</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To define our Discriminator model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    self.layers = nn.Sequential(
      # Layer 1
      nn.Conv2d(in_channels=d_l1_in_ch,
                out_channels=d_l1_c_num_k,
                kernel_size=d_l1_kernel_sz,
                stride=d_l1_stride,
                padding=d_l1_padding,
                bias=c_bias
      ),
      nn.LeakyReLU(d_leaky_relu_slope, inplace=True),
      # Layer 2
      nn.Conv2d(in_channels=d_l2_in_ch,
                out_channels=d_l2_c_num_k,
                kernel_size=d_l2_kernel_sz,
                stride=d_l2_stride,
                padding=d_l2_padding,
                bias=c_bias
      ),
      nn.BatchNorm2d(d_l2_c_num_k),
      nn.LeakyReLU(d_leaky_relu_slope, inplace=True),
      # Layer 3
      nn.Conv2d(in_channels=d_l3_in_ch,
                out_channels=d_l3_c_num_k,
                kernel_size=d_l3_kernel_sz,
                stride=d_l3_stride,
                padding=d_l3_padding,
                bias=c_bias
      ),
      nn.BatchNorm2d(d_l3_c_num_k),
      nn.LeakyReLU(d_leaky_relu_slope, inplace=True),
      # Layer 4
      nn.Conv2d(in_channels=d_l4_in_ch,
                out_channels=d_l4_c_num_k,
                kernel_size=d_l4_kernel_sz,
                stride=d_l4_stride,
                padding=d_l4_padding,
                bias=c_bias
      ),
      nn.BatchNorm2d(d_l4_c_num_k),
      nn.LeakyReLU(d_leaky_relu_slope, inplace=True),
      # Layer 5
      nn.Conv2d(in_channels=d_l5_in_ch,
                out_channels=d_l5_c_num_k,
                kernel_size=d_l5_kernel_sz,
                stride=d_l5_stride,
                padding=d_l5_padding,
                bias=c_bias
      ),
      nn.Sigmoid()
    )

  def forward(self, x):
    x = self.layers(x)
    return x</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize the variables for our Generator model hyperparameters, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre># For Generator - 5 Convolutional Layers
g_latent_sz = 100
g_filter_sz = 64

# Generator Layer 1 Convolution
g_l1_in_ch = g_latent_sz # Layer 1 Convolution No of channels
g_l1_c_num_k = g_filter_sz * 8 # Layer 1 Convolution No of Kernels
g_l1_kernel_sz = 4 # Layer 1 Kernel Size
g_l1_stride = 1 # Layer 1 Convolution Stride
g_l1_padding = 0 # Layer 1 Convolution Padding

# Generator Layer 2 Convolution
g_l2_in_ch = g_l1_c_num_k # Layer 2 Convolution No of channels
g_l2_c_num_k = g_filter_sz * 4 # Layer 2 Convolution No of Kernels
g_l2_kernel_sz = 4 # Layer 2 Kernel Size
g_l2_stride = 2 # Layer 2 Convolution Stride
g_l2_padding = 1 # Layer 2 Convolution Padding

# Generator Layer 3 Convolution
g_l3_in_ch = g_l2_c_num_k # Layer 3 Convolution No of channels
g_l3_c_num_k = g_filter_sz * 2 # Layer 3 Convolution No of Kernels
g_l3_kernel_sz = 4 # Layer 3 Kernel Size
g_l3_stride = 2 # Layer 3 Convolution Stride
g_l3_padding = 1 # Layer 3 Convolution Padding

# Generator Layer 4 Convolution
g_l4_in_ch = g_l3_c_num_k # Layer 4 Convolution No of channels
g_l4_c_num_k = g_filter_sz # Layer 4 Convolution No of Kernels
g_l4_kernel_sz = 4 # Layer 4 Kernel Size
g_l4_stride = 2 # Layer 4 Convolution Stride
g_l4_padding = 1 # Layer 4 Convolution Padding

# Generator Layer 5 Convolution
g_l5_in_ch = g_l4_c_num_k # Layer 5 Convolution No of channels
g_l5_c_num_k = 1 # Layer 5 Convolution No of Kernels
g_l5_kernel_sz = 4 # Layer 5 Kernel Size
g_l5_stride = 2 # Layer 5 Convolution Stride
g_l5_padding = 1 # Layer 5 Convolution Padding</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To define our Generator model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>class Generator(nn.Module):
  def __init__(self):
      super(Generator, self).__init__()
      self.layers = nn.Sequential(
        # Layer 1
        nn.ConvTranspose2d(in_channels=g_l1_in_ch,
                            out_channels=g_l1_c_num_k,
                            kernel_size=g_l1_kernel_sz,
                            stride=g_l1_stride,
                            padding=g_l1_padding,
                            bias=c_bias
        ),
        nn.BatchNorm2d(g_l1_c_num_k),
        nn.ReLU(inplace=True),
        # Layer 2
        nn.ConvTranspose2d(in_channels=g_l2_in_ch,
                            out_channels=g_l2_c_num_k,
                            kernel_size=g_l2_kernel_sz,
                            stride=g_l2_stride,
                            padding=g_l2_padding,
                            bias=c_bias
        ),
        nn.BatchNorm2d(g_l2_c_num_k),
        nn.ReLU(inplace=True),
        # Layer 3
        nn.ConvTranspose2d(in_channels=g_l3_in_ch,
                            out_channels=g_l3_c_num_k,
                            kernel_size=g_l3_kernel_sz,
                            stride=g_l3_stride,
                            padding=g_l3_padding,
                            bias=c_bias
        ),
        nn.BatchNorm2d(g_l3_c_num_k),
        nn.ReLU(inplace=True),
        # Layer 4
        nn.ConvTranspose2d(in_channels=g_l4_in_ch,
                            out_channels=g_l4_c_num_k,
                            kernel_size=g_l4_kernel_sz,
                            stride=g_l4_stride,
                            padding=g_l4_padding,
                            bias=c_bias
        ),
        nn.BatchNorm2d(g_l4_c_num_k),
        nn.ReLU(inplace=True),
        # Layer 5
        nn.ConvTranspose2d(in_channels=g_l5_in_ch,
                            out_channels=g_l5_c_num_k,
                            kernel_size=g_l5_kernel_sz,
                            stride=g_l5_stride,
                            padding=g_l5_padding,
                            bias=c_bias
        ),
        nn.Tanh()
      )

  def forward(self, x):
    x = self.layers(x)
    return x</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize variables for the device to train on, the batch size of MNIST data samples, the learning rate, the number of
        epochs, and the labels for the real and synthetic (fake) data samples, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>device = 'cpu'
learning_rate = 0.0002
num_epochs = 3
label_real = 1
label_fake = 0</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To define a function to initialize the Discriminator and the Generator model weights, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre># Default weights initialized by PyTorch leads to Vanishing Gradient problems.
# Hence, custom weights initialization is required

def custom_weights_init(m):
  classname = m.__class__.__name__
  if classname.find('Conv') != -1:
    m.weight.data.normal_(0.0, 0.02)
  elif classname.find('BatchNorm') != -1:
    m.weight.data.normal_(1.0, 0.02)
    m.bias.data.fill_(0)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the Generator model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>generator = Generator().to(device)
generator.apply(custom_weights_init)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Similarly, to create an instance of the Discriminator model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>discriminator = Discriminator().to(device)
discriminator.apply(custom_weights_init)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the binary cross entropy loss, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>criterion = nn.BCELoss()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the optimizer for both the Generator and Discriminator models, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To implement the iterative training loop to predict the class, compute the loss, and adjust the model parameters through
        backward pass, for both the Generator and Discriminator models, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>def train_discriminator(data_real, data_fake):
  d_optimizer.zero_grad()

  # Real data
  data_real = data_real.to(device)
  data_real_sz = data_real.size(0)
  x_real_label = torch.full((data_real_sz,), label_real, dtype=torch.float, device=device)
  y_hat_real = discriminator(data_real).view(-1)
  loss_real = criterion(y_hat_real, x_real_label)
  loss_real.backward()

  # Fake data
  z_fake_label = torch.full((data_real_sz,), label_fake, dtype=torch.float, device=device)
  y_hat_fake = discriminator(data_fake).view(-1)
  loss_fake = criterion(y_hat_fake, z_fake_label)
  loss_fake.backward()

  # Update the Discriminator weights
  d_optimizer.step()

  return loss_real + loss_fake

def train_generator(data_fake):
  g_optimizer.zero_grad()
  z_fake_sz = data_fake.size(0)
  z_real_label = torch.full((z_fake_sz,), label_real, dtype=torch.float, device=device)
  y_hat_fake = discriminator(data_fake).view(-1)
  loss_fake = criterion(y_hat_fake, z_real_label)
  loss_fake.backward()
  g_optimizer.step()

  return loss_fake

print('### Starting Training')
for epoch in range(num_epochs):
  print(f'\tEpoch -> {epoch+1}')
  for batch_index, (x_real_0, x_real_1) in enumerate(mnist_loader):
    # Train Discriminator
    x_real = x_real_0.to(device)
    x_real_sz = x_real.size(0)
    z_random = torch.randn(x_real_sz, g_latent_sz, 1, 1, device=device)
    z_fake = generator(z_random)
    d_loss = train_discriminator(x_real, z_fake)

    # Train Generator
    z_fake = generator(z_random)
    g_loss = train_generator(z_fake)

    if batch_index % 50 == 0:
      print(f'\tGAN Model -> Batch: {batch_index}, Generator Loss: {g_loss}, Discriminator Loss: {d_loss}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following would be a typical trimmed output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output</h4>
      <pre>### Starting Training
	Epoch -&gt; 1
	GAN Model -&gt; Batch: 0, Generator Loss: 2.23104190826416, Discriminator Loss: 1.7122244834899902
	GAN Model -&gt; Batch: 50, Generator Loss: 8.913931846618652, Discriminator Loss: 0.03291315585374832
	GAN Model -&gt; Batch: 100, Generator Loss: 4.616231441497803, Discriminator Loss: 0.17577435076236725
	GAN Model -&gt; Batch: 150, Generator Loss: 7.0666680335998535, Discriminator Loss: 0.0914294421672821
  [... TRIM ...]
	GAN Model -&gt; Batch: 750, Generator Loss: 1.8621577024459839, Discriminator Loss: 0.4158519506454468
	GAN Model -&gt; Batch: 800, Generator Loss: 3.454256534576416, Discriminator Loss: 1.0430299043655396
	GAN Model -&gt; Batch: 850, Generator Loss: 2.7888023853302, Discriminator Loss: 0.20228609442710876
	GAN Model -&gt; Batch: 900, Generator Loss: 4.444871425628662, Discriminator Loss: 0.8754826784133911
	Epoch -&gt; 2
	GAN Model -&gt; Batch: 0, Generator Loss: 7.739803314208984, Discriminator Loss: 1.2408348321914673
	GAN Model -&gt; Batch: 50, Generator Loss: 2.569080352783203, Discriminator Loss: 0.3771322965621948
	GAN Model -&gt; Batch: 100, Generator Loss: 3.3595800399780273, Discriminator Loss: 0.24128982424736023
	GAN Model -&gt; Batch: 150, Generator Loss: 3.3501620292663574, Discriminator Loss: 0.29581916332244873
  [... TRIM ...]
	GAN Model -&gt; Batch: 750, Generator Loss: 3.5940775871276855, Discriminator Loss: 0.24096110463142395
	GAN Model -&gt; Batch: 800, Generator Loss: 3.5626015663146973, Discriminator Loss: 0.13417811691761017
	GAN Model -&gt; Batch: 850, Generator Loss: 0.9706592559814453, Discriminator Loss: 1.0775089263916016
	GAN Model -&gt; Batch: 900, Generator Loss: 1.7001209259033203, Discriminator Loss: 0.6206040978431702
	Epoch -&gt; 3
	GAN Model -&gt; Batch: 0, Generator Loss: 1.8814767599105835, Discriminator Loss: 0.6723608374595642
	GAN Model -&gt; Batch: 50, Generator Loss: 3.032073974609375, Discriminator Loss: 0.17791661620140076
	GAN Model -&gt; Batch: 100, Generator Loss: 4.242057800292969, Discriminator Loss: 0.3889331817626953
	GAN Model -&gt; Batch: 150, Generator Loss: 3.995181083679199, Discriminator Loss: 0.074040487408638
  [... TRIM ...]
	GAN Model -&gt; Batch: 750, Generator Loss: 4.391271591186523, Discriminator Loss: 0.5113582611083984
	GAN Model -&gt; Batch: 800, Generator Loss: 5.010047912597656, Discriminator Loss: 0.24809230864048004
	GAN Model -&gt; Batch: 850, Generator Loss: 4.629855155944824, Discriminator Loss: 0.10992951691150665
	GAN Model -&gt; Batch: 900, Generator Loss: 6.0013227462768555, Discriminator Loss: 0.13961000740528107</pre>
    </div>
    <br/>    
    <div id="para-div">
      <p>Note that this training will take <span class="underbold">AT LEAST</span> 30 mins to complete !!!</p>
    </div>
    <div id="para-div">
      <p>To display some real data samples from the MNIST dataset, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>real_batch = next(iter(mnist_loader))
plt.figure(figsize=(4, 4))
plt.axis('off')
plt.title('Real MNIST Sample Images')
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:32], padding=2, normalize=True).cpu(), (1, 2, 0)))
plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration depicts real samples images from the MNIST dataset:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Real Samples" class="gen-img-cls" src="./images/gen-adversarial-4.png">
        <div class="gen-img-cap">Figure.4</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To display some synthetic data samples from the Generator model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="gen-src-outer-div">
      <div class="gen-src-body">
<pre>with torch.no_grad():
  z_random = torch.randn(img_sz, g_latent_sz, 1, 1, device=device)
  fake_batch = generator(z_random).detach().cpu()
  plt.figure(figsize=(4, 4))
  plt.axis('off')
  plt.title('Synthetic MNIST Sample Images')
  plt.imshow(np.transpose(vutils.make_grid(fake_batch[:32], padding=2, normalize=True).cpu(), (1, 2, 0)))
  plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration depicts synthetic samples images produced by the Generator model:</p>
    </div>
    <br/>
    <div id="para-div">
      <div id="gen-img-outer-div">
        <img alt="Synthetic Samples" class="gen-img-cls" src="./images/gen-adversarial-5.png">
        <div class="gen-img-cap">Figure.5</div>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The synthetic image samples generated by the Generator model seem <span class="underbold">ALMOST</span> close to the real
        image samples !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://arxiv.org/pdf/1406.2661v1" target="_blank"><span class="bold">GAN Paper</span></a></p>
    </div>
    <hr class="gen-line-hr" />
    <div>
      <a id="gen-footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
