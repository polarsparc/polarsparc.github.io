<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Apache Spark 2.x Quick Notes :: Part - 1">
    <meta name="subject" content="Apache Spark 2.x Quick Notes :: Part - 1">
    <meta name="keywords" content="big data, python, spark">
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <title>Apache Spark 2.x Quick Notes :: Part - 1</title>
    <link href="../css/polarsparc-v2.0.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Apache Spark 2.x Quick Notes :: Part - 1</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>09/29/2019</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" /> <br />
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p>Apache Spark is a general purpose, high-performance, open-source, unified analytics cluster computing
        engine for large scale distributed processing of data across a cluster of commodity computers (also
        referred to as nodes).</p>
      <p>The Spark stack provides support for batch processing, interactive querying using sql, streaming,
        machine learning, and graph processing.</p>
      <p>Spark can run on a variety of cluster computing engines such as the built-in standalone cluster
        manager, Hadoop YARN, Apache Mesos, Kubernetes, or in a cloud environment such as AWS, Azure, or
        Google Cloud.</p>
      <p>Spark can access data from a variety of sources such as local filesystem, Hadoop HDFS, Apache Hive,
        Apache HBase, Apache Cassandra, etc.</p>
      <p>Spark can be accessible through either APIs (Java, Scala, Python, R) or through the provided shells
        (Scala and Python).</p>
      <p>The following diagram illustrates the components of the Spark stack:</p>
      <div id="img-outer-div"> <img alt="Spark Stack" class="img-cls" src="./images/Spark-01.png" />
        <div class="img-cap">Spark Components</div>
      </div>
    </div>
    <div id="para-div">
      <p>Spark stack consists of the following components:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p>Data layer for persistence (Hadoop HDFS)</p>
        </li>
        <li>
          <p>Cluster Manager to manage the nodes in the cluster (Standalone Cluster Manager, Apache Mesos,
            Hadoop YARN, or Kubernetes)</p>
        </li>
        <li>
          <p>Core Spark cluster computing engine that is responsible for work scheduling, memory management,
            fault management, and storage management</p>
        </li>
        <li>
          <p>Spark SQL provides an SQL interface to access data distributed across the nodes in the cluster</p>
        </li>
        <li>
          <p>Spark Streaming that enables processing of a stream of live data in real-time</p>
        </li>
        <li>
          <p>Spark GraphX is a library for managing graph of data objects</p>
        </li>
        <li>
          <p>Spark MLlib is a library for the common machine learning algorithms</p>
        </li>
      </ul>
    </div>
    <br />
    <div id="section-div">
      <p>Installation and Setup (Single Node)</p>
    </div>
    <div id="para-div">
      <p>The installation will be performed on a Ubuntu 18.04 based desktop.</p>
      <p>Download a stable version of Spark (2.4.4 at the time of this article) from the project site located at the
        URL <a href="http://spark.apache.org"><span class="bold">spark.apache.org</span></a>.
      </p>
      <p>We choose to download the Spark version 2.4.4 (pre-built for Hadoop 2.7) for this setup.</p>
      <p>The following diagram illustrates the download from the Apache Spark site
        <a href="http://spark.apache.org"><span class="bold">spark.apache.org</span></a>:</p>
      <div id="img-outer-div"> <img alt="Spark Download" class="img-cls" src="./images/Spark-02.png" />
        <div class="img-cap">Spark 2.4.4 Download</div>
      </div>
      <p>Following are the steps to install and setup Spark on a single node cluster:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p>Ensure Java SE 8 is installed. In our case, <span class="bold">OpenJDK 8 (Java SE 8)</span> was installed in
            the directory <span class="bold">/usr/lib/jvm/java-8-openjdk-amd64</span></p>
          <div id="error-div">
            <h4>&#9760; ATTENTION &#9760;</h4>
            <pre>Spark 2.4.4 will only work with <span class="underbold">JDK 8</span></pre>
          </div>
        </li>
        <li>
          <p>Ensure Python 3.7 or above is installed. To make it easy and simple, we choose the open-source
            <a href="https://www.anaconda.com/distribution" target="_blank"> <span class="bold">Anaconda Python 3</span></a>
            distribution, which includes all the necessary <span class="bold">Python</span> packages.</p>
          <p>Download the <span class="bold">Python 3.7</span> version of the <span class="bold">Anaconda</span> distribution.</p>
          <p>Extract and install the downloaded <span class="bold">Anaconda Python 3</span> archive to a directory, say,
            <span class="bold">/home/polarsparc/Programs/anaconda3</span>.</p>
        </li>
        <li>
          <p>Extract the downloaded <span class="bold">spark-2.4.4-bin-hadoop2.7.tgz</span> file into a desired directory
            (say <span class="hi-yellow">/home/polarsparc/Programs/spark-2.4.4</span>)</p>
          <p>The following diagram illustrates the contents of the directory /home/polarsparc/Programs/spark-2.4.4 after extraction:</p>
          <div id="img-outer-div"> <img alt="Spark Extract" class="img-cls" src="./images/Spark-03.png" />
            <div class="img-cap">Spark 2.4.4 Extract</div>
          </div>
        </li>
        <li>
          <p>Open a terminal and change the working directory to /home/polarsparc/Programs</p>
          <p>Create a shell (.sh) file called <span class="hi-yellow">env.sh</span> with the following lines:</p>
          <div id="cmd-div">
            <p>export SPARK_HOME=/home/polarsparc/Programs/spark-2.4.4</p>
            <p>export PATH=$PATH:$SPARK_HOME/bin</p>
            <p>export SPARK_CLASSPATH=`echo $SPARK_HOME/jars/* | sed 's/ /:/g'`</p>
          </div>
        </li>
        <li>
          <p>Change the working directory to /home/polarsparc</p>
          <p>Source the environment variables in the file <span class="bold">env.sh</span>, by typing the following command:</p>
          <div id="cmd-div">
            <p>. ./Programs/env.sh</p>
          </div>
        </li>
        <li>
          <p>In the terminal window, execute the following command to launch the Python Spark shell:</p>
          <div id="cmd-div">
            <p>pyspark --master local[1]</p>
          </div>
          <p>The option <span class="hi-yellow">local[1]</span> indicates that we want to execute Spark core and the Python
            shell in a <span class="hi-yellow">standalone local</span> mode without any cluster manager. This mode is useful
            during development and testing.</p>
          <p>The following should be the typical output:</p>
          <div id="out-div">
            <h4>Output.1</h4>
            <pre>Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
19/09/29 12:41:58 WARN Utils: Your hostname, polarsparc resolves to a loopback address: 127.0.1.1; using 192.168.1.179 instead (on interface enp6s0)
19/09/29 12:41:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/09/29 12:41:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 3.7.4 (default, Aug 13 2019 20:35:49)
SparkSession available as 'spark'.
&gt;&gt;&gt;</pre>
          </div>
        </li>
        <li>
          <p>Exit the Python Spark shell, by typing the following command:</p>
          <div id="cmd-div">
            <p>exit()</p>
          </div>
        </li>
        <li>
          <p>In the terminal window, change to the directory <span class="bold">/home/polarsparc/Programs/spark-2.4.4/conf</span>
            and execute the following commands:</p>
          <div id="cmd-div">
            <p>cp slaves.template slaves</p>
            <p>cp spark-env.sh.template spark-env.sh</p>
          </div>
          <p>Delete all the contents of the file <span class="bold">spark-env.sh</span> under the directory
            /home/polarsparc/Programs/spark-2.4.4/conf and add the following lines (don't worry about the details for now):</p>
          <div id="cmd-div">
            <p>SPARK_IDENT_STRING=MySpark</p>
            <p>SPARK_DRIVER_MEMORY=2g</p>
            <p>SPARK_EXECUTOR_CORES=1</p>
            <p>SPARK_EXECUTOR_MEMORY=1g</p>
            <p>SPARK_LOCAL_IP=localhost</p>
            <p>SPARK_LOCAL_DIRS=$SPARK_HOME/work</p>
            <p>SPARK_MASTER_HOST=localhost</p>
            <p>SPARK_WORKER_CORES=2</p>
            <p>SPARK_WORKER_MEMORY=2g</p>
            <p>SPARK_WORKER_DIR=$SPARK_HOME/work</p>
          </div>
        </li>
        <li>
          <p>Change the working directory to /home/polarsparc</p>
          <p>Let us once again try to re-launch the Python Spark shell in the terminal window:</p>
          <div id="cmd-div">
            <p>pyspark --master local[1]</p>
          </div>
          <p>The following will be the typical output:</p>
          <div id="out-div">
            <h4>Output.2</h4>
            <pre>Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
19/09/29 13:46:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 3.7.4 (default, Aug 13 2019 20:35:49)
SparkSession available as 'spark'.
&gt;&gt;&gt; </pre>
          </div>
        </li>
        <li>
          <p>Let us go ahead and try the following command from the Python Spark shell (dont worry about the details for now):</p>
          <div id="cmd-div">
            <p>lines = sc.textFile('./Programs/spark-2.4.4/README.md')</p>
          </div>
        </li>
        <li>
          <p>If all goes well, there will be <span class="bold">*NO*</span> output</p>
          <p>Let us go ahead and try the following command from the Python Spark shell (dont worry about the details for now):</p>
          <div id="cmd-div">
            <p>lines.count()</p>
          </div>
          <p>The following will be the typical output:</p>
          <div id="out-div">
            <h4>Output.3</h4>
            <pre>105
&gt;&gt;&gt; </pre>
          </div>
        </li>
        <li>
          <p><span class="bold">BINGO</span> - the Python Spark shell works !!!</p>
          <p>The following diagram illustrates the screenshot for the Spark UI that can be accessed at
            <span class="hi-yellow">http://localhost:4040</span>:</p>
          <div id="img-outer-div"> <img alt="Spark UI" class="img-cls" src="./images/Spark-04.png" />
            <div class="img-cap">Spark 2.4.4 UI</div>
          </div>
        </li>
        <li>
          <p>Exit the Python Spark shell, by typing the following command:</p>
          <div id="cmd-div">
            <p>exit()</p>
          </div>
        </li>
        <li>
          <p>We need to configure <span class="bold">pyspark</span> to use <span class="hi-yellow">Jupyter Notebook</span>.</p>
          <p>In the terminal window, change the working directory to /home/polarsparc/Programs</p>
          <p>Modify the file called <span class="bold">env.sh</span> to append the following lines:</p>
          <div id="cmd-div">
            <p>export PYSPARK_DRIVER_PYTHON="jupyter"</p>
            <p>export PYSPARK_DRIVER_PYTHON_OPTS="notebook"</p>
          </div>
        </li>
        <li>
          <p>Change the working directory to /home/polarsparc</p>
          <p>Source the environment variables in the file <span class="bold">env.sh</span>, by typing the following command:</p>
          <div id="cmd-div">
            <p>. ./Programs/env.sh</p>
          </div>
        </li>
        <li>
          <p>Create a new directory called <span class="hi-yellow">/home/polarsparc/Projects/Python/Notebooks/Spark</span>
            and change to that directory.</p>
          <p>In the terminal window, once again execute the following command to launch the Python Spark shell:</p>
          <div id="cmd-div">
            <p>pyspark --master local[1]</p>
          </div>
          <p>The following will be the typical output:</p>
          <div id="out-div">
            <h4>Output.4</h4>
            <pre>[I 15:06:53.372 NotebookApp] JupyterLab extension loaded from /home/polarsparc/Programs/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 15:06:53.372 NotebookApp] JupyterLab application directory is /home/polarsparc/Programs/anaconda3/share/jupyter/lab
[I 15:06:53.374 NotebookApp] Serving notebooks from local directory: /home/polarsparc/Projects/Python/Notebooks/Spark
[I 15:06:53.374 NotebookApp] The Jupyter Notebook is running at:
[I 15:06:53.374 NotebookApp] http://localhost:8888/?token=df6db1202f8ff61f6a6b74081153f273ba47aef0d9597332
[I 15:06:53.374 NotebookApp]  or http://127.0.0.1:8888/?token=df6db1202f8ff61f6a6b74081153f273ba47aef0d9597332
[I 15:06:53.374 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 15:06:53.430 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///home/polarsparc/.local/share/jupyter/runtime/nbserver-23473-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=df6db1202f8ff61f6a6b74081153f273ba47aef0d9597332
     or http://127.0.0.1:8888/?token=df6db1202f8ff61f6a6b74081153f273ba47aef0d9597332</pre>
          </div>
          <p>This will also launch a new browser window for the Jupyter notebook. The following diagram illustrates the
            screenshot for the Jupyter notebook:</p>
          <div id="img-outer-div"> <img alt="Jupyter Notebook" class="img-cls" src="./images/Spark-05.png" />
            <div class="img-cap">Jupyter Notebook</div>
          </div>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>We have successfully completed the installation and the necessary setup on our single node Spark cluster.</p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
