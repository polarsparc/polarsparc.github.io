<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Apache Spark 4.x Quick Notes :: Part - 2">
    <meta name="subject" content="Apache Spark 4.x Quick Notes :: Part - 2">
    <meta name="keywords" content="big data, python, spark">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Apache Spark 4.x Quick Notes :: Part - 2</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Apache Spark 4.x Quick Notes :: Part - 2</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>11/28/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Spark Core Components</p>
    </div>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/Distributed/Spark-1.html" target="_blank"><span class="bold">Part-1</span></a> of
        this series, we introduced Apache Spark as a general purpose distributed computing engine for data processing on a cluster
        of commodity computers.</p>
      <p>What does that really mean though ? Let us break it down ...</p>
      <p>A Spark cluster consists of a <span class="hi-vanila">Driver</span> process running within the user's <span class="bold">
        Application</span>, a <span class="hi-vanila">Cluster Manager</span> running on the <span class="bold">Master</span> node,
        and <span class="hi-vanila">Executor</span> process(es) running within each of the <span class="bold">Worker</span> nodes.
        When a user Application submits a Spark job, the Driver communicates with the Master to determine the available Worker nodes
        with Executor(s). Next, the Driver partitions and distributes the job as task(s) to the available Executor process(es) (on
        the different Worker nodes) for further processing. As the application job executes, the Executor process(es) report back
        the state of the task(s) to the Driver process and thus the Driver maintains the overall status of the application job.</p>
      <p>Ok - this explains the high-level view of the distributed compute cluster. How does the Driver process know which Executors
        are available for processing the task(s) and whom to distribute the tasks to ? This is where the <span class="hi-vanila">
        Cluster Manager</span> comes into play. The Cluster Manager keeps track of the state of the cluster resources (i.e., which
        Executor process(es) on which Worker nodes are available, etc).</p>
      <p>The Driver process has a connection to the Cluster Manager via a <span class="hi-yellow">SparkSession</span> (or a <span
        class="hi-yellow">SparkContext</span>). SparkSession is a higher level wrapper around the SparkContext.</p>
      <p>Hope this all makes sense at a high-level now.</p>
    </div>
    <div id="para-div">
      <p>The following diagram illustrates the core components and their interaction in Apache Spark:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Spark Architecture" class="img-cls" src="./images/spark-03.png" />
      <div class="img-cap">Spark Architecture</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following table summarizes the core components of Apache Spark:</p>
      <br/>
      <table id="col2-table">
        <thead>
          <tr>
            <th>Component</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="col2-c1-odd"><span class="bold">SparkContext</span></td>
            <td class="col2-c2-odd">Represents a connection to the cluster</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">SparkSession</span></td>
            <td class="col2-c2-even">Represents a unified higher level abstraction of the cluster</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">Driver</span></td>
            <td class="col2-c2-odd">The process that creates and uses an instance of a <span class="bold">SparkSession</span> or a <span class="bold">SparkContext</span></td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">Worker Node</span></td>
            <td class="col2-c2-even">A node in the cluster that executes application code</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">Executor</span></td>
            <td class="col2-c2-odd">A process that is launched for an application on a <span class="bold">Worker Node</span> to execute a unit of work (task)
              and to store data (in-memory and/or on-disk)</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">Task</span></td>
            <td class="col2-c2-even">A unit of work that is sent to an <span class="bold">Executor</span></td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">Cluster Manager</span></td>
            <td class="col2-c2-odd">A service that is responsible for managing resources on the cluster. It decides which applications can use which
              <span class="bold">Worker Node</span> and accordingly lauches the <span class="bold">Executor</span> process</td>
          </tr>
        </tbody>
      </table>
      <br/>
    </div>
    <div id="para-div">
      <p>Now that we have a basic understanding of the core components of Apache Spark, we can explain some of the variables we
        defined in the file <span class="bold">$HOME/spark/conf/spark-env.sh</span> during the installation and setup in <a href=
        "https://polarsparc.github.io/Distributed/Spark-1.html" target="_blank"><span class="bold">Part-1</span></a> of this series.</p>
      <p>The following are the variables along with their respective description:</p>
      <br/>
      <table id="col2-table">
        <thead>
          <tr>
            <th>Variable</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="col2-c1-odd"><span class="bold">SPARK_IDENT_STRING</span></td>
            <td class="col2-c2-odd">A string representing a name for this instance of Spark</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">SPARK_DRIVER_MEMORY</span></td>
            <td class="col2-c2-even">Memory allocated for the Driver process</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">SPARK_EXECUTOR_CORES</span></td>
            <td class="col2-c2-odd">The number of CPU cores for use by the Executor process(es)</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">SPARK_EXECUTOR_MEMORY</span></td>
            <td class="col2-c2-even">Memory allocated for each Executor process</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">SPARK_LOCAL_IP</span></td>
            <td class="col2-c2-odd">The IP address used by the Driver and Executor to bind to on this node</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">SPARK_LOCAL_DIRS</span></td>
            <td class="col2-c2-even">The directory to use on this node for storing data</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">SPARK_MASTER_HOST</span></td>
            <td class="col2-c2-odd">The IP address used by the Master to bind to on this node</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">SPARK_WORKER_CORES</span></td>
            <td class="col2-c2-even">The total number of CPU cores to allow the Worker Node to use on this node</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">SPARK_WORKER_MEMORY</span></td>
            <td class="col2-c2-odd">The total amount of memory to allow the Worker Node to use on this node</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">SPARK_WORKER_DIR</span></td>
            <td class="col2-c2-even">The temporary directory to use on this node by the Worker Node</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">SPARK_EXECUTOR_INSTANCES</span></td>
            <td class="col2-c2-even">The number of Worker Nodes to start on this node</td>
          </tr>
        </tbody>
      </table>
      <br/>
      <p>Now that we have a good handle on the basics of Apache Spark, let us proceed to setup and leverage the <span class="bold">
        Standalone Cluster</span> to run Spark jobs.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Setup Standalone Cluster</p>
    </div>
    <div id="para-div">
      <p>For the hands-on demonstration, we will create a custom text file with some paragraphs about Spark, which can be downloaded
        from <a href="https://polarsparc.github.io/data/Spark.txt" target="_blank"><span class="bold">HERE</span></a> !!!</p>
    </div>
    <div id="para-div">
      <p>Let us assume the IP address of the Linux desktop to be <span class="bold">192.168.1.25</span> with the host name of <span
        class="bold">vader</span>.</p>
    </div>
    <div id="para-div">
      <p>To start the Spark Master node running the Standalone Cluster Nanager, execute the following command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name spark-master --network host -u $(id -u $USER):$(id -g $USER) -p 192.168.1.25:7077:7077 -p 192.168.1.25:8080:8080 -v $HOME/spark/conf:/opt/spark/conf -v $HOME/spark/logs:/opt/spark/logs -it ps-spark:v4.0.1 /opt/spark/sbin/start-master.sh</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>WARNING: Published ports are discarded when using host network mode
starting org.apache.spark.deploy.master.Master, logging to /opt/spark/logs/spark-MySparkCluster-org.apache.spark.deploy.master.Master-1-vader.out
Spark Command: /opt/java/openjdk/bin/java -cp /opt/spark/conf/:/opt/spark/jars/slf4j-api-2.0.16.jar:/opt/spark/jars/* -Xmx1g -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true org.apache.spark.deploy.master.Master --host vader --port 7077 --webui-port 8080
========================================
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/11/27 12:56:55 INFO Master: Started daemon with process name: 16@vader
25/11/27 12:56:55 INFO SignalUtils: Registering signal handler for TERM
25/11/27 12:56:55 INFO SignalUtils: Registering signal handler for HUP
25/11/27 12:56:55 INFO SignalUtils: Registering signal handler for INT
25/11/27 12:56:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/11/27 12:56:56 INFO SecurityManager: Changing view acls to: alice
25/11/27 12:56:56 INFO SecurityManager: Changing modify acls to: alice
25/11/27 12:56:56 INFO SecurityManager: Changing view acls groups to: alice
25/11/27 12:56:56 INFO SecurityManager: Changing modify acls groups to: alice
25/11/27 12:56:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: alice groups with view permissions: EMPTY; users with modify permissions: alice; groups with modify permissions: EMPTY; RPC SSL disabled
25/11/27 12:56:56 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
25/11/27 12:56:56 INFO Master: Starting Spark master at spark://vader:7077
25/11/27 12:56:56 INFO Master: Running Spark version 4.0.1
25/11/27 12:56:56 INFO JettyUtils: Start Jetty 0.0.0.0:8080 for MasterUI
25/11/27 12:56:56 INFO Utils: Successfully started service 'MasterUI' on port 8080.
25/11/27 12:56:56 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://vader:8080
25/11/27 12:56:56 INFO Master: I have been elected leader! New state: ALIVE</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Open the web browser and navigate to the URL <span class="bold">http://192.168.1.25:8080</span> to launch the Spark Master
        Web UI.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the Spark Master web UI:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Spark Master Web UI" class="img-cls" src="./images/spark-04.png" />
      <div class="img-cap">Spark Master Web UI</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Next, to start a Spark Worker node, execute the following command in a second terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name spark-worker --network host -u $(id -u $USER):$(id -g $USER) -p 192.168.1.25:8081:8081 -v $HOME/spark/conf:/opt/spark/conf -v $HOME/spark/data:/opt/spark/data -v $HOME/spark/logs:/opt/spark/logs -v $HOME/spark/work-dir:/opt/spark/work-dir -it ps-spark:v4.0.1 /opt/spark/sbin/start-worker.sh spark://192.168.1.25:7077</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>WARNING: Published ports are discarded when using host network mode
starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/logs/spark-MySparkCluster-org.apache.spark.deploy.worker.Worker-1-vader.out
Spark Command: /opt/java/openjdk/bin/java -cp /opt/spark/conf/:/opt/spark/jars/slf4j-api-2.0.16.jar:/opt/spark/jars/* -Xmx1g -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://192.168.1.25:7077
========================================
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/11/27 12:57:52 INFO Worker: Started daemon with process name: 14@vader
25/11/27 12:57:52 INFO SignalUtils: Registering signal handler for TERM
25/11/27 12:57:52 INFO SignalUtils: Registering signal handler for HUP
25/11/27 12:57:52 INFO SignalUtils: Registering signal handler for INT
25/11/27 12:57:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/11/27 12:57:52 INFO SecurityManager: Changing view acls to: alice
25/11/27 12:57:52 INFO SecurityManager: Changing modify acls to: alice
25/11/27 12:57:52 INFO SecurityManager: Changing view acls groups to: alice
25/11/27 12:57:52 INFO SecurityManager: Changing modify acls groups to: alice
25/11/27 12:57:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: alice groups with view permissions: EMPTY; users with modify permissions: alice; groups with modify permissions: EMPTY; RPC SSL disabled
25/11/27 12:57:52 INFO Utils: Successfully started service 'sparkWorker' on port 34025.
25/11/27 12:57:52 INFO Worker: Worker decommissioning not enabled.
25/11/27 12:57:53 INFO Worker: Starting Spark worker 192.168.1.25:34025 with 2 cores, 4.0 GiB RAM
25/11/27 12:57:53 INFO Worker: Running Spark version 4.0.1
25/11/27 12:57:53 INFO Worker: Spark home: /opt/spark
25/11/27 12:57:53 INFO ResourceUtils: ==============================================================
25/11/27 12:57:53 INFO ResourceUtils: No custom resources configured for spark.worker.
25/11/27 12:57:53 INFO ResourceUtils: ==============================================================
25/11/27 12:57:53 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
25/11/27 12:57:53 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
25/11/27 12:57:53 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://vader:8081
25/11/27 12:57:53 INFO Worker: Connecting to master 192.168.1.25:7077...
25/11/27 12:57:53 INFO TransportClientFactory: Successfully created connection to /192.168.1.25:7077 after 13 ms (0 ms spent in bootstraps)
25/11/27 12:57:53 INFO Worker: Successfully registered with master spark://vader:7077
25/11/27 12:57:53 INFO Worker: Worker cleanup enabled; old application directories will be deleted in: /opt/spark/work-dir</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Refresh the Spark Master Web UI and the following illustration depicts that a Spark Worker node is registered (pointed by
        the red arrow):</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Spark Master Web UI" class="img-cls" src="./images/spark-05.png" />
      <div class="img-cap">With Spark Worker Node</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Open the web browser and navigate to the URL <span class="bold">http://192.168.1.25:8081</span> to launch the Spark Worker
        Web UI.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the Spark Worker web UI:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Spark Worker Web UI" class="img-cls" src="./images/spark-06.png" />
      <div class="img-cap">Spark Worker Web UI</div>
    </div>
    <br/>
    <div id="para-div">
      <p>At this point there is one Worker node that is registered with the Master node and the Standalone cluster manager is ready
        to accept user application jobs !</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, open a new terminal window and start the pyspark shell by executing the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name pyspark --network host -u $(id -u $USER):$(id -g $USER) -p 192.168.1.25:4040:4040 -v $HOME/spark/conf:/opt/spark/conf -v $HOME/spark/data:/opt/spark/data -v $HOME/spark/work-dir:/opt/spark/work-dir -it ps-spark:v4.0.1 /opt/spark/bin/pyspark --master spark://192.168.1.25:7077 --deploy-mode client</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>WARNING: Published ports are discarded when using host network mode
Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/11/27 12:58:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting Spark log level to "INFO".
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 4.0.1
      /_/

Using Python version 3.10.12 (main, Aug 15 2025 14:32:43)
Spark context Web UI available at http://vader:4040
Spark context available as 'sc' (master = spark://192.168.1.25:7077, app id = app-20251127125844-0000).
SparkSession available as 'spark'.
&gt;&gt;&gt;</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Once again, refresh the Spark Master Web UI and the following illustration depicts that a pyspark application is registered
        (pointed by the blue arrow):</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Spark Master Web UI" class="img-cls" src="./images/spark-07.png" />
      <div class="img-cap">With PySpark Application</div>
    </div>
    <br/>
    <div id="para-div">
      <p>We are now ready for the hands-on demonstartion !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on PySpark (Standalone Cluster)</p>
    </div>
    <div id="para-div">
      <p>Notice that by default, the pyspark shell creates an instance of SparkSession named <span class="hi-grey">spark</span> and
        an instance of SparkContext named <span class="hi-grey">sc</span>.</p>
    </div>
    <div id="para-div">
      <p>In Spark, a <span class="hi-yellow">Resilient Distributed Dataset</span> (or <span class="bold">RDD</span> for short) is an
        immutable collection of object(s) that is partitioned and distributed across the Worker nodes in a cluster.</p>
    </div>
    <div id="para-div">
      <p>One can create an RDD in pyspark by calling the <span class="hi-vanilla">parallelize()</span> function on the SparkContext
        and passing in a list of objects.</p>
      <p>To create a pyspark RDD named <span class="bold">numbers</span>, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; numbers = sc.parallelize(['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'])</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-vanila">textFile()</span> function on the SparkContext takes as argument a string that represents the
        path to a text file and creates an RDD by loading the contents of that file.</p>
      <p>To create a pyspark RDD named <span class="bold">lines</span> from a text file, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; lines = sc.textFile('/opt/spark/data/Spark.txt')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-vanila">count()</span> function on an RDD is an action, which returns the number of elements from the
        associated RDD.</p>
      <p>To check the count of elements in the <span class="bold">lines</span> RDD, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; lines.count()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>13</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="hi-vanila">filter()</span> function on an RDD is a transformation operation, which returns another RDD
        containing elements from the associated RDD that satisfy the conditions of the specified lambda function.</p>
      <p>Note that the RDD Tranformation functions are not executed immediately. Instead, they are lazily evaluated, meaning they
        are evaluated only when an action function is invoked on them.</p>
      <p>To create a new RDD called <span class="bold">threes</span> by filtering (selecting) all the words with 3 letters from the
        <span class="bold">numbers</span> RDD, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; threes = numbers.filter(lambda s: len(s) == 3)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>In summary, RDDs support two types of operations - actions and transformations.</p>
      <p>Transformations return a new RDD from a specified RDD, while Actions compute results that are returned to the Driver.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-vanila">collect()</span> function on an RDD is an action operation that returns all the elements from
        the associated RDD. So, be very <span class="hi-red">CAREFUL</span> when using this function - this function expects all
        the objects of the RDD to fit in memory of a single node.</p>
      <p>To check the count of elements in the <span class="bold">lines</span> RDD, execute the following code snippet:</p>
    </div>
    <div id="para-div">
      <p>To collect and display all the elements in the <span class="bold">threes</span> RDD, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; threes.collect()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>['one', 'two', 'six']</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To create a new RDD called <span class="bold">sparks</span> by filtering (selecting) all the elements from the <span class
        ="bold">lines</span> RDD containing the work <span class="bold">spark</span>, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; sparks = lines.filter(lambda s: 'spark' in s.lower())</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-vanila">take()</span> function on an RDD is an action operation that returns the specified number of
        elements from the associated RDD.</p>
      <p>To take and display 3 elements from the <span class="bold">sparks</span> RDD, execute the following code snippet:</p>
    </div>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; sparks.take(3)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>['Apache Spark is an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley', "MapReduce paradigm, Spark's multi-stage in-memory primitives provides performance up to 100 times faster for certain applications.", "By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms."]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="hi-vanila">flatMap()</span> function on an RDD is a transformation operation that applies the specified
        lambda function to each element of the associated RDD and returns a new RDD with the objects from the iterators returned by
        the lambda function.</p>
    </div>
    <div id="para-div">
      <p>To create a new RDD called <span class="bold">words</span> by converting all the lines from the <span class="bold">sparks
        </span> RDD into individual words, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; words = sparks.flatMap(lambda s : s.split(' '))</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-vanila">countByValue()</span> function on an RDD is an action operation that returns the number of times
         each element occurs in the associated RDD.</p>
    </div>
    <div id="para-div">
      <p>To get a dictionary of all the words along with their respective counts from the RDD called <span class="bold">words</span>,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>&gt;&gt;&gt; words.countByValue()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>defaultdict(&lt;class 'int'&gt;, {'Apache': 3, 'Spark': 9, 'is': 3, 'an': 1, 'open': 1, 'source': 1, 'cluster': 3, 'computing': 1, 'framework': 1, 'originally': 1, 'developed': 1, 'in': 5, 'the': 3, 'AMPLab': 1, 'at': 1, 'University': 1, 'of': 2, 'California,': 1, 'Berkeley': 1, 'MapReduce': 1, 'paradigm,': 1, "Spark's": 1, 'multi-stage': 1, 'in-memory': 1, 'primitives': 1, 'provides': 1, 'performance': 1, 'up': 1, 'to': 3, '100': 1, 'times': 1, 'faster': 1, 'for': 1, 'certain': 1, 'applications.': 1, 'By': 1, 'allowing': 1, 'user': 1, 'programs': 1, 'load': 1, 'data': 1, 'into': 1, 'a': 8, "cluster's": 1, 'memory': 1, 'and': 2, 'query': 1, 'it': 2, 'repeatedly,': 1, 'well-suited': 1, 'machine': 2, 'learning': 1, 'algorithms.': 1, 'requires': 1, 'manager': 1, 'distributed': 2, 'storage': 1, 'system.': 1, 'For': 2, 'management,': 1, 'supports': 2, 'standalone': 1, '(native': 1, 'cluster),': 1, 'Hadoop': 2, 'YARN,': 1, 'or': 2, 'Mesos.': 1, 'storage,': 1, 'can': 3, 'interface': 1, 'with': 2, 'wide': 1, 'variety,': 1, 'including': 1, 'Distributed': 1, 'File': 1, 'System': 1, '(HDFS),': 1, 'Cassandra,': 1, 'OpenStack': 1, 'Swift,': 1, 'Amazon': 1, 'S3,': 1, 'Kudu,': 1, 'custom': 1, 'solution': 1, 'be': 2, 'implemented.': 1, 'also': 1, 'local': 1, 'file': 1, 'system': 1, 'used': 1, 'instead;': 1, 'such': 1, 'scenario,': 1, 'run': 1, 'on': 1, 'single': 1, 'one': 2, 'executor': 1, 'per': 1, 'CPU': 1, 'core.': 1, 'had': 1, 'excess': 1, '465': 1, 'contributors': 1, '2014,': 1, 'making': 1, 'not': 1, 'only': 1, 'most': 1, 'active': 1, 'project': 1, 'Software': 1, 'Foundation': 1, 'but': 1})</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>At this point, let us summarize the commonly used transformation and action functions on an RDD.</p>
      <p>The following is the summary of the commonly used RDD transformation functions:</p>
      <br/>
      <table id="col2-table">
        <thead>
          <tr>
            <th>Tranformation Function</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
            <tr>
              <td class="col2-c1-odd"><span class="bold">parallelize</span></td>
              <td class="col2-c2-odd">Takes a list of elements as input anc converts it into an RDD</td>
            </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">textFile</span></td>
            <td class="col2-c2-even">Takes a string that represents a path to a text file and loads the contents into an RDD</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">filter</span></td>
            <td class="col2-c2-odd">Executed to an existing RDD. Takes a lambda function as input and applies the specified lambda
              function to each element of the existing RDD. Returns a new RDD with only those elements that evaluated to true when
              the specified lambda function was applied</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">map</span></td>
            <td class="col2-c2-even">Executed to an existing RDD. Takes a lambda function as input and returns a new RDD by applying
              the specified lambda function to each element of the existing RDD</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">flatMap</span></td>
            <td class="col2-c2-odd">Executed to an existing RDD. Takes a lambda function as input and applies the specified lambda
              function to each element of the existing RDD. The lambda function returns an iterator for each element. Returns a new
              RDD which is a collection of all the elements from all the iterators after flattening them</td>
          </tr>
        </tbody>
      </table>
      <br/>
      <p>The following is the summary of the commonly used RDD action functions:</p>
      <br/>
      <table id="col2-table">
        <thead>
          <tr>
            <th>Action Function</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
            <tr>
              <td class="col2-c1-odd"><span class="bold">count</span></td>
              <td class="col2-c2-odd">Executed to an existing RDD. Returns the number of elements in the specified RDD</td>
            </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">foreach</span></td>
            <td class="col2-c2-even">Executed to an existing RDD. Takes a lambda function as input and applies the specified lambda
              function to each element of the specified RDD. There is no return value</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">take</span></td>
            <td class="col2-c2-odd">Executed to an existing RDD. Takes a integer as input and returns the specified number of elements
              from the specified RDD</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">collect</span></td>
            <td class="col2-c2-even">Executed to an existing RDD. Returns all the elements from the specified RDD. Use this function
              with CAUTION as all the elements from the specified RDD must to fit in memory of this node</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">countByValue</span></td>
            <td class="col2-c2-odd">Executed to an existing RDD. Returns the number of times each element occurs in the specified RDD</td>
          </tr>
        </tbody>
      </table>
      <br/>
    </div>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>Note that RDD is currently in <span class="underbold">MAINTENANCE</span> mode and the shift is towards the Dataframes !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>With this, we conclude the setup and demonstration of Apache Spark on a Linux desktop in the Standalone Cluster mode !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/Distributed/Spark-1.html" target="_blank"><span class="bold">Apache Spark 4.x Quick Notes :: Part - 1</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
