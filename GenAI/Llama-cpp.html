<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on Llama.cpp">
    <meta name="subject" content="Quick Primer on Llama.cpp">
    <meta name="keywords" content="llama.cpp, llm, python">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on Llama.cpp</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on Llama.cpp</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>12/26/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr"/>
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><a href="https://github.com/ggml-org/llama.cpp" target="_blank"><span class="hi-yellow">llama.cpp</span></a> is a powerful
        and efficient open source inference platform that enables one to run various <span class="bold">Large Language Models</span>
        (or <span class="bold">LLM</span>(s) for short) on a local machine.</p>
      <p>The <span class="bold">llama.cpp</span> platform comes with a built-in Web UI interface that allows one to interact with the
        local LLM(s) via the provided web user interface. In addition, the platform exposes a local API endpoint, which enables app
        developers to build AI applications/workflows to interact with the local LLM(s) via the exposed API endpoint.</p>
      <p>Last but not the least, the <span class="bold">llama.cpp</span> platform efficiently leverages the underlying system resouces
        of the local machine, such as the CPU(s) and the GPU(s), to optimally run the LLMs for better performance.</p>
      <p>In this primer, we will demonstrate how one can effectively setup and run the <span class="bold">llama.cpp</span> platform
        using a <span class="bold">Docker</span> image.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Installation and Setup</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will can on a <span class="bold">Ubuntu 24.04 LTS</span> based Linux desktop. Ensure that <span
        class="bold">Docker</span> is installed and setup on the desktop (see <a href="http://polarsparc.github.io/Docker/Docker.html"
        target="_blank">INSTRUCTIONS</a>). Also, ensure the <span class="bold">Python 3.1x</span> programming language is installed
        and setup on the desktop.</p>
    </div>
    <div id="para-div">
      <p>We will create the required models directory by executing the following command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ mkdir -p $HOME/.llama_cpp/models</p>
    </div>
    <br/>
    <div id="para-div">
      <p>From the <span class="bold">llama.cpp</span> docker <a href="https://github.com/ggml-org/llama.cpp/pkgs/container/llama.cpp"
        target="_blank">RESPOSITORY</a>, one can identify the current version of the docker image. At the time of this article, the
        latest version of the docker image ended with the version <span class="hi-red">b7524</span>.</p>
      <p>We require the docker image with the tag word <span class="underbold">full</span>. If the desktop has an Nvidia GPU, one can
        look for the docker image with the tag words <span class="underbold">full-cuda</span>.</p>
      <p>To pull and download the full docker image for <span class="bold">llama.cpp</span> with CUDA support, execute the following
        command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker pull ghcr.io/ggml-org/llama.cpp:full-cuda-b7524</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>full-cuda-b7524: Pulling from ggml-org/llama.cpp
bccd10f490ab: Pull complete 
edd1dba56169: Pull complete 
e06eb1b5c4cc: Pull complete 
7f308a765276: Pull complete 
3af11d09e9cd: Pull complete 
42896cdfd7b6: Pull complete 
600519079558: Pull complete 
0ae42424cadf: Pull complete 
73b7968785dc: Pull complete 
4fbc369d043d: Pull complete 
76db7eb25496: Pull complete 
cfc283de0b19: Pull complete 
4f4fb700ef54: Pull complete 
8c5e4a3b9157: Pull complete 
Digest: sha256:614d1b70a777b4914f5a928fdde72d6602c3b5bde6114c446dbe1c4d4e51d2d1
Status: Downloaded newer image for ghcr.io/ggml-org/llama.cpp:full-cuda-b7524
ghcr.io/ggml-org/llama.cpp:full-cuda-b7524</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To install the necessary <span class="bold">Python</span> packages, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ pip install dotenv langchain langchain-core langchain-openai pydantic</p>
    </div>
    <br/>
    <div id="para-div">
      <p>This completes all the system installation and setup for the <span class="bold">llama.cpp</span> hands-on demonstration.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on with llama.cpp</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Before we get started, we need to determine all the available <span class="bold">llama.cpp</span> command(s). To determine
        the supported command(s), execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name llama_cpp ghcr.io/ggml-org/llama.cpp:full-cuda-b7524 --help</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>Unknown command: --help
Available commands: 
  --run (-r): Run a model (chat) previously converted into ggml
              ex: -m /models/7B/ggml-model-q4_0.bin
  --run-legacy (-l): Run a model (legacy completion) previously converted into ggml
              ex: -m /models/7B/ggml-model-q4_0.bin -no-cnv -p "Building a website can be done in 10 simple steps:" -n 512
  --bench (-b): Benchmark the performance of the inference for various parameters.
              ex: -m model.gguf
  --perplexity (-p): Measure the perplexity of a model over a given text.
              ex: -m model.gguf -f file.txt
  --convert (-c): Convert a llama model into ggml
              ex: --outtype f16 "/models/7B/" 
  --quantize (-q): Optimize with quantization process ggml
              ex: "/models/7B/ggml-model-f16.bin" "/models/7B/ggml-model-q4_0.bin" 2
  --all-in-one (-a): Execute --convert & --quantize
              ex: "/models/" 7B
  --server (-s): Run a model on the server
              ex: -m /models/7B/ggml-model-q4_0.bin -c 2048 -ngl 43 -mg 1 --port 8080</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>We want to leverage <span class="bold">llama.cpp</span> to serve an LLM model for inference, so we will use the <span class
        ="hi-green">--server</span> command.</p>
    </div>
    <div id="para-div">
      <p>To determine all the options for the <span class="bold">--server</span> command, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name llama_cpp ghcr.io/ggml-org/llama.cpp:full-cuda-b7524 --server --help</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The output will be very long so will <span class="underbold">NOT</span> show it here.</p>
    </div>
    <div id="para-div">
      <p>The next step is to identify the Nvidia GPU device(s) on the desktop. To identify the CUDA device(s), execute the following
        command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name llama_cpp --gpus all ghcr.io/ggml-org/llama.cpp:full-cuda-b7524 --server --list-devices</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /app/libggml-cuda.so
load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
Available devices:
  CUDA0: NVIDIA GeForce RTX 4060 Ti (15944 MiB, 15079 MiB free)</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>From the Output.3 above, the CUDA device is <span class="underbold">CUDA0</span>.</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following table summarizes the commonly used options with the <span class="bold">--server</span> command:</p>
      <br/>
      <table id="col2-table">
        <thead>
          <tr>
            <th>Option</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="col2-c1-odd"><span class="bold">--model</span></td>
            <td class="col2-c2-odd">Specifies the path to the LLM model GGUF file being served</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">--n-gpu-layers</span></td>
            <td class="col2-c2-even">Sets the number of layers to offload to the GPU device. If not set (or set to 0), the model runs on the CPU</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">--device</span></td>
            <td class="col2-c2-odd">Specifies the backend device to use</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">--predict</span></td>
            <td class="col2-c2-even">The maximum number of tokens to generate. The default is -1 which indicates infinite generation</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">--temperature</span></td>
            <td class="col2-c2-odd">Controls the creativity and randomness of the output. Lower values indicate more deterministic output</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">--top-k</span></td>
            <td class="col2-c2-even">Sets sampling where only tokens whose cumulative probability exceeds the specified threshold</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">--ctx-size</span></td>
            <td class="col2-c2-odd">Sets the number of tokens that the model can remember at any time during output text generation</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">--flash-attn</span></td>
            <td class="col2-c2-even">Enables flash attention for faster inference. The default value is 'auto'</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">--alias</span></td>
            <td class="col2-c2-odd">Specifies the model name that can be used in the APIs. Default is the model name</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">--host</span></td>
            <td class="col2-c2-even">Indicates the host to use for the server</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">--port</span></td>
            <td class="col2-c2-odd">Indicates the port to use for the server</td>
          </tr>
          <tr>
            <td class="col2-c1-even"><span class="bold">--threads</span></td>
            <td class="col2-c2-even">Indicates the number of CPU threads to use during generation. Default is all CPU threads</td>
          </tr>
          <tr>
            <td class="col2-c1-odd"><span class="bold">--log-timestamps</span></td>
            <td class="col2-c2-odd">Enable timestamps in log messages</td>
          </tr>
        </tbody>
      </table>
    </div>
    <br/>
    <div id="para-div">
      <p>For the hands-on demostrations, we will download and use two models from Huggingface - first is the <span class="hi-purple">
        Gemma 3N 4B</span> LLM model and the other is the <span class="hi-purple">Gemma 3 4B</span> LLM model.</p>
    </div>
    <div id="para-div">
      <p>We will start with the <span class="bold">Gemma 3N 4B</span> LLM model. To download and serve the desired LLM model, execute
        the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name llama_cpp --gpus all --network host -v $HOME/.llama_cpp/models:/root/.cache/llama.cpp ghcr.io/ggml-org/llama.cpp:full-cuda-b7524 --server --hf-repo unsloth/gemma-3n-E4B-it-GGUF --host 192.168.1.25 --port 8000 --device CUDA0 --temp 0.2 --log-timestamps</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical trimmed output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /app/libggml-cuda.so
load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gemma-3n-E4B-it-GGUF_gemma-3n-E4B-it-Q4_K_M.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf to /root/.cache/llama.cpp/unsloth_gemma-3n-E4B-it-GGUF_gemma-3n-E4B-it-Q4_K_M.gguf.downloadInProgress (server_etag:"362b2c2e867b05e30a9315c640fd66ae953005ea1d2c40b8ce37b0066603a8f2", server_last_modified:)...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1345  100  1345    0     0  33315      0 --:--:-- --:--:-- --:--:-- 33315
100 4328M  100 4328M    0     0  98.4M      0  0:00:43  0:00:43 --:--:-- 98.8M
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7524 (5ee4e43f2) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 8, n_threads_batch = 8, total_threads = 16

system_info: n_threads = 8 (n_threads_batch = 8) / 16 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

init: using 15 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gemma-3n-E4B-it-GGUF_gemma-3n-E4B-it-Q4_K_M.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
llama_params_fit_impl: projected to use 5174 MiB of device memory vs. 15944 MiB of free device memory
llama_params_fit_impl: will leave 9910 >= 1024 MiB of free device memory, no changes needed
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 0.41 seconds
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Ti) (0000:04:00.0) - 15084 MiB free
llama_model_loader: loaded meta data with 51 key-value pairs and 847 tensors from /root/.cache/llama.cpp/unsloth_gemma-3n-E4B-it-GGUF_gemma-3n-E4B-it-Q4_K_M.gguf (version GGUF V3 (latest))
.....[ TRIM ].....
main: model loaded
main: server is listening on http://192.168.1.25:8000
main: starting the main loop...
srv  update_slots: all slots are idle</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, launch the <span class="bold">Web Browser</span> and open the URL <span class="bold">http://192.168.1.25:8000</span>.
        The following illustration depicts the <span class="bold">llama.cpp</span> user interface:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-1.png" alt="Llama.cpp Web UI" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Enter the user prompt in the text box at the bottom and click on the circled UP arrow (indicated by the red arrow) as shown
        in the illustration below:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-2.png" alt="Llama.cpp LLM Prompt" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The response from the LLM is as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-3.png" alt="Llama.cpp LLM Response" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Next, enter the user prompt (for solving a logical puzzle) in the text box at the bottom and click on the circled UP arrow
        (indicated by the red arrow) as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-4.png" alt="Llama.cpp LLM Puzzle Prompt" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The response from the LLM is as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-5.png" alt="Llama.cpp LLM Puzzle Response" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Exit the browser and stop the <span class="bold">llama.cpp</span> server running in the terminal window.</p>
    </div>
    <div id="para-div">
      <p>Shifting gears, we will now move to the <span class="bold">Gemma 3 4B</span> LLM model. To download and serve the desired
        LLM model, execute the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name llama_cpp --gpus all --network host -v $HOME/.llama_cpp/models:/root/.cache/llama.cpp ghcr.io/ggml-org/llama.cpp:full-cuda-b7524 --server --hf-repo unsloth/gemma-3-4b-it-GGUF:Q4_K_XL --host 192.168.1.25 --port 8000 --device CUDA0 --temp 0.2 --log-timestamps</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical trimmed output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /app/libggml-cuda.so
load_backend: loaded CPU backend from /app/libggml-cpu-haswell.so
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_gemma-3-4b-it-UD-Q4_K_XL.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.gguf to /root/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_gemma-3-4b-it-UD-Q4_K_XL.gguf.downloadInProgress (server_etag:"a855cedae6f99feeb0ff5f7e46eb02d03a3db3a206034065509e809f7e89ea45", server_last_modified:)...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1343  100  1343    0     0  50992      0 --:--:-- --:--:-- --:--:-- 50992
100 2426M  100 2426M    0     0   100M      0  0:00:24  0:00:24 --:--:--  100M
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_mmproj-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/mmproj-F16.gguf to /root/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_mmproj-F16.gguf.downloadInProgress (server_etag:"2182a97cbae2f01c910d358024e46f26303ace2481dea400605a688fbebaaee2", server_last_modified:)...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1315  100  1315    0     0  22712      0 --:--:-- --:--:-- --:--:-- 22712
100  811M  100  811M    0     0   104M      0  0:00:07  0:00:07 --:--:--  106M
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7524 (5ee4e43f2) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 8, n_threads_batch = 8, total_threads = 16

system_info: n_threads = 8 (n_threads_batch = 8) / 16 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

init: using 15 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_gemma-3-4b-it-UD-Q4_K_XL.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
llama_params_fit_impl: projected to use 6019 MiB of device memory vs. 15944 MiB of free device memory
llama_params_fit_impl: will leave 8889 >= 1024 MiB of free device memory, no changes needed
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 0.37 seconds
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Ti) (0000:04:00.0) - 14908 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from /root/.cache/llama.cpp/unsloth_gemma-3-4b-it-GGUF_gemma-3-4b-it-UD-Q4_K_XL.gguf (version GGUF V3 (latest))
.....[ TRIM ].....
main: model loaded
main: server is listening on http://192.168.1.25:8000
main: starting the main loop...
srv  update_slots: all slots are idle</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Once again, launch the <span class="bold">Web Browser</span> and open the URL <span class="bold">http://192.168.1.25:8000</span>.
    </div>
    <div id="para-div">
      <p>For the next task, we will use the bank check as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-6.png" alt="Llama.cpp Bank Check" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Click on the attachment icon (red number 1 arrow) to attach the above bank check image, then enter the user prompt in the
        text box at the bottom and click on the circled UP arrow (red number 2 arrow) as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-7.png" alt="Llama.cpp Attach Check" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The response from the LLM after processing the bank check image is as shown in the illustration below:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/llama_cpp-8.png" alt="Llama.cpp LLM Check Response" />
      <div class="img-cap">Figure.8</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Next, to test the <span class="bold">llama.cpp</span> inference platform via the API endpoint, execute the following user
        prompt in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ curl -s -X POST "http://192.168.1.25:8000/completion" -H "Content-Type: application/json" -d '{"prompt": "Describe an ESP32 microcontroller in less than 100 words", "max_tokens": 150}' | jq</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>{
  "index": 0,
  "content": ".\n\nThe ESP32 is a powerful, low-cost microcontroller packed with Wi-Fi and Bluetooth capabilities. It's ideal for IoT projects, connecting devices to the internet, and creating smart home applications. Featuring a dual-core processor, it can handle complex tasks and supports various communication protocols.  It's popular among hobbyists and professionals alike due to its versatility, ease of use, and extensive community support.\n",
  "tokens": [],
  "id_slot": 3,
  "stop": true,
  "model": "unsloth/gemma-3-4b-it-GGUF:Q4_K_XL",
  "tokens_predicted": 88,
  "tokens_evaluated": 15,
  "generation_settings": {
    "seed": 4294967295,
    "temperature": 0.20000000298023224,
    "dynatemp_range": 0.0,
    "dynatemp_exponent": 1.0,
    "top_k": 40,
    "top_p": 0.949999988079071,
    "min_p": 0.05000000074505806,
    "top_n_sigma": -1.0,
    "xtc_probability": 0.0,
    "xtc_threshold": 0.10000000149011612,
    "typical_p": 1.0,
    "repeat_last_n": 64,
    "repeat_penalty": 1.0,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "dry_multiplier": 0.0,
    "dry_base": 1.75,
    "dry_allowed_length": 2,
    "dry_penalty_last_n": 131072,
    "dry_sequence_breakers": [
      "\n",
      ":",
      "\"",
      "*"
    ],
    "mirostat": 0,
    "mirostat_tau": 5.0,
    "mirostat_eta": 0.10000000149011612,
    "stop": [],
    "max_tokens": 150,
    "n_predict": 150,
    "n_keep": 0,
    "n_discard": 0,
    "ignore_eos": false,
    "stream": false,
    "logit_bias": [],
    "n_probs": 0,
    "min_keep": 0,
    "grammar": "",
    "grammar_lazy": false,
    "grammar_triggers": [],
    "preserved_tokens": [],
    "chat_format": "Content-only",
    "reasoning_format": "deepseek",
    "reasoning_in_content": false,
    "thinking_forced_open": false,
    "samplers": [
      "penalties",
      "dry",
      "top_n_sigma",
      "top_k",
      "typ_p",
      "top_p",
      "min_p",
      "xtc",
      "temperature"
    ],
    "speculative.n_max": 16,
    "speculative.n_min": 0,
    "speculative.p_min": 0.75,
    "timings_per_token": false,
    "post_sampling_probs": false,
    "lora": []
  },
  "prompt": "&lt;bos&gt;Describe an ESP32 microcontroller in less than 100 words",
  "has_new_line": true,
  "truncated": false,
  "stop_type": "eos",
  "stopping_word": "",
  "tokens_cached": 102,
  "timings": {
    "cache_n": 0,
    "prompt_n": 15,
    "prompt_ms": 43.712,
    "prompt_per_token_ms": 2.9141333333333335,
    "prompt_per_second": 343.15519765739384,
    "predicted_n": 88,
    "predicted_ms": 1046.312,
    "predicted_per_token_ms": 11.88990909090909,
    "predicted_per_second": 84.10493237198848
  }
}
</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>We have successfully tested the exposed local API endpoint from the command-line !</p>
    </div>
    <div id="para-div">
      <p>Now, we will test <span class="bold">llama.cpp</span> using the Python <span class="bold">Langchain</span> API code snippets.</p>
    </div>
    <div id="para-div">
      <p>Create a file called <span class="hi-yellow">.env</span> with the following environment variables defined:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>LLM_TEMPERATURE=0.2
LLAMA_CPP_BASE_URL='http://192.168.1.25:8000/v1'
LLAMA_CPP_MODEL='unsloth/gemma-3-4b-it-GGUF:Q4_K_XL'
LLAMA_CPP_API_KEY='llama_cpp'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To load the environment variables and assign them to corresponding <span class="bold">Python</span> variables, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from dotenv import load_dotenv, find_dotenv

import os

load_dotenv(find_dotenv())

llm_temperature = os.getenv('LLM_TEMPERATURE')
llama_cpp_base_url = os.getenv('LLAMA_CPP_BASE_URL')
llama_cpp_model = os.getenv('LLAMA_CPP_MODEL')
llama_cpp_api_key = os.getenv('LLAMA_CPP_API_KEY')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of the LLM client class for <span class="bold">OpenAI</span> running on the host URL, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_openai import ChatOpenAI

llm_openai = ChatOpenAI(
  model=llama_cpp_model,
  base_url=llama_cpp_base_url,
  api_key=llama_cpp_api_key,
  temperature=float(llm_temperature)
)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To get a text response for a user prompt from the Gemma 3 4B LLM model running on the <span class="bold">llama.cpp</span>
        inference platform, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
  ('system', 'You are a helpful assistant'),
  ('human', '{input}'),
])

chain = prompt | llm_openai

response = chain.invoke({'input': 'Compare the GDP of India vs USA in 2024 and provide the response in JSON format'})
response.content</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>'```json\n{\n  "comparison": "GDP Comparison - India vs. USA (2024 - Estimates)",\n  "date": "October 26, 2023 (Based on latest available estimates and projections)",\n  "disclaimer": "Figures are estimates and projections, subject to change based on economic conditions and revisions by international organizations.  Data is primarily sourced from the World Bank, IMF, and Trading Economics.",\n  "data": {\n    "country": "United States",\n    "gdp_2024_estimate": {\n      "value": 27.64 trillion,\n      "unit": "USD (United States Dollars)",\n      "source": "IMF (October 2023)",\n      "currency_type": "USD"\n    },\n    "country": "India",\n    "gdp_2024_estimate": {\n      "value": 13.96 trillion,\n      "unit": "USD (United States Dollars)",\n      "source": "IMF (October 2023)",\n      "currency_type": "USD"\n    },\n    "comparison_summary": {\n      "percentage_difference": "The US GDP is approximately 2.08 times larger than India\'s GDP in 2024.",\n      "percentage_change_india": "India\'s GDP is projected to grow by approximately 6.3% in 2024.",\n      "percentage_change_usa": "The US GDP is projected to grow by approximately 2.1% in 2024."\n    },\n    "notes": [\n      "These are nominal GDP figures (current prices).",\n      "GDP per capita (average economic output per person) is significantly higher in the United States.",\n      "Economic forecasts are inherently uncertain and subject to revision."\n    ]\n  }\n}\n```\n\n**Explanation of the JSON:**\n\n*   **`comparison`**: A brief title for the comparison.\n*   **`date`**:  The date the information was compiled (based on available estimates).\n*   **`disclaimer`**:  Important note about the estimates and their potential for change.\n*   **`data`**:  Contains the GDP figures and sources.\n    *   **`country`**:  The name of the country.\n    *   **`gdp_2024_estimate`**:  Details about the GDP estimate for that year.\n        *   **`value`**: The GDP value in USD.\n        *   **`unit`**: The unit of measurement (USD).\n        *   **`source`**: The organization that provided the estimate (IMF in this case).\n        *   **`currency_type`**: The currency type.\n    *   **`comparison_summary`**:  Provides a concise summary of the comparison.\n        *   **`percentage_difference`**:  Calculates the percentage difference between the two countries\' GDPs.\n        *   **`percentage_change_india`**:  The projected percentage change in India\'s GDP.\n        *   **`percentage_change_usa`**: The projected percentage change in the USA\'s GDP.\n*   **`notes`**:  Additional context and caveats.\n\n**Important Considerations:**\n\n*   **Estimates:**  As noted in the disclaimer, these are *estimates* for 2024.  Final figures will be released later in the year and may vary.\n*   **Sources:**  I\'ve primarily used the IMF\'s World Economic Outlook (October 2023) as my source.  You can find the full report on the IMF website.  Other sources like the World Bank and Trading Economics were consulted for corroboration.\n*   **Nominal vs. Real GDP:** This response uses *nominal* GDP (current prices).  *Real* GDP (adjusted for inflation) would provide a more accurate comparison of economic growth over time.\n\nTo get the most up-to-date information, I recommend checking the following resources:\n\n*   **International Monetary Fund (IMF):** [https://www.imf.org/](https://www.imf.org/)\n*   **World Bank:** [https://www.worldbank.org/](https://www.worldbank.org/)\n*   **Trading Economics:** [https://tradingeconomics.com/](https://tradingeconomics.com/)'</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For the next task, we will attempt to present the LLM model response in a structured form using a Pydantic data class. For
        that, we will first define a class object by executing the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic import BaseModel, Field

class GeographicInfo(BaseModel):
  country: str = Field(description="Name of the Country")
  capital: str = Field(description="Name of the Capital City")
  population: int = Field(description="Population of the country in billions")
  land_area: int = Field(description="Land Area of the country in square miles")
  list_of_rivers: list = Field(description="List of top 5 rivers in the country")</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To receive a LLM model response in the desired format for the specific user prompt from the <span class="bold">llama.cpp</span>
        platform, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>struct_llm_openai = llm_openai.with_structured_output(GeographicInfo)

chain = prompt | struct_llm_openai

response = chain.invoke({'input': 'Provide the geographic information of India and include the capital city, population in billions, land area in square miles, and list of rivers'})
response</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>GeographicInfo(country='India', capital='New Delhi', population=1428000000, land_area=3287263, list_of_rivers=[{'river_name': 'Ganges (Ganga)', 'significance': 'Sacred river, central to Hindu religion and culture; major waterway for transportation and irrigation.'}, {'river_name': 'Yamuna', 'significance': 'Major tributary of the Ganges, flows through Delhi and Agra.'}, {'river_name': 'Indus', 'significance': 'Historically important river, now largely in Pakistan; significant for irrigation and water resources.'}, {'river_name': 'Brahmaputra', 'significance': 'Originates in Tibet, flows through India and Bangladesh; vital for agriculture and flood control.'}, {'river_name': 'Narmada', 'significance': 'Considered sacred, flows through central India; important for irrigation and pilgrimage.'}, {'river_name': 'Godavari', 'significance': 'Major river in South India, considered sacred; important for agriculture and religious festivals.'}, {'river_name': 'Krishna', 'significance': 'Major river in South India, crucial for irrigation and water supply.'}, {'river_name': 'Mahanadi', 'significance': 'Important river in eastern India, flows through Odisha; vital for irrigation and fisheries.'}, {'river_name': 'Kaveri (Cauvery)', 'significance': 'Major river in southern India, important for agriculture and water resources.'}, {'river_name': 'Tapi', 'significance': 'Flows through western India, used for irrigation and hydroelectric power.'}])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>With this, we conclude the various demonstrations on using the <span class="bold">llama.cpp</span> platform for running
        and working with the pre-trained LLM model(s) locally !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://github.com/ggml-org/llama.cpp" target="_blank"><span class="bold">llama.cpp</span></a></p>
      <p><a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md" target="_blank"><span class="bold">llama.cpp Docker</span></a></p>
      <p><a href="https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md" target="_blank"><span class="bold">llama.cpp Server Options</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
