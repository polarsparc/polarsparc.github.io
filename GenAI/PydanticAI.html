<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on PydanticAI">
    <meta name="subject" content="Quick Primer on PydanticAI">
    <meta name="keywords" content="ollama, pydantic_ai, llm, agentic">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on PydanticAI</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on PydanticAI</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">05/03/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr"/>
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><a href="https://ai.pydantic.dev/" target="_blank"><span class="hi-yellow">PydanticAI</span></a> is an open-source, Python
        based agentic framework that aims to make it easy for application developers to build production grade Gen AI apps that can
        interface with different <span class="bold">LLM</span> models.</p>
      <p>The following are the list of some of the core classes from <span class="bold">PydanticAI</span> framework:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span class="hi-grey">Agent</span> :: is the primary abstraction for interacting with the LLM models and is a container
            for the following:</p>
          <ul id="blue-disc-ul">
            <li><p><span class="hi-vanila">LLM Model</span> - a model associated with the agent</p></li>
            <li><p><span class="hi-vanila">System Prompt(s)</span> - a set of instructions provided by a user to guide the model</p></li>
            <li><p><span class="hi-vanila">Structured Output Type</span> - model results returned in the specified structured data
              format</p></li>
            <li><p><span class="hi-vanila">Model Setting(s)</span> - optional model settings to control the model behavior</p></li>
          </ul>
        </li>
        <li>
          <p><span class="hi-grey">Model</span> :: refers to the class that implements a portable and vendor agnostic API to make
            requests to the underying LLM model(s). For interacting with LLM model(s) that are served via <span class="bold">Ollama
            </span>, one must use the <span class="hi-vanila">OpenAIModel</span> class. The <span class="bold">Model</span> class
            uses an instance of a <span class="hi-vanila">Provider</span> class, which encapsulates various parameters such as, the
            endpoint URL, API key, etc., to connect and make requests to the underlying LLM model.</p>
        </li>
        <li>
          <p><span class="hi-grey">Funtion Tools</span> :: are mechanisms for the LLM model(s) to retrieve information from external
            sources in order to generate responses to user prompts. They are useful for augmenting on what the LLM model(s) can do.
            The following are the two ways to register a Python function as a <span class="bold">Function Tool</span>:</p>
          <ul id="blue-disc-ul">
            <li><p><span class="hi-vanila">@agent.tool_plain</span> decorator for Python function(s) that do not need access to any
              LLM model information context</p></li>
            <li><p><span class="hi-vanila">@agent.tool</span> decorator for Python function(s) that need access to all the LLM model
              information context</p></li>
          </ul>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>The intent of this article is <span class="underbold">NOT</span> to be exhaustive, but a primer to get started quickly.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Installation and Setup</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will be on a <span class="bold">Ubuntu 24.04 LTS</span> based Linux desktop. Ensure that <span
        class="bold">Ollama</span> is installed and setup on the desktop (see <a href="http://polarsparc.github.io/GenAI/Ollama.html"
        target="_blank"><span class="bold">instructions</span></a>).</p>
      <p>In addition, ensure that the <span class="bold">Python 3.1x</span> programming language as well as the <span class="bold">
        Jupyter Notebook</span> package is installed and setup on the desktop.</p>
    </div>
    <div id="para-div">
      <p>Assuming that the ip address on the Linux desktop is <span class="hi-grey">192.168.1.25</span>, start the <span class="bold">
        Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama --network=host -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.6.7</p>
    </div>
    <br/>
    <div id="para-div">
      <p>If the linux desktop has <span class="hi-green">Nvidia GPU</span> with decent amount of VRAM (at least 16 GB) and has been
        enabled for use with <span class="bold">docker</span> (see <a href="https://polarsparc.github.io/Docker/DockerNVidia.html"
        target="_blank"><span class="bold">instructions</span></a>), then execute the following command instead to start <span class
        ="bold">Ollama</span>:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama --gpus=all --network=host -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.6.7</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For the LLM model, we will be using the recently released <span class="hi-purple">Qwen 3 4B</span> model.</p>
    </div>
    <div id="para-div">
      <p>Open a new terminal window and execute the following <span class="bold">docker</span> command to download the LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run qwen3:4b</p>
    </div>
    <br/>
    <div id="para-div">
      <p>To install the necessary <span class="bold">Python</span> modules for this primer, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ pip install dotenv openai pydantic pydantic-ai pydantic-ai-slim</p>
    </div>
    <br/>
    <div id="para-div">
      <p>This completes all the installation and setup for the <span class="bold">PydanticAI</span> hands-on demonstrations.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on with PydanticAI</p>
    </div>
    <div id="para-div">
      <p>Create a file called <span class="hi-yellow">.env</span> with the following environment variables defined:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>LLM_MAX_RETRIES=5
LLM_TEMPERATURE=0.2
OLLAMA_MODEL='qwen3:4b'
OLLAMA_BASE_URL='http://192.168.1.25:11434/v1'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To load the environment variables and assign them to corresponding <span class="bold">Python</span> variables, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from dotenv import load_dotenv, find_dotenv

import os

load_dotenv(find_dotenv())

llm_max_retries = int(os.getenv('LLM_MAX_RETRIES'))
llm_temperature = float(os.getenv('LLM_TEMPERATURE'))
ollama_model = os.getenv('OLLAMA_MODEL')
ollama_base_url = os.getenv('OLLAMA_BASE_URL')
ollama_api_key = 'ollama'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of the <span class="bold">Provider</span> class for the <span class="bold">Ollama</span> running
        on the host URL, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic_ai.providers.openai import OpenAIProvider

llm_provider = OpenAIProvider(base_url=ollama_base_url, api_key=ollama_api_key)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of the <span class="bold">Model</span> class for the <span class="bold">Ollama</span> running the
        <span class="hi-purple">qwen3:4b</span> model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic_ai.models.openai import OpenAIModel

ollama_model = OpenAIModel(model_name=ollama_model, provider=llm_provider)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of the <span class="bold">ModelSetttings</span> class for the <span class="bold">Ollama</span>
        platform running the desired LLM model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic_ai.settings import ModelSettings

llm_model_settings = ModelSettings(temperature=llm_temperature)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">temperature</span> parameter in the above code is a value that is between 0.0 and 1.0. It determines
        whether the output from the LLM model should be more "creative" or be more "predictive". A higher value means more "creative"
        and a lower value means more "predictive".</p>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of the <span class="bold">Agent</span> class with a custom system prompt, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic_ai import Agent

ai_agent = Agent(ollama_model,
retries=llm_max_retries,
system_prompt=('You are a helpful mathematical genius',
               'Your final answer should be in the form of a python dictionary'
              )
)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To test the agent with a user prompt, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response = ai_agent.run_sync('Which of the two numbers is bigger - 9.01234 vs 9.01234',
                              model_settings=llm_model_settings)
print(response.output)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code would generate the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>&lt;think&gt;
Okay, let's see. The user is asking which number is bigger between 9.01234 and 9.01234. Wait, that's the same number twice. So, they must have made a typo or maybe a trick question. Let me check again.

The numbers given are both 9.01234. So, they are exactly the same. Therefore, neither is bigger than the other. They are equal. But the user might be testing if I notice that they are the same. Maybe they intended to have different numbers but made a mistake. But according to the question as stated, both numbers are identical. So the answer is that they are equal. But how to present that in a Python dictionary? The user said the final answer should be a Python dictionary. Maybe like {'result': 'equal'} or something. But I need to make sure. Let me confirm once more. The numbers are 9.01234 and 9.01234. So, they are the same. So the answer is that they are equal. So the dictionary would have a key like 'comparison' with the value indicating they are equal. Alternatively, maybe the user intended different numbers but there's a typo. But given the numbers as written, they are the same. So the correct answer is that they are equal.
&lt;/think&gt;

{"comparison": "equal"}</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For the next demonstration on structured output, we will create a <span class="bold">Pydantic</span> class that will be
        used to capture basic geographic information of a country. To create our <span class="bold">Pydantic</span> class, execute
        the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic import BaseModel, Field

class GeographicInfo(BaseModel):
  country: str = Field(description="Name of the Country")
  capital: str = Field(description="Name of the Capital City")
  population: int = Field(description="Population of the country in billions")
  land_area: int = Field(description="Land Area of the country in square miles")
  list_of_rivers: list = Field(description="List of top 5 rivers in the country")</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To initialize an new instance of the <span class="bold">Agent</span> class for the structured output, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>ai_agent2 = Agent(ollama_model,
                  retries=llm_max_retries,
                  output_type=GeographicInfo,
                  model_settings=llm_model_settings)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To test the agent with a user prompt, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response2 = ai_agent2.run_sync('Get the Geographic Info for India')
print(response2.output)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code would generate the following typical output:</p>
    </div>
    <br/>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>AgentRunResult(output=GeographicInfo(country='India', capital='New Delhi', population=1400000000, land_area=1269000, list_of_rivers=['Ganges', 'Brahmaputra', 'Yamuna', 'Godavari', 'Kaveri']))</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The next demonstration is on the use of tools. To initialize an new instance of the <span class="bold">Agent</span> class
        for invoking specific tools, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>ai_agent3 = Agent(ollama_model,
                  retries=llm_max_retries,
                  model_settings=llm_model_settings,
                  system_prompt='Execute the appropriate tool to complete the specific task')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>For this demonstration, we will make use of three custom tools - one to compute the simple interest for a year, second to
        compute the compound interest for a year, and third to execute Linux shell commands.
      </p>
    </div>
    <div id="para-div">
      <p>To create the three tools that can be invoked by the agent, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>@ai_agent3.tool_plain
async def yearly_simple_interest(principal: float, rate:float) -> float:
  """Tool to compute simple interest rate for a year."""
  print(f'Simple interest -> Principal: {principal}, Rate: {rate}')
  return principal * rate / 100.00

@ai_agent3.tool_plain
async def yearly_compound_interest(principal: float, rate:float) -> float:
  """Tool to compute compound interest rate for a year."""
  print(f'Compound interest -> Principal: {principal}, Rate: {rate}')
  return principal * (1 + rate / 100.0)

@ai_agent3.tool_plain
async def execute_shell_command(command: str) -> str:
  """Tool to execute shell commands"""
  print(f'Executing shell command: {command}')
  try:
    result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)
    if result.returncode != 0:
      return f'Error executing shell command - {command}'
    return result.stdout
  except subprocess.CalledProcessError as e:
    print(e)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice the use of the <span class="hi-yellow">@ai_agent3.tool_plain</span> decorator on the above functions.</p>
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To test the agent with a user prompt for computing simple interest, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response3 = ai_agent3.run_sync('find the simple interest for a principal of 1000 at a rate of 4.25')
print(response3.all_messages())</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code would generate the following typical output:</p>
    </div>
    <br/>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>[ModelRequest(parts=[SystemPromptPart(content='Execute the appropriate tool to complete the specific task', timestamp=datetime.datetime(2025, 5, 3, 23, 11, 18, 40859, tzinfo=datetime.timezone.utc), dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='find the simple interest for a principal of 1000 at a rate of 4.25', timestamp=datetime.datetime(2025, 5, 3, 23, 11, 18, 40864, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], instructions=None, kind='request'), ModelResponse(parts=[TextPart(content='', part_kind='text'), ToolCallPart(tool_name='yearly_simple_interest', args='{"principal":1000,"rate":4.25}', tool_call_id='call_h2x7on90', part_kind='tool-call')], model_name='qwen3:4b', timestamp=datetime.datetime(2025, 5, 3, 23, 11, 22, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='yearly_simple_interest', content=42.5, tool_call_id='call_h2x7on90', timestamp=datetime.datetime(2025, 5, 3, 23, 11, 22, 148577, tzinfo=datetime.timezone.utc), part_kind='tool-return')], instructions=None, kind='request'), ModelResponse(parts=[TextPart(content="<think>\nOkay, the user asked for the simple interest on a principal of 1000 at 4.25%. Let me check the tools available. There's a function called yearly_simple_interest that takes principal and rate. The parameters are required, so I need to plug in 1000 and 4.25. The calculation should be straightforward: (1000 * 4.25% * 1) = 42.5. The tool response confirmed 42.5, so that's correct. I should present the answer clearly, maybe format the rate as a percentage for readability. Let me make sure the explanation is simple and direct.\n</think>\n\nThe simple interest for a principal of $1000 at a rate of 4.25% over one year is **$42.50**. \n\n**Calculation:**  \nSimple Interest = Principal × Rate × Time  \n= 1000 × 0.0425 × 1  \n= $42.50", part_kind='text')], model_name='qwen3:4b', timestamp=datetime.datetime(2025, 5, 3, 23, 11, 25, tzinfo=datetime.timezone.utc), kind='response')]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Next, to test the agent with a user prompt for computing compound interest, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response4 = ai_agent3.run_sync('find the compound interest for a principal of 1000 at a rate of 4.25')
print(response4.all_messages())</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code would generate the following typical output:</p>
    </div>
    <br/>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>[ModelRequest(parts=[SystemPromptPart(content='Execute the appropriate tool to complete the specific task', timestamp=datetime.datetime(2025, 5, 3, 23, 12, 24, 176111, tzinfo=datetime.timezone.utc), dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='find the compound interest for a principal of 1000 at a rate of 4.25', timestamp=datetime.datetime(2025, 5, 3, 23, 12, 24, 176115, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], instructions=None, kind='request'), ModelResponse(parts=[TextPart(content='', part_kind='text'), ToolCallPart(tool_name='yearly_compound_interest', args='{"principal":1000,"rate":4.25}', tool_call_id='call_lot80b2k', part_kind='tool-call')], model_name='qwen3:4b', timestamp=datetime.datetime(2025, 5, 3, 23, 12, 31, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='yearly_compound_interest', content=1042.5, tool_call_id='call_lot80b2k', timestamp=datetime.datetime(2025, 5, 3, 23, 12, 31, 241071, tzinfo=datetime.timezone.utc), part_kind='tool-return')], instructions=None, kind='request'), ModelResponse(parts=[TextPart(content="<think>\nOkay, the user asked for the compound interest on a principal of 1000 at a rate of 4.25. Let me check the tools available. There's a function called yearly_compound_interest that takes principal and rate as parameters. The parameters are required, so I need to make sure both are included. The principal is 1000 and the rate is 4.25. I should call that function with these values. The previous response from the tool gave 1042.5, which seems correct. Let me confirm the calculation. Compound interest formula is A = P(1 + r/n)^(nt). Since it's annual, n=1. So 1000*(1 + 0.0425)^1 = 1000*1.0425 = 1042.5. Yep, that's right. So the answer is 1042.5.\n</think>\n\nThe compound interest for a principal of 1000 at a rate of 4.25 over a year is 1042.5.", part_kind='text')], model_name='qwen3:4b', timestamp=datetime.datetime(2025, 5, 3, 23, 12, 34, tzinfo=datetime.timezone.utc), kind='response')]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Again, to test the agent with a user prompt for finding the ip address on the system, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response5 = ai_agent3.run_sync(user_prompt=('Determine the ip address on the system by executing the appropriate linux command',
                                            'The final answer should be in the form of a python dictionary'))
print(response5.all_messages())</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code would generate the following typical output:</p>
    </div>
    <br/>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>[ModelRequest(parts=[SystemPromptPart(content='Execute the appropriate tool to complete the specific task', timestamp=datetime.datetime(2025, 5, 3, 23, 58, 8, 140831, tzinfo=datetime.timezone.utc), dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content=('Determine the ip address on the system by executing the appropriate linux command', 'The final answer should be in the form of a python dictionary'), timestamp=datetime.datetime(2025, 5, 3, 23, 58, 8, 140836, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], instructions=None, kind='request'), ModelResponse(parts=[TextPart(content='', part_kind='text'), ToolCallPart(tool_name='execute_shell_command', args='{"command":"hostname -I"}', tool_call_id='call_5bhg20tg', part_kind='tool-call')], model_name='qwen3:4b', timestamp=datetime.datetime(2025, 5, 3, 23, 58, 15, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='execute_shell_command', content='192.168.1.25 172.17.0.1 172.18.0.1 \n', tool_call_id='call_5bhg20tg', timestamp=datetime.datetime(2025, 5, 3, 23, 58, 15, 403400, tzinfo=datetime.timezone.utc), part_kind='tool-return')], instructions=None, kind='request'), ModelResponse(parts=[TextPart(content='<think>\nOkay, the user wants to determine the IP address on their system using a Linux command. Let me think about the appropriate command for that.\n\nI remember that the \'ifconfig\' command is used to check network interfaces and their IP addresses. But wait, some systems might use \'ip addr\' instead. However, the user specified to execute the appropriate Linux command, so I should check which one is more commonly used.\n\nAlternatively, maybe \'hostname -I\' could list all IP addresses. But the user might need the most accurate method. Let me recall: \'ifconfig\' is traditional, but \'ip addr show\' is more modern. However, the user might not have \'ifconfig\' installed, depending on their system configuration.\n\nWait, the user is asking for the IP address, so perhaps the most reliable way is to use \'hostname -I\' which prints all IP addresses. But if they need a specific one, maybe \'ip addr show\' would be better. However, the user didn\'t specify which one, so I need to choose the most common command.\n\nAlternatively, maybe the user is on a system where \'ifconfig\' is available. Let me check the functions available. The tool provided is \'execute_shell_command\', so I can run any command.\n\nBut the user\'s answer was given as 192.168.1.25, 172.17.0.1, 172.18.0.1. So maybe the command \'ifconfig\' was used. Let me check: when you run \'ifconfig\', it shows the interfaces and their IP addresses. For example, eth0 might have 192.168.1.25, and others. So the user might have run \'ifconfig\' and got those IPs.\n\nSo the correct command would be \'ifconfig\' or \'ip addr show\'. But since the user\'s answer includes multiple IPs, perhaps the command is \'ifconfig\' or \'hostname -I\'.\n\nBut the user\'s answer shows three IPs, which might be multiple interfaces. So the correct command is \'ifconfig\' or \'ip addr show\'.\n\nBut the user\'s answer is given as the output of the command. So the assistant should have executed the command and returned the IPs. The user\'s answer is the output of the command. So the assistant\'s response is to call the execute_shell_command function with the appropriate command.\n\nSo the correct function call would be to execute \'ifconfig\' or \'hostname -I\'. Let me check which one is more likely. The user\'s answer shows multiple IPs, which \'ifconfig\' would display. So the assistant should call \'ifconfig\' or \'ip addr show\'.\n\nBut the user\'s answer is the output of the command. So the assistant should call the execute_shell_command function with the command \'ifconfig\' or \'hostname -I\'.\n\nBut the user\'s answer is given as the output, so the assistant\'s response is to call the function with the correct command. The assistant\'s previous response was to call \'execute_shell_command\' with \'ifconfig\', and the output was the IPs. So the correct answer is the IPs, which are in the tool_response.\n\nSo the final answer should be a Python dictionary with the IP addresses. The user\'s answer is the output of the command, so the assistant should return that as the answer.\n</think>\n\n{"ip_addresses": ["192.168.1.25", "172.17.0.1", "172.18.0.1"]}', part_kind='text')], model_name='qwen3:4b', timestamp=datetime.datetime(2025, 5, 3, 23, 58, 25, tzinfo=datetime.timezone.utc), kind='response')]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Finally, to test the agent with a user prompt to determine both the hostname and maximum memory in the system, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response6 = ai_agent3.run_sync('find the host name as well as the maximum memory available on the system')
print(response6.output)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code would generate the following typical output:</p>
    </div>
    <br/>
    <br/>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>&lt;think&gt;
Okay, the user is asking to find the host name and the maximum memory available on the system. Let me think about how to approach this.

First, the host name. On a Unix-like system, you can use the 'hostname' command to get the host name. So I can execute that in the shell. But wait, the user might be on a different OS, but since the tools available include execute_shell_command, I'll assume it's a Unix-based system unless told otherwise. So the command would be 'hostname'.

Next, the maximum memory. That's a bit trickier. The maximum memory available on the system could refer to the total physical memory. On Linux, you can check this with 'free' command, which shows memory usage. The 'free' command's output includes the total memory. Alternatively, 'dmidecode' can be used to get more detailed system information, but that might require root privileges. Alternatively, 'grep' with 'MemTotal' from 'free' would work. So the command would be 'free | grep "MemTotal" | awk '{print $2}' to extract the total memory in bytes. But maybe the user wants it in a different unit, like GB? But the function's output is a number, so perhaps it's better to return it in bytes as per the command.

Wait, the user might not know the units, so maybe just the numerical value from the system. So executing 'free' and parsing the MemTotal line. Alternatively, using 'cat /proc/meminfo' and then parsing that. But the 'free' command is more user-friendly. So the command would be 'free | grep "MemTotal" | awk '{print $2}'.

So I need to run these two commands. But the user wants both the host name and the maximum memory. So I'll need to execute two separate shell commands. However, the tool allows executing a single command at a time. So maybe first run 'hostname' to get the host name, then run the 'free' command to get the memory.

But the user might expect both pieces of information. So I should call the execute_shell_command function twice: once for hostname, once for the memory. Then combine the results.

Looking back at the previous interaction, the user had a response where the assistant called the execute_shell_command with 'hostname' and got 'panther' as the result. Then the user provided another response with '65763340' which might be the memory value. But in the example, the assistant called the function again with 'free | grep "MemTotal" | awk '{print $2}' and got 65763340 as the output. So that's the memory in bytes.

So the answer would be: Host name is panther, and the maximum memory is 65763340 bytes. But maybe the user wants it in a more readable format, like GB. Let me check: 65763340 bytes is approximately 62.6 MB. But the function's output is just the number, so perhaps that's acceptable.

So the final answer would be the host name and the memory value as obtained from the commands.
&lt;/think&gt;

The host name is **panther**, and the maximum memory available on the system is **65763340 bytes**.</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice that the agent runs the <span class="hi-yellow">execute_shell_command</span> function twice to get both the host name
        and the maximum memory available on the system.</p>
    </div>
    <div id="para-div">
      <p>This concludes the hands-on demonstration on using the <span class="bold">PydanticAI</span> framework !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://ai.pydantic.dev/" target="_blank"><span class="bold">PydanticAI</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
