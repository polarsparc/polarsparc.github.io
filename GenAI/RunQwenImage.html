<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Test :: Using Qwen-Image an Qwen-Image-Edit on a Local Machine">
    <meta name="subject" content="Quick Test :: Using Qwen-Image an Qwen-Image-Edit on a Local Machine">
    <meta name="keywords" content="diffusion, qwen-image, qwen-image-edit">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Test :: Using Qwen-Image an Qwen-Image-Edit on a Local Machine</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Test :: Using Qwen-Image an Qwen-Image-Edit on a Local Machine</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>11/11/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Using Qwen-Image for Image Generation</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the following section, we will demonstrate how one can use the <span class="hi-purple">Qwen-Image</span> text-to-image
        <span class="hi-yellow">Diffusion</span> model on a decent <span class="bold">8-core</span> desktop with <span class="bold">
        64GB</span> system memory and <span class="bold">16GB</span> video memory <span class="bold">NVidia</span> GPU.</p>
    </div>
    <div id="para-div">
      <p>Ensure that <span class="bold">Python 3.1x</span> programming language is installed and setup on the desktop.</p>
    </div>
    <div id="para-div">
      <p>In addition, install the following necessary <span class="bold">Python</span> modules by executing the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ pip install accelerate diffusers huggingface_hub pillow torch</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The first step is to download the <span class="bold">Qwen-Image</span> model from the <span class="bold">HuggingFace</span>
        repository to the default <span class="bold">HuggingFace</span> directory <span class="bold">$HOME/.cache/huggingface</span>
        on the desktop.</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from huggingface_hub import snapshot_download

image_repo_id = 'Qwen/Qwen-Image'

snapshot_download(repo_id=image_repo_id)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The above code execution will take a few minutes to complete as the model needs to be downloaded to the desktop over the
        Internet.</p>
    </div>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>With a 1 Gbps internet speed, the 'snapshot_download' command will take between <span class="underbold">10 to 15</span> minutes to download the model !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Create a directory called <span class="bold">/tmp/images</span> where the model generated image would be stored.</p>
    </div>
    <div id="para-div">
      <p>Execute the following <span class="bold">Python</span> code snippet to run the text-to-image diffusion model:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from diffusers import DiffusionPipeline
import torch

image_dir = '/tmp/images'
num_inference_steps = 16
device = 'cpu'
torch_bfloat16 = torch.bfloat16

c_pipe = DiffusionPipeline.from_pretrained(image_repo_id, torch_dtype=torch_bfloat16)
c_pipe = c_pipe.to(device)
c_pipe.enable_sequential_cpu_offload()

prompt = '''
A digital art of a robot and a tiger walking together, in a forest filled with snow, with mountains in the background
'''

image = c_pipe(
    prompt=prompt,
    height=640,
    width=720,
    output_type='pil',
    num_inference_steps=num_inference_steps,
    generator=torch.Generator(device).manual_seed(7)
).images[0]

image.save('/tmp/images/robot-tiger.jpg')</pre>
      </div>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>It is <span class="underbold">VERY IMPORTANT</span> to use the float type of <span class="underbold">torch.bfloat16</span>. Else will encounter <span class="underbold">RUNTIME ERRORS</span> !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>On the desktop with the specified specs, the model will leverage the CPU memory and typically run for about 5 mins before
        generating the desired image !!!</p>
    </div>
    <div id="para-div">
      <p>The following is the image generated by the <span class="bold">Qwen-Image</span> model for the specific prompt:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/robot-tiger.jpg" alt="Robot and Tiger" />
    </div>
    <br/>
    <div id="para-div">
      <p>Execute the following <span class="bold">Python</span> code snippet to run the text-to-image diffusion model:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>prompt = '''
A panda performing a dance in the center of Rome colosseum, with people clapping and showering flowers in the air. Render the image
in van gogh style
'''

image = c_pipe(
    prompt=prompt,
    height=640,
    width=720,
    output_type='pil',
    num_inference_steps=num_inference_steps,
    generator=torch.Generator(device).manual_seed(7)
).images[0]

image.save('/tmp/images/panda-dance.jpg')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following is the image generated by the <span class="bold">Qwen-Image</span> model for the specific prompt:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/panda-dance.jpg" alt="Panda Dance Rome" />
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">Qwen-Image</span> model is pretty impressive in image generation !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Using Qwen-Image-Edit for Image Manipulation</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Next, we will demonstrate how one can use the <span class="hi-purple">Qwen-Image-Edit</span> image manipulation <span class
        ="hi-yellow">Diffusion</span> model on a decent <span class="bold">8-core</span> desktop with <span class="bold">64GB</span>
        system memory.</p>
      <p>Note that this model needs *<span class="underbold">AT LEAST</span>* <span class="bold">24GB</span> video RAM to effectively
        run on a <span class="bold">NVidia</span> GPU.</p>
    </div>
    <div id="para-div">
      <p>The first step is to download the <span class="bold">Qwen-Image-Edit</span> model from the <span class="bold">HuggingFace</span>
        repository to the default <span class="bold">HuggingFace</span> directory <span class="bold">$HOME/.cache/huggingface</span>
        on the desktop.</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from huggingface_hub import snapshot_download

edit_repo_id = 'Qwen/Qwen-Image-Edit'

snapshot_download(repo_id=edit_repo_id)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The above code execution will take a few minutes to complete as the model needs to be downloaded to the desktop over the
        Internet.</p>
    </div>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>With a 1 Gbps internet speed, the 'snapshot_download' command will take between <span class="underbold">10 to 15</span> minutes to download the model !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>We will use the directory <span class="bold">/tmp/images</span> for the model to look for images to edit and to store
        edited images.</p>
    </div>
    <div id="para-div">
      <p>Execute the following <span class="bold">Python</span> code snippet to run the image manipulation diffusion model:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from diffusers import QwenImageEditPipeline
from PIL import Image
import torch

num_inference_steps = 16
device = 'cpu'
torch_bfloat16 = torch.bfloat16

e_pipe = QwenImageEditPipeline.from_pretrained(edit_repo_id, torch_dtype=torch_bfloat)
e_pipe = e_pipe.to(device=device, torch_dtype=torch_bfloat)
e_pipe.enable_sequential_cpu_offload()

prompt = '''
Remove people from the given image
'''

image_file = '/tmp/images/torii-gate.jpg'
image = Image.open(img_dir + image_file).convert('RGB')

inputs = {
  'image': image,
  'prompt': prompt,
  'generator': torch.manual_seed(7),
  'num_inference_steps': num_inference_steps,
}

with torch.inference_mode():
    output = e_pipe(**inputs)
    output_image = output.images[0]
    output_image.save('/tmp/images/' + image_file[:-4] + '_1.jpg')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The image manipulation task is to remove people from the given image and on the desktop with the specified specs, the model
        will leverage the CPU memory and typically run for about 6 to 8 mins before generating a modified image !!!</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the before and after images:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/torii-gate.png" alt="Torii Gate" />
    </div>
    <br/>
    <div id="para-div">
      <p>Let us try one more image manipulation task of generating a cartoon version of the image. Execute the following <span class
        ="bold">Python</span> code snippet to run the image manipulation diffusion model:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>prompt = '''
Convert the given image to a tintin herge style cartoon
'''

image_file = '/tmp/images/torii-gate_1.jpg'
image = Image.open(img_dir + image_file).convert('RGB')

inputs = {
  'image': image,
  'prompt': prompt,
  'generator': torch.manual_seed(7),
  'num_inference_steps': num_inference_steps,
}

with torch.inference_mode():
    output = e_pipe(**inputs)
    output_image = output.images[0]
    output_image.save('/tmp/images/' + image_file[:-4] + '_2.jpg')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The image manipulation task is to cartoonize the given image and on the desktop with the specified specs, the model will
        leverage the CPU memory and typically run for about 6 to 8 mins before generating the desired version of the image !!!</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the before and after images:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/torii-gate-2.png" alt="Torii Gate Cartoon" />
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">Qwen-Image-Edit</span> model is quite impressive with image manipulation !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://huggingface.co/Qwen/Qwen-Image" target="_blank"><span class="bold">HuggingFace Qwen-Image</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
