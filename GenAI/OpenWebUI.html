<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on Open WebUI with Ollama">
    <meta name="subject" content="Quick Primer on Open WebUI with Ollama">
    <meta name="keywords" content="ollama, open-webui, llm">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on Open WebUI with Ollama</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on Open WebUI with Ollama</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>05/31/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr"/>
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><a href="https://openwebui.com/" target="_blank"><span class="hi-yellow">Open WebUI</span></a> is a popular, open source,
        user friendly, and web-based UI platform that allows one to locally host and interact with various LLM models using <span
        class="bold">OpenAI</span> compatible LLM runners, such as <span class="bold">Ollama</span>.</p>
      <p>In this primer, we will demonstrate how one can effectively setup and run the <span class="bold">Open WebUI</span> along
        with the <span class="bold">Ollama</span> platform using pre-built <span class="bold">Docker</span> images.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Installation and Setup</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will can on a <span class="bold">Ubuntu 24.04 LTS</span> based Linux desktop AND a <span class
        ="bold">Apple Silicon</span> based Macbook Pro. Ensure that <span class="bold">Docker</span> is installed and setup on the
        desktop (see <a href="http://polarsparc.github.io/Docker/Docker.html" target="_blank"> instructions</a>).</p>
      <p>For Linux and MacOS, ensure that <span class="bold">Ollama</span> platform is installed and setup on the desktop (see <a
        href="http://polarsparc.github.io/GenAI/Ollama.html" target="_blank"><span class="bold">instructions</span></a>).</p>
    </div>
    <div id="para-div">
      <p>For Linux and MacOS, we will setup two required directories by executing the following command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ mkdir -p $HOME/.ollama/open-webui</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For Linux and MacOS, to pull and download the docker image for <span class="bold">open-webui</span>, execute the following
        command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker pull ghcr.io/open-webui/open-webui:0.6.13</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>0.6.13: Pulling from open-webui/open-webui
61320b01ae5e: Pull complete 
fa70febde0f6: Pull complete 
9d545c45fb8c: Pull complete 
09c4893e5320: Pull complete 
331425d04a0a: Pull complete 
4f4fb700ef54: Pull complete 
e59efb0a4f10: Pull complete 
cf1f8292132e: Pull complete 
5fe20a65e325: Pull complete 
dc36d13e3503: Pull complete 
83e9df231c27: Pull complete 
3c537f9569f7: Pull complete 
f1b7f5f04ad1: Pull complete 
cc1f0481efaa: Pull complete 
7f3cf9d660ec: Pull complete 
Digest: sha256:ddc64d14ec933e8c1caf017df8a0068bece3e35acbc59e4aa4971e5aa4176a72
Status: Downloaded newer image for ghcr.io/open-webui/open-webui:0.6.13
ghcr.io/open-webui/open-webui:0.6.13</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>This completes all the system installation and setup for the <span class="bold">Open WebUI</span> hands-on demonstration.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on with Open WebUI</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the following sections, we will show the commands for both Linux and MacOS, however, we will <span class="underbold">
        ONLY</span> show the output from Linux. Note that all the commands have been tested on both Linux and MacOS respectively.</p>
    </div>
    <div id="para-div">
      <p>Assuming that the ip address on the Linux desktop is <span class="hi-grey">192.168.1.25</span>, start the <span class="bold">
        Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama --network=host -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For MacOS, start the <span class="bold">Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama --network=host -p 11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>time=2025-05-31T00:42:59.447Z level=INFO source=routes.go:1234 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-05-31T00:42:59.450Z level=INFO source=images.go:479 msg="total blobs: 23"
time=2025-05-31T00:42:59.451Z level=INFO source=images.go:486 msg="total unused blobs removed: 0"
time=2025-05-31T00:42:59.451Z level=INFO source=routes.go:1287 msg="Listening on [::]:11434 (version 0.9.0)"
time=2025-05-31T00:42:59.451Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-05-31T00:42:59.453Z level=INFO source=gpu.go:377 msg="no compatible GPUs were discovered"
time=2025-05-31T00:42:59.453Z level=INFO source=types.go:130 msg="inference compute" id=0 library=cpu variant="" compute="" driver=0.0 name="" total="62.7 GiB" available="57.6 GiB"</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For the hands-on demonstration, we will use the <span class="hi-purple">Microsoft Phi-4 Mini</span> pre-trained LLM model.</p>
    </div>
    <div id="para-div">
      <p>For Linux and MacOS, open a new terminal window and execute the following <span class="bold">docker</span> command to download
        the desired LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run phi4-mini</p>
    </div>
    <br/>
    <div id="para-div">
      <p>After the pre-trained LLM model is downloaded successfully, the command would wait for an user input.</p>
      <p>To test the just downloaded LLM model, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; describe a gpu in less than 50 words in json format</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output on Linux:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>```json
{
  "gpu": {
    "description": "A Graphics Processing Unit optimized for rendering images and videos, accelerating computational tasks."
  }
}
```</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Now we will shift gears to get our hands dirty with <span class="hi-yellow">Open WebUI</span>.</p>
    </div>
    <div id="para-div">
      <p>On Linux, to start the <span class="bold">Open WebUI</span> platform, execute the following command in a new terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name open-webui --network=host --add-host=host.docker.internal:host-gateway -p 192.168.1.25:8080:8080 -v $HOME/.ollama/open-webui:/app/backend/data -e WEBUI_AUTH=False -e OLLAMA_API_BASE_URL=http://192.168.1.25:11434/api ghcr.io/open-webui/open-webui:0.6.13</p>
    </div>
    <br/>
    <div id="para-div">
      <p>On MacOS, to start the <span class="bold">Open WebUI</span> platform, execute the following command in a new terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name open-webui --network=host --add-host=host.docker.internal:host-gateway -p 8080:8080 -v $HOME/.ollama/open-webui:/app/backend/data -e WEBUI_AUTH=False -e OLLAMA_API_BASE_URL=http://127.0.0.1:11434/api ghcr.io/open-webui/open-webui:0.6.13</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical trimmed output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>-Loading WEBUI_SECRET_KEY from file, not provided as an environment variable.
Generating WEBUI_SECRET_KEY
Loading WEBUI_SECRET_KEY from .webui_secret_key
...[ TRIM ]...
WARNING: CORS_ALLOW_ORIGIN IS SET TO '*' - NOT RECOMMENDED FOR PRODUCTION DEPLOYMENTS.
INFO  [open_webui.env] Embedding model set: sentence-transformers/all-MiniLM-L6-v2
WARNI [langchain_community.utils.user_agent] USER_AGENT environment variable not set, consider setting it to identify your requests.
...[ TRIM ]...
INFO:     Started server process [1]
INFO:     Waiting for application startup.
2025-05-31 22:04:38.110 | INFO     | open_webui.utils.logger:start_logger:140 - GLOBAL_LOG_LEVEL: INFO - {}
2025-05-31 22:04:38.111 | INFO     | open_webui.main:lifespan:489 - Installing external dependencies of functions and tools... - {}
2025-05-31 22:04:38.118 | INFO     | open_webui.utils.plugin:install_frontmatter_requirements:241 - No requirements found in frontmatter. - {}</pre>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>On Linux and MacOS, the docker command options --network=host AND --add-host=host.docker.internal:host-gateway are very <span class="underbold">IMPORTANT</span> as it enables a container to connect to services on the host</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>On Linux, open the web browser and enter the URL link <span class="bold">http://192.168.1.25:8080</span> and on MacOS, open
        the web browser and enter the URL link <span class="bold">http://127.0.0.1:8080</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the browser presented to the user on launch on Linux:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-01.png" alt="Start Screen" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Click on the <span class="bold">Okay, Let's Go!</span> button to navigate to the main screen as shown in the following
        illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-02.png" alt="Main Screen" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Click on the top left <span class="bold">qwen3:4b</span> drop-down to select the <span class="bold">phi4-mini:latest</span>
        model as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-03.png" alt="Model Selection" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Once the <span class="bold">phi4-mini:latest</span> LLM model is selected, one is presented with a screen as shown in the
        following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-04.png" alt="Phi-4 Model" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Next step is to enter a user prompt in the prompt textbox and click on the <span class="bold">Up Arrow</span> as shown in
        the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-05.png" alt="Enter Prompt" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The LLM model will respond with a text corresponding to the user prompt as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-06.png" alt="LLM Response" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>For the next task, we will use the <a href="https://polarsparc.github.io/data/NVIDIA-3Q-2024.pdf" target="_blank"><span class
        ="bold">Nvidia 3rd Quarter 2024</span></a> financial report to analyze it !!!</p>
    </div>
    <div id="para-div">
      <p>First we need to upload the financial report PDF by first clicking on the <span class="bold">+</span> and then the <span
        class="bold">Upload Files</span> option as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-07.png" alt="Upload Document" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Once the document has been uploaded, enter a user prompt in the prompt textbox and then click on the <span class="bold">Up
        Arrow</span> as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-08.png" alt="Analyze Prompt" />
      <div class="img-cap">Figure.8</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The LLM model will respond with the following response as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-09.png" alt="Summary Response" />
      <div class="img-cap">Figure.9</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, for the next task, we will use this image of <a href="https://polarsparc.github.io/GenAI/images/test-image.png" target=
        "_blank"><span class="bold">Leadership Books</span></a> to analyze and describe it !!!</p>
    </div>
    <div id="para-div">
      <p>Click on the top left <span class="bold">phi4-mini:latest</span> drop-down to select the <span class="bold">gemma3:4b</span>
        model as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-10.png" alt="Model Selection" />
      <div class="img-cap">Figure.10</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Once the <span class="bold">gemma3:4b</span> LLM model is selected, upload the desired image, enter a user prompt in the
        prompt textbox and then click on the <span class="bold">Up Arrow</span> as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-11.png" alt="Gemma3 Image Processing" />
      <div class="img-cap">Figure.11</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The LLM model will respond with the following response as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-12.png" alt="Image Analysis Response" />
      <div class="img-cap">Figure.12</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Moving along to the next task, one again click on the top left <span class="bold">gemma3:4b</span> drop-down to select the
        <span class="bold">qwen3:4b</span> model as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-13.png" alt="Model Selection" />
      <div class="img-cap">Figure.13</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Once the <span class="bold">qwen3:4b</span> LLM model is selected, enter a user prompt in the prompt textbox and then click
        on the <span class="bold">Up Arrow</span> as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-14.png" alt="Qwen3 Code Assist" />
      <div class="img-cap">Figure.14</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The LLM model will respond with the following response as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/open-webui-15.png" alt="Generated Code" />
      <div class="img-cap">Figure.15</div>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="bold">BINGO</span> - we have successfully tested <span class="bold">Open WebUI</span> interfacing with <span
        class="bold">Ollama</span> !</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://github.com/open-webui/open-webui" target="_blank"><span class="bold">Open WebUI</span></a></p>
      <p><a href="https://ollama.com/" target="_blank"><span class="bold">Ollama</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
