<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on Running GGUF models on Ollama">
    <meta name="subject" content="Quick Primer on Running GGUF models on Ollama">
    <meta name="keywords" content="gguf, ollama">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on Running GGUF models on Ollama</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on Running GGUF models on Ollama</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">01/04/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="para-div">
      <p><span class="hi-yellow">GPT-Generated Unified Format</span> (or <span class="hi-grey">GGUF</span> for short) is a binary
        file format for efficient storage, distribution, and deployment of LLM models.</p>
    </div>
    <div id="para-div">
      <p>In the article on <a href="https://polarsparc.github.io/GenAI/Ollama.html" target="_blank"><span class="bold">Ollama</span>
        </a>, we demonstrated how one can deploy LLM model(s) on a local desktop.</p>
      <p>In this article, we will demonstrate how one can deploy and use any LLM model in the <span class="bold">GGUF</span> format
        using <span class="bold">Ollama</span>.</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will be on a <span class="bold">Ubuntu 24.04 LTS</span> based Linux desktop. Ensure that <span
        class="bold">Docker</span> is installed and setup on the desktop (see <a href="http://polarsparc.github.io/Docker/Docker.html"
        target="_blank"><span class="bold">instructions</span></a>).</p>
      <p>Also, ensure the command-line utility <span class="bold">curl</span> is installed on the Linux desktop.</p>
    </div>
    <div id="para-div">
      <p>The following are the steps one can follow to download, deploy, and use a <span class="bold">GGUF</span> model in <span
        class="bold">Ollama</span>:</p>
    </div>
    <div id="para-div">
      <p>Open two <span class="bold">Terminal</span> windows (referred to as <span class="bold">term-1</span> and <span class="bold">
        term-2</span>).</p>
      <ul id="blue-arw-ul">
        <li>
          <p>Create a directory for downloading and storing the LLM model <span class="bold">GGUF</span> file by executing the
            following command in <span class="bold">term-1</span>:</p>
          <br/>
          <div id="cmd-div">
            <p>$ mkdir -p $HOME/.ollama/GGUF</p>
          </div>
          <br/>
        </li>
        <li>
          <p>For this demonstration, we will deploy and test the open source 8-bit quantized <span class="hi-purple">DeepSeek-v3</span>
            model that has caused some abuzz recently and challenging the popular proprietary AI models.</p>
          <p>We will download the 8-bit quantized <span class="hi-purple">DeepSeek-v3</span> model in the <span class="bold">GGUF</span>
            format from <span class="bold">HuggingFace</span> by executing the following commands in <span class="bold">term-1</span>:</p>
          <br/>
          <div id="cmd-div">
            <p>$ cd $HOME/.ollama/GGUF</p>
            <p>$ curl -L -O https://huggingface.co/LoupGarou/deepseek-coder-6.7b-instruct-pythagora-v3-gguf/resolve/main/deepseek-coder-6.7b-instruct-pythagora-v3-Q8_0.gguf</p>
          </div>
          <br/>
          <br/>
          <div id="warn-div">
            <h4>!!! ATTENTION !!!</h4>
            <pre>With a 1 Gbps internet speed, the 'download' will take between <span class="underbold">3 to 5</span> minutes !!!</pre>
          </div>
          <br/>
        </li>
        <li>
          <p>Create a model file for <span class="bold">Ollama</span> for the just downloaded <span class="bold">GGUF</span> file by
            executing the following command in <span class="bold">term-1</span>:</p>
          <br/>
          <div id="cmd-div">
            <p>$ cd $HOME/.ollama</p>
            <p>$ echo 'from /root/.ollama/GGUF/deepseek-coder-6.7b-instruct-pythagora-v3-Q8_0.gguf' > deepseek-v3-Q8_0_gguf.txt</p>
          </div>
          <br/>
        </li>
        <li>
          <p>Start the <span class="bold">Ollama</span> platform by executing the following <span class="bold">docker</span> command
            in <span class="bold">term-1</span>:</p>
          <br/>
          <div id="cmd-div">
            <p>$ cd $HOME</p>
            <p>$ docker run --rm --name ollama --gpus=all --network="host" -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.5.4</p>
          </div>
          <br/>
        </li>
        <li>
          <p>To list all the LLM models that are deployed in the <span class="bold">Ollama</span> platform, execute the following
            <span class="bold">docker</span> command in <span class="bold">term-2</span>:</p>
          <br/>
          <div id="cmd-div">
            <p>$ cd $HOME</p>
            <p>$ docker exec -it ollama ollama list</p>
          </div>
          <br/>
          <p>The following would be the typical output:</p>
          <br/>
          <div id="out-div">
            <h4>Output.1</h4>
            <pre>NAME                 ID              SIZE      MODIFIED</pre>
          </div>
          <br/>
        </li>
        <li>
          <p>To deploy the just downloaded <span class="bold">GGUF</span> model into the <span class="bold">Ollama</span> platform,
            execute the following <span class="bold">docker</span> command in <span class="bold">term-2</span>:</p>
          <br/>
          <div id="cmd-div">
            <p>$ cd $HOME</p>
            <p>$ docker exec -it ollama ollama create deepseek-v3-Q8_0 -f /root/.ollama/deepseek-v3-Q8_0_gguf.txt</p>
          </div>
          <br/>
          <p>The above command would take about a <span class="underbold">minute</span> to execute and generate the following typical
            output on completion:</p>
          <br/>
          <div id="out-div">
            <h4>Output.2</h4>
            <pre>transferring model data 100% 
using existing layer sha256:636545fc45204417c1c38ce42126b807f126d80dddc912e07c3a8d90ecdfcd00 
using autodetected template alpaca 
using existing layer sha256:afa0ae3294fbad4c6b60d110ae6e034b3dfdd5e0acf4d2f3eaa0b888633f7ffe 
creating new layer sha256:6e6eb6f365d1c295f24b2bf7e7db63a37d5da88dda6a453a84f0c140476a377b 
writing manifest 
success</pre>
          </div>
          <br/>
        </li>
        <li>
          <p>To verify the 8-bit quantized <span class="hi-purple">DeepSeek-v3</span> model was deployed successfully, execute the
            following <span class="bold">docker</span> command in <span class="bold">term-2</span>:</p>
          <br/>
          <div id="cmd-div">
            <p>$ cd $HOME</p>
            <p>$ docker exec -it ollama ollama list</p>
          </div>
          <br/>
          <p>The following was the output from my desktop:</p>
          <br/>
          <div id="out-div">
            <h4>Output.3</h4>
            <pre>NAME                       ID              SIZE      MODIFIED           
deepseek-v3-Q8_0:latest    5af2e8c42525    7.2 GB    About a minute ago</pre>
          </div>
          <br/>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>To test the 8-bit quantized <span class="hi-purple">DeepSeek-v3</span> model, execute the following <span class="bold">
        docker</span> command in <span class="bold">term-2</span>:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run deepseek-v3-Q8_0:latest</p>
    </div>
    <br/>
    <div id="para-div">
      <p>After the LLM model is loaded (which will take a few seconds the first time), the command would wait for an user input at
        the prompt <span class="hi-vanila">&gt;&gt;&gt;Send a message (/? for help)</span>.</p>
    </div>
    <div id="para-div">
      <p>To test the just loaded LLM model, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; assuming ollama llm chat, generate python code using langchain to chat with the llm with message history</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Using <span class="hi-purple">DeepSeek-v3</span>, the following would be the typical response:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>```python
from langchain.chat_models import OLLAMA_LLM
from langchain.schema import (
    AIMessage, 
    HumanMessage, 
    SystemMessage
)

class LLMWithHistory:
    
    def __init__(self):
        self.llm = OLLAMA_LLM()
        self.messages = []
        
    def add_message(self, message):
        if isinstance(message, str):
            message = HumanMessage(content=message)
        self.messages.append(message)
    
    def get_response(self):
        ai_message = AIMessage(content=str(self.llm([msg.content for msg in self.messages], return_generated=True)))
        self.add_message(ai_message)
        return ai_message
        
chatbot = LLMWithHistory()
chatbot.add_message("Hello, how are you?")
print(chatbot.get_response().content) # "Hello!"
```</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Interesting - there is no class called <span class="hi-red">OLLAMA_LLM</span> !!!</p>
    </div>
    <div id="para-div">
      <p>Using <span class="hi-green">ChatGPT</span>, the following was the generated response:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>from langchain.chat_models import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage

# Initialize the chat model
# Replace 'YOUR_API_KEY' with your actual API key or authentication method
llm = ChatOpenAI(model="ollama-llm", temperature=0.7)

# Initialize the message history
message_history = [
    SystemMessage(content="You are a helpful assistant."),
]

def chat_with_llm(user_input):
    global message_history

    # Add the user's message to the history
    message_history.append(HumanMessage(content=user_input))

    # Generate a response from the LLM
    response = llm(message_history)

    # Add the AI's response to the history
    message_history.append(AIMessage(content=response.content))

    # Return the AI's response
    return response.content

# Example usage
if __name__ == "__main__":
    print("Chat with the LLM. Type 'exit' to end the conversation.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            print("Ending the chat. Goodbye!")
            break

        ai_response = chat_with_llm(user_input)
        print(f"AI: {ai_response}")</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Interesting - the initialization of the class <span class="hi-orange">ChatOpenAI</span> is not correct !!!</p>
    </div>
    <div id="para-div">
      <p>On to the next test to solve a <span class="bold">Calculus Derivative</span> problem using the following prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; find the derivative of y = (x + 1) * sqrt(x)</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Using <span class="hi-purple">DeepSeek-v3</span>, the following would be the typical response:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/gguf-ollama-1.png" alt="DeepSeek-v3" />
      <div class="img-cap">DeepSeek-v3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Hmm - the answer is <span class="hi-red">WRONG</span> !!!</p>
    </div>
    <div id="para-div">
      <p>Using <span class="hi-green">ChatGPT</span>, the following was the generated response:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/gguf-ollama-2.png" alt="ChatGPT" />
      <div class="img-cap">ChatGPT</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Good - the answer is <span class="hi-green">CORRECT</span> !!!</p>
    </div>
    <div id="para-div">
      <p>To exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>With this, we conclude this article on downloading, deploying, and using LLM models in the <span class="bold">GGUF</span>
        format !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md" target="_blank"><span class="bold">GGUF Format</span></a></p>
      <p><a href="https://huggingface.co/models?search=gguf" target="_blank"><span class="bold">GGUF Models on HuggingFace</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
