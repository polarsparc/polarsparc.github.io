<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on LangChain">
    <meta name="subject" content="Quick Primer on LangChain">
    <meta name="keywords" content="ollama, langchain, llm, rag, vector_store, agentic">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on LangChain</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on LangChain</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>06/15/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr"/>
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><a href="https://www.langchain.com/" target="_blank"><span class="hi-yellow">LangChain</span></a> is a popular, open-source,
        orchestration framework that provides a standardized abstraction for developing AI applications by hiding the complexities of
        working with the different <span class="bold">Large Language</span> models (or <span class="bold">LLM</span>s for short).</p>
      <p><span class="bold">LangChain</span> provides a collection of core component abstractions (or modular building blocks) that
        could be linked or "<span class="bold">Chain</span>ed" together to create complex AI workflows (or applications) to solve a
        plethora of tasks.</p>
      <p>The following are the list of some of the important component modules from <span class="bold">LangChain</span>:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span class="hi-grey">Models</span> :: are standardized abstractions for interacting with AI models and are primarily of
            three types:</p>
          <ul id="blue-disc-ul">
            <li><p><span class="hi-vanila">LLM Models</span> are designed for interacting with the various LLMs, which take text input
              and generate text output</p></li>
            <li><p><span class="hi-vanila">Chat Models</span> are designed for interacting with the various LLMs using a chat style
              interaction in which the previous chat input is used as the context</p></li>
            <li><p><span class="hi-vanila">Embedding Models</span> are designed to convert an input text to a numerical embedding
              vector</p></li>
          </ul>
        </li>
        <li>
          <p><span class="hi-grey">Prompts</span> :: are a set of instructions (or contextual inputs) provided by a user to guide the
            model to generate relevant response (or output). There are three types of standardized prompt abstractions:</p>
          <ul id="blue-disc-ul">
            <li><p><span class="hi-vanila">Prompt Templates</span> are reusable string templates with placeholders for contexts and
              instructions (or inputs), which can be dynamically set by a user at runtime, to generate specific prompt instructions
              for interacting with various LLMs</p></li>
            <li><p><span class="hi-vanila">Example Selectors</span> are a way to pass specific and relevant examples along with the
              input prompt to the LLMs in order to control the response (or output) from the LLMs</p></li>
            <li><p><span class="hi-vanila">Output Parsers</span> are responsible for taking the text response (or output) from the
              LLMs and transforming them to a specific structured format</p></li>
          </ul>
        </li>
        <li>
          <p><span class="hi-grey">Retrieval</span> :: are related to how one can pass large amounts of user specific contextual data
            along with the instruction prompt to the LLMs. There are four types of standardized retrieval abstractions:</p>
          <ul id="blue-disc-ul">
            <li><p><span class="hi-vanila">Document Loaders</span> are useful for loading data documents from different sources, such
              as files (.txt, .pdf, .mp4, etc), from the web, or any other available sources.</p></li>
            <li><p><span class="hi-vanila">Text Splitters</span> are useful for splitting long documents into smaller semantically
              meaningful chunks so that they can be within the limits of the LLMs context window size</p></li>
            <li><p><span class="hi-vanila">Vector Stores</span> are used for storing vector embeddings of user specific documents and
              for later retrieval of similar documents</p></li>
            <li><p><span class="hi-vanila">Retrievers</span> are generic interfaces which are useful for querying embedded documents
              from various vectors sources including <span class="bold">Vector Stores</span></p></li>
          </ul>
        </li>
        <li>
          <p><span class="hi-grey">Memory</span> :: are used for preserving user prompts and responses from the LLMs so that they can
            be used for providing the context for future interactions with the LLMs</p>
          </ul>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>The intent of this article is <span class="underbold">NOT</span> to be exhaustive, but a primer to get started quickly.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Installation and Setup</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will be on a <span class="bold">Ubuntu 24.04 LTS</span> based Linux desktop. Ensure that <span
        class="bold">Ollama</span> is installed and setup on the desktop (see <a href="http://polarsparc.github.io/GenAI/Ollama.html"
        target="_blank"><span class="bold">instructions</span></a>).</p>
      <p>In addition, ensure that the <span class="bold">Python 3.1x</span> programming language as well as the <span class="bold">
        Jupyter Notebook</span> package is installed and setup on the desktop.</p>
    </div>
    <div id="para-div">
      <p>Assuming that the ip address on the Linux desktop is <span class="hi-grey">192.168.1.25</span>, start the <span class="bold">
        Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>If the linux desktop has <span class="hi-green">Nvidia GPU</span> with decent amount of VRAM (at least 16 GB) and has been
        enabled for use with <span class="bold">docker</span> (see <a href="https://polarsparc.github.io/Docker/DockerNVidia.html"
        target="_blank"><span class="bold">instructions</span></a>), then execute the following command instead to start <span class
        ="bold">Ollama</span>:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama --gpus=all -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For the LLM model, we will be using the recently released <span class="hi-purple">Microsoft Phi-4 Mini</span> model.</p>
    </div>
    <div id="para-div">
      <p>Open a new terminal window and execute the following <span class="bold">docker</span> command to download the LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run phi4-mini</p>
    </div>
    <br/>
    <div id="para-div">
      <p>To install the necessary <span class="bold">Python</span> modules for this primer, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ pip install chromadb dotenv langchain lanchain-core langchain-ollama lanchain-chroma langchain-community</p>
    </div>
    <br/>
    <div id="para-div">
      <p>This completes all the installation and setup for the <span class="bold">LangChain</span> hands-on demonstrations.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on with LangChain</p>
    </div>
    <div id="para-div">
      <p>Create a file called <span class="hi-yellow">.env</span> with the following environment variables defined:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>LLM_TEMPERATURE=0.0
OLLAMA_MODEL='phi4-mini:latest'
OLLAMA_BASE_URL='http://192.168.1.25:11434'
CHROMA_DB_DIR='/home/polarsparc/.chromadb'
BOOKS_DATASET='./leadership_books.csv'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To load the environment variables and assign them to <span class="bold">Python</span> variable, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from dotenv import load_dotenv, find_dotenv

import os

load_dotenv(find_dotenv())

llm_temperature = float(os.getenv('LLM_TEMPERATURE'))
ollama_model = os.getenv('OLLAMA_MODEL')
ollama_base_url = os.getenv('OLLAMA_BASE_URL')
chroma_db_dir = os.getenv('CHROMA_DB_DIR')
books_dataset = os.getenv('BOOKS_DATASET')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize a client instance of <span class="bold">Ollama</span> running the desired LLM model <span class="hi-purple">
        phi4-mini:latest</span>, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_ollama import OllamaLLM

ollama_llm = OllamaLLM(base_url=ollama_base_url, model=ollama_model, temperature=llm_temperature)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The <span class="bold">temperature</span> parameter in the above code is a value that is between 0.0 and 1.0. It determines
        whether the output from the LLM model should be more "creative" or be more "predictive". A higher value means more "creative"
        and a lower value means more "predictive".</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-yellow">PromptTemplate</span> class allows one to create an instance of an LLM prompt using a string
        template with placeholders (word surrounded by curly braces). To create an instance of a prompt from a string template,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.prompts import PromptTemplate

template = """
Question: {question}

Answer: Summarize in less than {tokens} words.
"""

prompt = PromptTemplate.from_template(template=template)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To test the prompt instance with some test values for the placeholders, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>prompt.format(question='describe a leader', tokens=50)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>'\nQuestion: describe a leader\n\nAnswer: Summarize in less than 50 words.\n'</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To create our first simple <span class="bold">chain</span> (using the pipe '|' operator) for sending the prompt to our LLM
        model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>chain = prompt | ollama_llm</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To run the just created simple chain by providing actual values for the placeholders in the prompt, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>print(chain.invoke({'question': 'describe a leader', 'tokens': 50}))</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>A good leader is confident, empathetic and inspires others to achieve common goals. They communicate effectively while being adaptable.</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>In the above simple chain, the LLM model generated a unstructured text response (or output). There are situations when one
        would require a more structured response, like a list, or json, etc. This is where the <span class="bold">Output Parsers</span>
        come in handy. <span class="bold">LangChain</span> provides a collection of pre-built output parsers as follows:</p>
      <ul id="blue-disc-ul">
        <li><p><span class="hi-vanila">CommaSeparatedListOutputParser</span></p></li>
        <li><p><span class="hi-vanila">JsonOutputParser</span></p></li>
        <li><p><span class="hi-vanila">PandasDataFrameOutputParser</span></p></li>
        <li><p><span class="hi-vanila">XMLOutputParser</span></p></li>
        <li><p><span class="hi-vanila">YamlOutputParser</span></p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>For this demonstration, we will leverage the <span class="bold">CommaSeparatedListOutputParser</span> output parser. First,
        we will need to change the prompt a little bit. To create a new instance of a prompt from a string template that instructs
        the LLM to generate a list of items, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.prompts import PromptTemplate

template2 = """
Generate a list of top five qualities about {subject}, with each feature using less than {tokens} words.

{format_instructions}
"""

prompt2 = PromptTemplate.from_template(template=template2)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create a simple <span class="bold">chain</span> that sends the new prompt to our LLM model, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>chain2 = prompt2 | ollama_llm</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To run the just created simple chain by providing actual values for the placeholders in the prompt, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>output = chain2.invoke({'subject': 'a leader', 'format_instructions': format_instructions, 'tokens': 5})
print(output)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>Visionary, Inspiring, Decisive, Empathetic, Resilient.</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of the output parser, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create a new <span class="bold">chain</span> that sends the response from our LLM model to the output parser, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>chain3 = prompt2 | ollama_llm | output_parser</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To run the just created chain by providing actual values for the placeholders in the prompt, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>output2 = chain3.invoke({'subject': 'a leader', 'format_instructions': format_instructions, 'tokens': 5})
print(output2)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>['Visionary', 'Inspiring', 'Decisive', 'Empathetic', 'Resilient.']</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Even though the LLM model(s) have been trained on vast amounts of data, one needs to provide an appropriate context to the
        LLM model(s) for better response (or output). To get better results, one can augment the LLM model's knowledge by providing
        additional context from external data sources, such as vector stores. This is where the <span class="hi-yellow">Retrieval
        Augmented Generation</span> (or <span class="hi-vanila">RAG</span> for short) comes into play.</p>
    </div>
    <div id="para-div">
      <p>To demonstrate the power of <span class="bold">RAG</span>, we will create a new instance of a prompt from a string template
        that instructs the LLM to generate a list of books on a given subject by executing the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.prompts import PromptTemplate

prompt3 = PromptTemplate.from_template(template='Generate a numbered list of top three books on {subject}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create a simple <span class="bold">chain</span> that sends the new prompt to our LLM model, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>chain4 = prompt3 | ollama_llm</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To run the just created simple chain by providing actual values for the placeholders in the prompt, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>output3 = chain4.invoke({'subject': 'leadership'})
print(output3)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>1. "Leadership: Theory and Practice" by Peter G. Northouse

2. "The 21 Irrefutable Laws of Leadership: Follow Them and You Too Will Become a Leader" by John C. Maxwell

3. "Leaders Eat Last: Why Some Teams Pull Together And Others Don't" by Simon Sinek</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>We will leverage the <span class="hi-yellow">Chroma</span> as the vector store for the <span class="bold">RAG</span> demo.
        In addition, we will handcraft as small dataset containing information on some popular leadership books.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the truncated contents of the small leadership books dataset:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/langchain-1.png" alt="Books Dataset" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The pipe-separated leadership books dataset can be downloaded from <a href="https://polarsparc.github.io/data/leadership_books.csv"
        target="_blank"><span class="bold">HERE</span></a> !!!</p>
    </div>
    <div id="para-div">
      <p>To load the pipe-separated leadership books dataset into a <span class="bold">pandas</span> dataframe, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>import pandas as pd

books_df = pd.read_csv(books_dataset, sep='|')
books_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the typical output as shown in Figure.1 above.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-yellow">Document</span> class allows one to encapsulate the textual content with metadata, which can
        be stored in a vector store for later searches.</p>
    </div>
    <div id="para-div">
      <p>To create a collection of documents from the rows of the <span class="bold">pandas</span> dataframe, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.documents import Document

documents = []
for i in range(0, len(books_df)):
    doc = Document(page_content=books_df.iloc[i]['Summary'], metadata={'Title': books_df.iloc[i]['Title'], 'Author': books_df.iloc[i]['Author']})
    documents.append(doc)
documents</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration depicts the partial collection of documents:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/langchain-2.png" alt="Documents Collection" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>To store the documents (with textual content) into a vector store (as embeddings), we need to create an instance of vector
        embedding class corresponding to the LLM model. To create an instance of <span class="bold">Ollama</span> embedding, execute
        the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_ollama import OllamaEmbeddings

ollama_embedding = OllamaEmbeddings(base_url=ollama_base_url, model=ollama_model)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create an instance of the <span class="bold">Chroma</span> vector store with <span class="bold">Ollama</span> embedding,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_chroma import Chroma

vector_store = Chroma(embedding_function=ollama_embedding, persist_directory=chroma_db_dir)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To persist the collection of documents into the <span class="bold">Chroma</span> vector store, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>vector_store.add_documents(documents)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration depicts the list of document IDs stored in the vector store:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/langchain-3.png" alt="Documents in VectorStore" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create an instance of a vector store retriever that will return at most <span class="bold">2</span> similar documents,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>n_top = 2

retriever = vector_store.as_retriever(search_kwargs={'k': n_top})</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To perform a query on similar documents from the <span class="bold">Chroma</span> vector store, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>query = 'a leaders guide to managing teams'

similar_docs = retriever.invoke(query)
similar_docs</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration depicts the list of document IDs stored in the vector store:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/langchain-4.png" alt="Documents Query Result" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>To create a new prompt from a string template, with a placeholder for the contextual info from the vector store to instruct
        the LLM to generate a list of books by executing the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.prompts import PromptTemplate

template3 = """
Generate a numbered list of top three books on {subject} that are similar to the following:

{document}
"""

prompt4 = PromptTemplate.from_template(template=template3)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create a simple <span class="bold">chain</span> that sends the new prompt to our LLM model, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>chain5 = prompt4 | ollama_llm</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To run the just created simple chain by providing the contextual info from the vector store into the prompt, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>output4 = chain5.invoke({'subject': 'leadership', 'document': similar_docs[0]})
print(output4)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>1. "Leaders Eat Last: Why Some Teams Pull Together And Others Don't" by Simon Sinek
  - This book delves into why leaders should prioritize creating an environment where team members feel safe and valued, drawing on real-life examples to illustrate how this approach can lead to better teamwork.

2. "Drive: The Surprising Truth About What Motivates Us" by Daniel H. Pink
  - In this insightful work, author Daniel H. Pink explores the science behind motivation at both personal levels (individuals) and organizational ones (teams), offering practical advice for leaders on how best to inspire their teams.

3. "The Five Dysfunctions of a Team: A Leadership Fable" by Patrick Lencioni
  - This book offers an engaging narrative that follows Kathryn Petersen, CEO DecisionTech's team through various challenges as they strive towards unity and success in the face of adversity.</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The reponse in Output.6 above is much more inline with what one would expect given the additional context.</p>
    </div>
    <div id="para-div">
      <p>Moving on to the example on memory, to initialize an instance of the chat model for <span class="bold">Ollama</span> running
        the LLM model <span class="hi-purple">phi4-mini:latest</span>, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_ollama import ChatOllama

ollama_chat_llm = ChatOllama(model=ollama_model, temperature=0.0)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To initialize an instance of memory store that will preserve responses from the previous LLM model interaction(s), execute
        the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory

in_memory_store = {}

def get_chat_history(session_id: str) -> BaseChatMessageHistory:
  if session_id not in in_memory_store:
    in_memory_store[session_id] = InMemoryChatMessageHistory()
  return in_memory_store[session_id]</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create a new chat prompt template, with placeholders for the input as well as the chat history, execute the following code
        snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt5 = ChatPromptTemplate.from_messages([
  ('system', 'Helpful AI assistant!'),
  MessagesPlaceholder(variable_name='chat_history'),
  ('human', '{input}')
])</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create a simple <span class="bold">chain</span> that sends the user input to the chat LLM model, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>chain6 = prompt5 | ollama_chat_llm</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To create a <span class="bold">chain</span> that sends the user request (along with the chat history) to the chat LLM model
        and captures the responses in the memory store, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from langchain_core.runnables.history import RunnableWithMessageHistory

  chain6_with_history = RunnableWithMessageHistory(chain6, get_chat_history, input_messages_key='input', history_messages_key='chat_history')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates no output.</p>
    </div>
    <div id="para-div">
      <p>To run the just created chain by providing an user input, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>config = {'configurable': {'session_id': 'langchain'}}

output5 = chain6_with_history.invoke({'input': 'Suggest only top 3 leadership quotes'}, config=config)
print(output5.content)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>1. "Leaders are born, not made."" - Warren Bennis
2. "The best way to predict the future is to invent it." - Alan Kay
3. "Leadership isn't about being in charge; it's about taking care of those in your charge."" - Simon Sinek

These quotes encapsulate different aspects and philosophies on leadership that have inspired many individuals across various fields.

Would you like more insights or another set from a specific domain?</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To demonstrate that the chain remembers and uses the previous chat response(s) as a context in the next interaction, execute
        the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>output6 = chain6_with_history.invoke({'input': 'Quotes are not inspiring. try again'}, config=config)
print(output6.content)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>I apologize for the mismatch with expectations; let's explore some inspirational quotations about leadership:

1. "The strength of character is what makes us great leaders." - Nelson Mandela
2. "Leadership without vision limits possibilities." - Peter Drucker
3. "A leader who fears failure will never succeed in anything worthwhile, but a good follower can always learn from mistakes and grow stronger through them." - Unknown

These quotes aim to inspire by highlighting the importance of character strength for leadership success.

If you need further inspiration or another set tailored more closely towards your interests (like innovation-driven leaders), please let me know!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>This concludes the hands-on demonstration of using the various <span class="bold">LangChain</span> components !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://www.langchain.com/" target="_blank"><span class="bold">LangChain</span></a></p>
      <p><a href="https://python.langchain.com/docs/introduction/" target="_blank"><span class="bold">LangChain Documentation</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
