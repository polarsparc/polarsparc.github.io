<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on Hugging Face">
    <meta name="subject" content="Quick Primer on Hugging Face">
    <meta name="keywords" content="hugging_face, transformers">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on Hugging Face</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on Hugging Face</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">04/27/2024</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr"/>
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p>There seems to be a great interest as well as misinformation amongst folks around what <a href="https://huggingface.co/"
        target="_blank"><span class="hi-yellow">Hugging Face</span></a> really is. The intent of this article is to clarify and
        introduce the <span class="bold">Hugging Face</span> platform.</p>
      <p><span class="bold">Hugging Face</span> is a popular, open-source, democratized ecosystem for AI/ML community to discover,
        experiment, and share models, datasets, libraries, and applications. Think of it like how <span class="bold">GitHub</span>
        is to developers, but for the AI/ML community.</p>
      <p>In addition, <span class="bold">Hugging Face</span> provides an open-source <span class="bold">Python</span> library called
        <span class="hi-green">transformers</span>, which provides access to a plethora of pre-trained models which can be used for
        building, training, and deploying applications for natural language processing, audio/image/video processing, etc.</p>
      <p>Finally, <span class="bold">Hugging Face</span> provides a way for users to host their AI/ML applicfations via <a href=
        "https://huggingface.co/spaces" target="_blank"><span class="hi-yellow">Spaces</span></a> to either showcase projects or
        work collaboratively with others in the platform.</p>
      <p>In this primer, we will keep our focus on the commonly used language processing tasks as well as model tuning using the
        <span class="bold">Hugging Face</span> platform.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Installation and Setup</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will be on a <span class="bold">Ubuntu 22.04 LTS</span> based Linux desktop.</p>
      <p>Ensure that the <span class="bold">Python 3.x</span> programming language as well as the <span class="bold">Jupyter
        Notebook</span> package is installed and setup on the Linux desktop.</p>
    </div>
    <div id="para-div">
      <p>To install the necessary <span class="bold">Python</span> packages for this primer, execute the following command:</p>
    </div>
    <div id="cmd-div">
      <p>$ pip install torch datasets transformers</p>
    </div>
    <div id="para-div">
      <p>Note that the <span class="bold">transformers</span> package downloads pre-trained models from the <span class="bold">
        Hugging Face</span> models hub into a local cache directory as specified by the environment variable <span class="hi-yellow">
        TRANSFORMERS_CACHE</span>.</p>
      <p>The default cache directory is at <span class="hi-green">~/.cache/huggingface/hub</span></p>
    </div>
    <div id="para-div">
      <p>To run the <span class="bold">transformers</span> model in an offline model (after the model is downloaded and cached),
        one set the environment variable <span class="hi-yellow">TRANSFORMERS_OFFLINE</span> to a value of <span class="hi-red">1</span>.</p>
    </div>
    <div id="para-div">
      <p>This completes all the installation and setup for the <span class="bold">Hugging Face</span> hands-on demonstrations.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on with Hugging Face</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-vanila">Pipeline</span> is a base class that provides a simple API abstraction over complex sequence of
        operations and makes it easy to use the different models for the various AI/ML tasks, such as sentiment analysis (implemented
        by the <span class="hi-vanila">TextClassificationPipeline</span> class), text summarization (implemented by the <span class=
        "hi-vanila">SummarizationPipeline</span> class), text translation (implemented by the <span class="hi-vanila">TranslationPipeline
        </span> class), and so on.</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-blue">pipeline()</span> function is a wrapper that creates an instance of the <span class="bold">Pipeline
        </span> class and performs several tasks, such as, pre-processing (tokenization, embedding), model execution, and post-processing
        (labels, scores), etc.</p>
    </div>
    <div id="para-div">
      <p>Now, let us explore some of the commonly used natural language processing tasks in the following sections.</p>
    </div>
    <br/>
    <div id="step-div">
      <p>Sentiment Analysis</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="hi-yellow">Sentiment analysis</span> is the process of analyzing text to determine if the emotional tone of
        the message is positive, negative, or neutral.</p>
    </div>
    <div id="para-div">
      <p>To perform the pre-defined task of sentiment analysis on a given text using the <span class="bold">transformers</span> model,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import pipeline

sa_model = pipeline('sentiment-analysis')

text = 'The restaurant was meh, but expensive'

sa_model(text)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>[{'label': 'POSITIVE', 'score': 0.9943925738334656}]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the default pre-trained model used by <span class="bold">Hugging Face</span> for the <span class="bold">Sentiment
        Analysis</span> task is <span class="hi-purple">distilbert/distilbert-base-uncased-finetuned-sst-2-english</span>.</p>
    </div>
    <div id="para-div">
      <p>To perform sentiment analysis on a list of texts using the default pre-trained <span class="bold">transformers</span> model,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import pipeline

sa_model2 = pipeline('sentiment-analysis')

text_list = ['The book on Leadership was very good',
             'The movie was not that great']

sa_model2(text_list)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>[{'label': 'POSITIVE', 'score': 0.9998422861099243},
 {'label': 'NEGATIVE', 'score': 0.9997527003288269}]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>As indicated earlier, <span class="bold">Hugging Face</span> is a hub for various pre-trained models shared by other firms
        and individuals. One can explore the various pre-trained <a href="https://huggingface.co/models" target="_blank"><span class
        ="hi-yellow">Models</span></a> shared by others based on the task at hand, such as sentiment analysis.</p>
      <p>One of the popular pre-trained models is <span class="hi-purple">ProsusAI/finbert</span>. Let us you this model to re-test
        our list of text.</p>
    </div>
    <div id="para-div">
      <p>To perform sentiment analysis on a list of texts using the model <span class="bold">ProsusAI/finbert</span>, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import pipeline

sa_model3 = pipeline('sentiment-analysis', model='ProsusAI/finbert')

text_list = ['The book on Leadership was very good',
             'The movie was not that great']

sa_model3(text_list)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>[{'label': 'neutral', 'score': 0.6887921690940857},
 {'label': 'neutral', 'score': 0.8795873522758484}]</pre>
    </div>
    <br/>
    <div id="step-div">
      <p>Text Generation</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="hi-yellow">Text Generation</span> is the process of repeatedly predicting the most probable next word given
        the initial sequence of words, then appending the predicted word to the initial sequence, and repeating the process until
        a constraint (such as maximum length) is met.</p>
    </div>
    <div id="para-div">
      <p>To perform the pre-defined task of text generation on a given text using the <span class="bold">transformers</span> model,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import pipeline

tg_model = pipeline('text-generation')

prompt = 'Found a good book on Leadership and it was'

tg_model(prompt, max_length=60)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>[{'generated_text': 'Found a good book on Leadership and it was really helpful."\n\nHe said a few of his friends made some mistakes in their careers, including dropping the "great job" question.\n\n"I can\'t remember the last time I used that word, but it was definitely on the front page'}]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the default pre-trained model used by <span class="bold">Hugging Face</span> for the <span class="bold">Text
        Generation</span> task is <span class="hi-purple">openai-community/gpt2</span>.</p>
    </div>
    <div id="para-div">
      <p>Now, let us try the same text generation task using the model <span class="bold">microsoft/phi-2</span> by executing the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import pipeline

tg_model2 = pipeline('text-generation', model='microsoft/phi-2')

prompt = 'Found a good book on Leadership and it was'

tg_model2(prompt, max_length=60)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>[{'generated_text': 'Found a good book on Leadership and it was a great read. I am going to share it with my team.\nI am a big fan of the book "The 7 Habits of Highly Effective People" by Stephen Covey. It is a great read and I highly recommend it.\nI'}]</pre>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! AWARENESS !!!</h4>
      <pre>The model <span class="bold">microsoft/phi-2</span> will take some time to download and execute !!!</pre>
    </div>
    <br/>
    <div id="step-div">
      <p>Text Summarization</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="hi-yellow">Text Summarization</span> is the process of condensing a lengthy text document into a more compact
        version that still retains the most important information and meaning from the original text.</p>
    </div>
    <div id="para-div">
      <p>To perform the pre-defined task of text summarization on a given text using the <span class="bold">transformers</span> model,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import pipeline

sm_model = pipeline('summarization')

text2 = "New York City comprises 5 boroughs sitting where the Hudson River meets the Atlantic Ocean. At its core is Manhattan, a densely
         populated borough that's among the world's major commercial, financial and cultural centers. Its iconic sites include skyscrapers
         such as the Empire State Building and sprawling Central Park. Broadway theater is staged in neon-lit Times Square"

sm_model(text2)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>[{'summary_text': "New York City comprises 5 boroughs sitting where the Hudson River meets the Atlantic Ocean . At its core is Manhattan, a densely populated borough that's among the world's major commercial, financial and cultural centers . Its iconic sites include skyscrapers such as the Empire State Building and sprawling Central"}]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the default pre-trained model used by <span class="bold">Hugging Face</span> for the <span class="bold">Text
        Summarization</span> task is <span class="hi-purple">sshleifer/distilbart-cnn-12-6</span>.</p>
    </div>
    <br/>
    <div id="step-div">
      <p>Zero-Shot Classification</p>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="hi-yellow">Zero-Shot Classification</span> is the process of classifying a given text into a set of specified
        categories.</p>
    </div>
    <div id="para-div">
      <p>To perform the pre-defined task of zero-shot classification on a given text using the <span class="bold">transformers</span>
        model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import pipeline

zs_model = pipeline('zero-shot-classification')

text3 = 'Spring Boot is a popular Java Framework and favored by developers'

zs_model(text3, candidate_labels=['Books', 'Movies', 'Technology'])</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>{'sequence': 'Spring Boot is a popular Java Framework and favored by developers',
 'labels': ['Technology', 'Books', 'Movies'],
 'scores': [0.8282957673072815, 0.10491892695426941, 0.06678532809019089]}</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the default pre-trained model used by <span class="bold">Hugging Face</span> for the <span class="bold">Zero-Shot
        Classification</span> task is <span class="hi-purple">facebook/bart-large-mnli</span>.</p>
    </div>
    <br/>
    <div id="step-div">
      <p>Fine Tuning a Model</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The more data a model is trained on, the better the model performs. Rather than starting from scratch, one could use a <span
        class="bold">pre-trained</span> model to train the loaded model on the custom domain data set (referred to as <span class=
        "hi-yellow">Transfer Learning</span>), and then use the trained model for text processing tasks in the custom domain. This
        process is often referred to as <span class="hi-yellow">Fine Tuning</span> of a model.</p>
    </div>
    <div id="para-div">
      <p>For this demonstration, we will use a custom restaurant reviews data set. The reviews data set is split into two <span class
        ="bold">CSV</span> (tab separated) files.</p>
    </div>
    <div id="para-div">
      <p>To load the custom restaurant reviews data set using the <span class="bold">Hugging Face</span> utility function <span class
        ="hi-blue">load_dataset()</span> from the <span class="hi-vanila">datasets</span> module, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from datasets import load_dataset

dataset = load_dataset('csv', data_files={'train': './data/reviews-train.tsv', 'eval': './data/reviews-eval.tsv'}, delimiter='\t')

dataset</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 750
    })
    eval: Dataset({
        features: ['text', 'label'],
        num_rows: 251
    })
})</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that the reviews are text values of variable length. However, the <span class="bold">Deep Learning</span> models work
        only with numerical values. In order to convert the variable length texts to numbers, one would need some kind of a mechanism
        to split the text into words and symbols. This is where the <span class="bold">Hugging Face</span> tokenizer class <span class
        ="hi-vanila">AutoTokenizer</span> comes in handy.</p>
    </div>
    <div id="para-div">
      <p>To initialize an instance of the <span class="bold">AutoTokenizer</span> class from the <span class="hi-vanila">transformers
        </span> module, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import AutoTokenizer

default_model = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'

tokenizer = AutoTokenizer.from_pretrained(default_model)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To tokenize the given text, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>text = 'The restaurant was meh, but expensive'

tokens = tokenizer.tokenize(text)
tokens</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.9</h4>
      <pre>['the', 'restaurant', 'was', 'me', '##h', ',', 'but', 'expensive']</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To convert tokens to numbers, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>token_ids = tokenizer.convert_tokens_to_ids(tokens)
token_ids</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>[1996, 4825, 2001, 2033, 2232, 1010, 2021, 6450]</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Each of the reviews are of variable length words. For any <span class="bold">transformer</span> model to learn from the
        collection of reviews, the following challenges need to be addressed:</p>
      <ul id="blue-sqr-ul">
        <li><p>One needs a way to indicate the start and the end of each of the reviews from the data set. For this, the tokenizer
          uses <span class="hi-yellow">special</span> tokens to represent the start and the end</p></li>
        <li><p>Each of the reviews from the data set are of variable length (in terms of tokens). They all need to be represented
          of equal length upto a maximum length. For this, the tokenizer uses a dummy <span class="hi-yellow">padding</span> token
          for filling empty slots</p></li>
        <li><p>To identify the real tokens from the dummy padding tokens, the tokenizer uses a mask to the identify the real versus
          dummy tokens</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>To convert a given text into numerical tokens upto a maximum length satisfying the above constraints, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>token_ids_mask = tokenizer(text, padding=True, truncation=True, max_length=10, return_tensors='pt')
token_ids_mask</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>{'input_ids': tensor([[ 101, 1996, 4825, 2001, 2033, 2232, 1010, 2021, 6450,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}</pre>
    </div>
    <div id="para-div">
      <p>To convert each of reviews in the data set to its equivalent numerical representation, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>def tokenize_reviews(reviews):
  return tokenizer(reviews['text'], padding='max_length', truncation=True)

dataset = dataset.map(tokenize_reviews, batched=True)
dataset</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.12</h4>
      <pre>DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 750
    })
    eval: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 251
    })
})</pre>
    </div>
    <div id="para-div">
      <p>Note that our restaurant reviews only have <span class="underbold">TWO</span> classes - <span class="bold">Negative</span>
        (with a label of <span class="bold">'0'</span>) and <span class="bold">Positive</span> (with a label of <span class="bold">
        '1'</span>).</p>
    </div>
    <div id="para-div">
      <p>To predict the sentiment of a sample review text using the default <span class="bold">pre-trained</span> model, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>sa_classifier = AutoModelForSequenceClassification.from_pretrained(default_model, num_labels=2)

outputs = sa_classifier(input_ids=id_mask['input_ids'], attention_mask=id_mask['attention_mask'])

probs = F.softmax(outputs.logits, dim=-1)
probs</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.13</h4>
      <pre>tensor([[0.0056, 0.9944]], grad_fn=&lt;SoftmaxBackward0&gt;)</pre>
    </div>
    <div id="para-div">
      <p>The <span class="bold">Output.13</span> above seems to indicate a <span class="bold">Positive</span> sentiment, which is
        really not true !</p>
    </div>
    <div id="para-div">
      <p>This is an indication that we need to <span class="bold">Fine Tune</span> the default pre-trained model using our custom
        restaurant reviews data set.</p>
    </div>
    <div id="para-div">
      <p>Before we <span class="bold">Fine Tune</span> the default <span class="bold">pre-trained</span> model, we need to train the
        <span class="bold">tokenizer</span> on the tokens from our custom restaurant reviews data set.</p>
    </div>
    <div id="para-div">
      <p>To train our <span class="bold">tokenizer</span> on the tokens from the custom restaurant reviews data set, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>batch_sz = 16

def reviews_text_iterator(tag):
  for i in range(0, len(dataset[tag]), batch_sz):
      yield dataset[tag]['text'][i : i + batch_sz]

tokenizer = tokenizer.train_new_from_iterator(reviews_text_iterator('train'), len(tokenizer))
tuned_tokenizer = tokenizer.train_new_from_iterator(reviews_text_iterator('eval'), len(tokenizer))</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To train our default <span class="bold">pre-trained</span> model on the custom restaurant reviews data set, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>batch_sz = 16
num_train_epochs = 5
model_dir = './model'

logging_steps = len(dataset['train']) // batch_sz
num_steps = int((len(dataset['train']) / batch_sz) * num_train_epochs)
warmup_steps = int(0.2 * num_steps)
save_steps = 500

training_args = TrainingArguments(
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=batch_sz,
    per_device_eval_batch_size=batch_sz,
    load_best_model_at_end=True,
    evaluation_strategy='steps',
    save_strategy='steps',
    learning_rate=2e-5,
    logging_strategy='steps',
    warmup_steps= warmup_steps,
    save_steps=save_steps,
    output_dir=model_dir
)

trainer = Trainer(model=tuned_sa_classifier, tokenizer=tuned_tokenizer, args=training_args, train_dataset=dataset['train'])
trainer.train()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are some details about the various parameters:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-grey">num_train_epochs</span> is the number of training epochs to run</p></li>
        <li><p><span class="hi-grey">per_device_train_batch_size</span> is the batch size for the training data to the underlying
          device (CPU or GPU)</p></li>
        <li><p><span class="hi-grey">per_device_eval_batch_size</span> is the batch size for the evaluation data to the underlying
          device (CPU or GPU)</p></li>
        <li><p><span class="hi-grey">evaluation_strategy</span> indicates the model evaluation strategy to use during training. In
          our case, it is after <span class="hi-vanila">num_steps</span> iterations.</p></li>
        <li><p><span class="hi-grey">save_strategy</span> indicates the model checkpointing strategy to use during training. In our
          case, it is after <span class="hi-vanila">num_steps</span> iterations.</p></li>
        <li><p><span class="hi-grey">load_best_model_at_end</span> indicates which trained model instance to use from the training.
          In our case, we have indicated to use the best evaluated model from the checkpoint after the training.</p></li>
      </ul>
    </div>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>If your desktop has a decent GPU, the training will take about 2 minutes. Else, will run for about 25 minutes on a CPU !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Once again, to predict the sentiment of a sample review text using the default <span class="bold">pre-trained</span> model,
        execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>tuned_sa_classifier.to('cpu')

tuned_outputs = tuned_sa_classifier(input_ids=id_mask['input_ids'], attention_mask=id_mask['attention_mask'])

tuned_probs = F.softmax(tuned_outputs.logits, dim=-1)
tuned_probs</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <div id="out-div">
      <h4>Output.14</h4>
      <pre>tensor([[0.9838, 0.0162]], grad_fn=&lt;SoftmaxBackward0&gt;)</pre>
    </div>
    <div id="para-div">
      <p>The <span class="bold">Output.14</span> above indicates a <span class="bold">Negative</span> sentiment, which is seems to
        be the reality !</p>
    </div>
    <br/>
    <div id="step-div">
      <p>Large Language Model</p>
    </div>
    <br/>
    <div id="para-div">
      <p>A <span class="hi-yellow">Large Language Model</span> (or <span class="hi-vanila">LLM</span> for short) is a very large deep
        neural network (with billions of parameters) that is <span class="bold">pre-trained</span> on vast amounts of data from the
        Internet and has an enormous potential to achieve general-purpose language generation capabilities as well as perform other
        language processing tasks such as summarizing text, translating languages, completing sentences, etc.</p>
    </div>
    <div id="para-div">
      <p>To perform text generation on a given prompt using the recently released <span class="hi-purple">microsoft/Phi-3-mini-4k-instruct</span>
        LLM model, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

default_llm_model = 'microsoft/Phi-3-mini-4k-instruct'

llm_tokenizer = AutoTokenizer.from_pretrained(default_llm_model, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(default_llm_model, torch_dtype='auto', trust_remote_code=True)

prompts = [
    {'role': 'user', 'content': 'Describe what Hugging Face is all about in a sentence'}
]

pipe = pipeline('text-generation', model=llm_model, tokenizer=llm_tokenizer)

generation_args = {
  'max_new_tokens': 150,
  'return_full_text': False,
  'temperature': 0.0,
  'do_sample': False
}

llm_output = pipe(prompts, **generation_args)
llm_output</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.15</h4>
      <pre>[{'generated_text': "Hugging Face is a leading platform for natural language processing (NLP) that provides pre-trained models and tools for building and sharing machine learning models.\n\nHere's a more detailed description in a sentence:\n\nHugging Face is an innovative company that offers a comprehensive suite of open-source machine learning models and libraries, primarily focused on natural language understanding and generation, facilitating collaboration and accessibility in the AI research and development community."}]</pre>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! AWARENESS !!!</h4>
      <pre>The LLM model <span class="bold">microsoft/Phi-3-mini-4k-instruct</span> will take few minutes to download and execute !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>This concludes the various demonstrations on using the <span class="bold">Hugging Face</span> packages for the various text
        processing tasks !</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://huggingface.co/" target="_blank"><span class="bold">Hugging Face</span></a></p>
      <p><a href="https://polarsparc.github.io/data/reviews-train.tsv" target="_blank"><span class="bold">Restaurant Reviews - Training Data</span></a></p>
      <p><a href="https://polarsparc.github.io/data/reviews-eval.tsv" target="_blank"><span class="bold">Restaurant Reviews - Evaluation Data</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
