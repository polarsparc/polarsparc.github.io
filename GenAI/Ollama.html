<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on Ollama">
    <meta name="subject" content="Quick Primer on Ollama">
    <meta name="keywords" content="ollama, llm, python">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on Ollama</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on Ollama</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>05/30/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr"/>
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><a href="https://ollama.com/" target="_blank"><span class="hi-yellow">Ollama</span></a> is a powerful open source platform
        that simplifies the process of running various <span class="bold">Large Language Models</span> (or <span class="bold">LLM
        </span>s for short) on a local machine. It enables one to download the various pre-trained LLM models such as, DeepSeek-R1,
        Google Gemma-3, IBM Granite 3.3, Microsoft Phi-4, Alibaba Qwen 3, etc., and run them locally.</p>
      <p>In addition, the <span class="bold">Ollama</span> platform exposes a local API endpoint, which enables developers to build
        AI applications/workflows that can interact with the local LLMs using the API endpoint.</p>
      <p>Last but not the least, the <span class="bold">Ollama</span> platform effectively leverages the underlying hardware resouces
        of the local machine, such as CPU(s) and GPU(s), to efficiently and optimally run the LLMs for better performance.</p>
      <p>In this primer, we will demonstrate how one can effectively setup and run the <span class="bold">Ollama</span> platform using
        the <span class="bold">Docker</span> image.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Installation and Setup</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will can on a <span class="bold">Ubuntu 24.04 LTS</span> based Linux desktop AND a <span class=
        "bold">Apple Silicon</span> based Macbook Pro. Ensure that <span class="bold">Docker</span> is installed and setup on the
        desktop (see <a href="http://polarsparc.github.io/Docker/Docker.html" target="_blank"> instructions</a>).</p>
      <p>For Linux and MacOS, ensure that the <span class="bold">Python 3.1x</span> programming language as well as the <span class
        ="bold">Jupyter Notebook</span> packages are installed. In addition, ensure the command-line utilities <span class="bold">
        curl</span> and <span class="bold">jq</span> are installed.</p>
    </div>
    <div id="para-div">
      <p>For Linux and MacOS, we will setup two required directories by executing the following command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ mkdir -p $HOME/.ollama</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For Linux and MacOS, to pull and download the docker image for <span class="bold">Ollama</span>, execute the following
        command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker pull ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>0.9.0: Pulling from ollama/ollama
13b7e930469f: Pull complete 
97ca0261c313: Pull complete 
f6a9ed9582e4: Pull complete 
f6b71baa717c: Pull complete 
Digest: sha256:2ea3b768a8f2dcd4d910f838d79702bb952089414dd578146619c0a939647ac6
Status: Downloaded newer image for ollama/ollama:0.9.0
docker.io/ollama/ollama:0.9.0</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For Linux and MacOS, to install the necessary <span class="bold">Python</span> packages, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ pip install dotenv ollama pydantic</p>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>Note that by default, <span class="bold">docker</span> on MacOS is ONLY configured to use upto 8GB of RAM !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are the steps to adjust the <span class="bold">docker</span> resource usage configuration on MacOS:</p>
    </div>
    <div id="para-div">
      <p>Open the <span class="bold">Docker Desktop</span> on MacOS and click on the <span class="bold">Settings</span> gear icon as
        shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-1.png" alt="Docker MacOS Settings" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Click on the <span class="bold">Resources</span> item from the options on the left-hand side as shown in the following
        illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-2.png" alt="Docker MacOS Resources" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Choose the <span class="bold">CPU</span>, <span class="bold">Memory</span>, and <span class="bold">Disk Usage</span> limits
        and then click on the Network item under <span class="bold">Resources</span> on the left-hand side as shown in the following
        illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-3.png" alt="Docker MacOS Limits" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Choose the <span class="bold">Enable Host Networking</span> option and then click on the <span class="bold">Apply & Restart
        </span> button as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-4.png" alt="Docker MacOS Restart" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Finally, reboot the MacOS system for the changes to take effect.</p>
    </div>
    <div id="para-div">
      <p>This completes all the system installation and setup for the <span class="bold">Ollama</span> hands-on demonstration.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on with Ollama</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the following sections, we will show the commands for both Linux and MacOS, however, we will <span class="underbold">
        ONLY</span> show the output from Linux. Note that all the commands have been tested on both Linux and MacOS respectively.</p>
    </div>
    <div id="para-div">
      <p>Assuming that the ip address on the Linux desktop is <span class="hi-grey">192.168.1.25</span>, start the <span class="bold">
        Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For MacOS, start the <span class="bold">Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama -p 11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>time=2025-05-31T00:42:59.447Z level=INFO source=routes.go:1234 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-05-31T00:42:59.450Z level=INFO source=images.go:479 msg="total blobs: 23"
time=2025-05-31T00:42:59.451Z level=INFO source=images.go:486 msg="total unused blobs removed: 0"
time=2025-05-31T00:42:59.451Z level=INFO source=routes.go:1287 msg="Listening on [::]:11434 (version 0.9.0)"
time=2025-05-31T00:42:59.451Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-05-31T00:42:59.453Z level=INFO source=gpu.go:377 msg="no compatible GPUs were discovered"
time=2025-05-31T00:42:59.453Z level=INFO source=types.go:130 msg="inference compute" id=0 library=cpu variant="" compute="" driver=0.0 name="" total="62.7 GiB" available="57.6 GiB"</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>If the linux desktop has <span class="hi-green">Nvidia GPU</span> with decent amount of VRAM (at least 16 GB) and has been
        enabled for use with <span class="bold">docker</span> (see <a href="https://polarsparc.github.io/Docker/DockerNVidia.html"
        target="_blank"><span class="bold">instructions</span></a>), then execute the following command instead to start <span class
        ="bold">Ollama</span>:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama --gpus=all -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.9.0</p>
    </div>
    <br/>
    <div id="para-div">
      <p>On the MacOS, currently there is <span class="underbold">NO SUPPORT</span> for the <span class="bold">Apple Silicon GPU</span>
        and the above command <span class="underbold">WILL NOT</span> work !!!</p>
    </div>
    <div id="para-div">
      <p>For the hands-on demonstration, we will download and use three different pre-trained LLM models: the <span class="hi-purple">
        Microsoft Phi-4 Mini</span>, the <span class="hi-purple">Google Gemma-3 4B</span> and the <span class="hi-purple">Qwen-3 4B
        </span> respectively.</p>
    </div>
    <div id="para-div">
      <p>Open a new terminal window and execute the following <span class="bold">docker</span> command to download the Alibaba Qwen-3
        LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run qwen3:4b</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>pulling manifest 
pulling 163553aea1b1: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  2.6 GB                         
pulling eb4402837c78: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  1.5 KB                         
pulling d18a5cc71b84: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   11 KB                         
pulling cff3f395ef37: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   120 B                         
pulling 5efd52d6d9f2: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   487 B                         
verifying sha256 digest 
writing manifest 
success</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the open terminal window, execute the following <span class="bold">docker</span> command to download the Google Gemma-3
        LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run gemma3:4b</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>pulling manifest 
pulling ac71e9e32c0b... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  1.5 GB                         
pulling 3da071a01bbe... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  6.6 KB                         
pulling 4a99a6dd617d... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   11 KB                         
pulling f9ed27df66e9... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   417 B                         
verifying sha256 digest 
writing manifest 
success</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the open terminal window, execute the following <span class="bold">docker</span> command to download the Microsoft Phi-4
        LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run phi4-mini</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>pulling manifest 
pulling 377655e65351... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  3.3 GB                         
pulling e0a42594d802... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   358 B                         
pulling dd084c7d92a3... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  8.4 KB                         
pulling 0a74a8735bf3... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||    55 B                         
pulling ffae984acbea... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   489 B                         
verifying sha256 digest 
writing manifest 
success</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To test the just downloaded Microsoft Phi-4 LLM model, execute the following user prompt in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; describe a gpu in less than 50 words in json format</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>```json
{
  "gpu": {
    "description": "A Graphics Processing Unit optimized for rendering images and videos, accelerating computational tasks."
  }
}
```</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, we will shift gears to test the local API endpoint.</p>
    </div>
    <div id="para-div">
      <p>For Linux, open a new terminal window and execute the following command to list all the LLM models that are hosted in the
        running <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ curl -s http://192.168.1.25:11434/api/tags | jq</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For MacOS, open a new terminal window and execute the following command to list all the LLM models that are hosted in the
        running <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ curl -s http://127.0.0.1:11434/api/tags | jq</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output on Linux:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>{
  "models": [
    {
      "name": "qwen3:4b",
      "model": "qwen3:4b",
      "modified_at": "2025-05-01T23:42:43.224705385Z",
      "size": 2620788019,
      "digest": "a383baf4993bed20dd7a61a68583c1066d6f839187b66eda479aa0b238d45378",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "qwen3",
        "families": [
          "qwen3"
        ],
        "parameter_size": "4.0B",
        "quantization_level": "Q4_K_M"
      }
    },
    {
      "name": "gemma3:4b",
      "model": "gemma3:4b",
      "modified_at": "2025-03-14T11:38:45.044730835Z",
      "size": 3338801718,
      "digest": "c0494fe00251c4fc844e6a1801f9cbd26c37441d034af3cb9284402f7e91989d",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "gemma3",
        "families": [
          "gemma3"
        ],
        "parameter_size": "4.3B",
        "quantization_level": "Q4_K_M"
      }
    },
    {
      "name": "phi4-mini:latest",
      "model": "phi4-mini:latest",
      "modified_at": "2025-03-09T18:13:49.120068131Z",
      "size": 2491876774,
      "digest": "78fad5d182a7c33065e153a5f8ba210754207ba9d91973f57dffa7f487363753",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "phi3",
        "families": [
          "phi3"
        ],
        "parameter_size": "3.8B",
        "quantization_level": "Q4_K_M"
      }
    }
  ]
}</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>From the Output.7 above, it is evident we have the three LLM models ready for use !</p>
    </div>
    <div id="para-div">
      <p>Moving along to the next task !</p>
    </div>
    <div id="para-div">
      <p>For Linux, to send a user prompt to the LLM model for a response, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <pre>$ curl -s http://192.168.1.25:11434/api/generate -d '{
  "model": "phi4-mini:latest",
  "prompt": "describe a gpu in less than 50 words",
  "stream": false
}' | jq</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For MacOS, to send a user prompt to the LLM model for a response, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <pre>$ curl -s http://127.0.0.1:11434/api/generate -d '{
  "model": "phi4-mini:latest",
  "prompt": "describe a gpu in less than 50 words",
  "stream": false
}' | jq</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>{
  "model": "phi4-mini:latest",
  "created_at": "2025-05-31T00:45:54.457045269Z",
  "response": "A GPU (Graphics Processing Unit) accelerates image and video processing tasks, performing parallel computations efficiently to handle complex graphics rendering.",
  "done": true,
  "done_reason": "stop",
  "context": [
    200021,
    24346,
    261,
    82563,
    306,
    3760,
    1572,
    220,
    1434,
    6391,
    200020,
    200019,
    32,
    47969,
    350,
    27141,
    44532,
    14856,
    8,
    21798,
    1381,
    3621,
    326,
    3823,
    12323,
    13638,
    11,
    22415,
    26697,
    192859,
    34229,
    316,
    5318,
    8012,
    21276,
    42827,
    13
  ],
  "total_duration": 375475515,
  "load_duration": 17916687,
  "prompt_eval_count": 12,
  "prompt_eval_duration": 28019855,
  "eval_count": 26,
  "eval_duration": 329218459
}</pre>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="bold">BAM</span> - we have successfully tested the local API endpoints !</p>
    </div>
    <div id="para-div">
      <p>Now, we will test <span class="bold">Ollama</span> using <span class="bold">Python</span> code snippets.</p>
    </div>
    <div id="para-div">
      <p>Create a file called <span class="hi-yellow">.env</span> with the following environment variables defined:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>LLM_TEMPERATURE=0.0
# For Linux
OLLAMA_BASE_URL='http://192.168.1.25:11434'
# For MacOS - Comment the above and uncomment the below
# OLLAMA_BASE_URL='http://127.0.0.1:11434' 
OLLAMA_LANG_MODEL='phi4-mini:latest'
OLLAMA_TOOLS_MODEL='qwen3:4b'
OLLAMA_VISION_MODEL='gemma3:4b'
TEST_IMAGE='./data/test-image.png'
RECEIPT_IMAGE='./data/test-receipt.jpg'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To load the environment variables and assign them to corresponding <span class="bold">Python</span> variables, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from dotenv import load_dotenv, find_dotenv

import os

load_dotenv(find_dotenv())

llm_max_retries = int(os.getenv('LLM_MAX_RETRIES'))
llm_temperature = float(os.getenv('LLM_TEMPERATURE'))
ollama_model = os.getenv('OLLAMA_MODEL')
ollama_base_url = os.getenv('OLLAMA_BASE_URL')
ollama_api_key = 'ollama'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of the client class for <span class="bold">Ollama</span> running
        on the host URL, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from ollama import Client

client = Client(host=ollama_base_url)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To list all the LLM models that are hosted in the running <span class="bold">Ollama</span> platform, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>client.list()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.9</h4>
      <pre>ListResponse(models=[Model(model='qwen3:4b', modified_at=datetime.datetime(2025, 5, 1, 23, 42, 43, 224705, tzinfo=TzInfo(UTC)), digest='a383baf4993bed20dd7a61a68583c1066d6f839187b66eda479aa0b238d45378', size=2620788019, details=ModelDetails(parent_model='', format='gguf', family='qwen3', families=['qwen3'], parameter_size='4.0B', quantization_level='Q4_K_M')), Model(model='gemma3:4b', modified_at=datetime.datetime(2025, 3, 14, 11, 38, 45, 44730, tzinfo=TzInfo(UTC)), digest='c0494fe00251c4fc844e6a1801f9cbd26c37441d034af3cb9284402f7e91989d', size=3338801718, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M')), Model(model='phi4-mini:latest', modified_at=datetime.datetime(2025, 3, 9, 18, 13, 49, 120068, tzinfo=TzInfo(UTC)), digest='78fad5d182a7c33065e153a5f8ba210754207ba9d91973f57dffa7f487363753', size=2491876774, details=ModelDetails(parent_model='', format='gguf', family='phi3', families=['phi3'], parameter_size='3.8B', quantization_level='Q4_K_M')))])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To get a text response for a user prompt from the Microsoft Phi-4 LLM model running on the <span class="bold">Ollama</span>
        platform, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>client.chat(model='phi4-mini:latest', messages=[{'role': 'user', 'content': 'Describe ollama in less than 50 words'}])</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>ChatResponse(model='phi4-mini:latest', created_at='2025-05-31T18:13:11.824089883Z', done=True, done_reason='stop', total_duration=2533171172, load_duration=1986435499, prompt_eval_count=12, prompt_eval_duration=133903353, eval_count=34, eval_duration=412196167, message=Message(role='assistant', content='Ollama is an advanced, open-source large language model capable of generating human-like text by processing vast amounts of data to produce coherent and contextually relevant responses.', images=None, tool_calls=None))</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For the next task, we will attempt to present the LLM model response in a structured form using a Pydantic data class. For
        that, we will first define a class object by executing the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic import BaseModel

class GpuSpecs(BaseModel):
  name: str
  vram: int
  cuda_cores: int
  tensor_cores: int</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To receive a LLM model response in the desired format for the specific user prompt from the <span class="bold">Ollama</span>
        platform, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response = client.chat(model='phi4-mini:latest',
                       messages=[{'role': 'user', 'content': 'Extract the GPU specifications for Nvidia RTX 4060 Ti'}],
                       format=GpuSpecs.model_json_schema())</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To display the results in the structred form, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>rtx_4060 = (GpuSpecs.model_validate_json(response.message.content))
rtx_4060</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>GpuSpecs(name='nvidia_rtx_4060_ti_specs', vram=12, cuda_cores=6144, tensor_cores=384)</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Moving along, we will now demonstrate the Optical Character Recognition (OCR) capabilities by processing the image of a <a
        href="https://polarsparc.github.io/GenAI/images/test-receipt.jpg" target="_blank"><span class="bold">Transaction Receipt</span>
        </a> !!!</p>
    </div>
    <div id="para-div">
      <p>Execute the following code snippet to define a method to convert a JPG image to base64 string, use it to convert the image
        of the receipt to a base64 string, and send a user prompt to the <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from io import BytesIO
from PIL import Image

import base64

def jpg_to_base64(image):
  jpg_buffer = BytesIO()
  pil_image = Image.open(image)
  pil_image.save(jpg_buffer, format='JPEG')
  return base64.b64encode(jpg_buffer.getvalue()).decode('utf-8')

response = client.chat(
    model=ollama_vision_model,
    messages=[
        {
            'role': 'user',
            'content': 'Itemize all the transactions from this receipt image in detail',
            'images': [jpg_to_base64(receipt_image)]
        }
    ]
)

print(response['message']['content'])</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code snippet generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.12</h4>
      <pre>**Darth Vader #1234: Transactions**

*   **Feb 17:** AMAZON MKTPL*N606Z9AF3Amzn.com/billWA - $9.87
*   **Feb 17:** AMAZON MKTPL*L89WB2J1Amzn.com/billWA - $29.99
*   **Feb 20:** AMAZON RETA*C14EN8XC3WWW.AMAZON.CO.WA - $74.63

**Rey Skywalker #9876: Transactions**

*   **Feb 15:** TJMAX*0224LRAWNCEVILLENJ - $21.31
*   **Feb 15:** WEGMANS*93PRINCETONNJ - $17.75
*   **Feb 15:** PATEL BROTHERS EAST WINDSORNJ - $77.75
*   **Feb 15:** TJ MAX*B28EAST WINDSORNJ - $90.58
*   **Feb 15:** TRADER JOE S*607PRINCE TONNJ - $2.69
*   **Feb 16:** SHOPRITE LAWNCSL*S1LRAWNCEVILLENJ - $30.16
*   **Feb 18:** WEGMANS*93PRINCE TONNJ - $19.35
*   **Feb 18:** HALO FARMLAWNCEVILLENJ - $13.96

**Total:** $258.76</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Next task, we will demonstrate the tool processing capabilities of the the <span class="bold">Ollama</span> platform.</p>
    </div>
    <div id="para-div">
      <p>Execute the following code snippet to create a custom tool for executing shell commands and reference it in the user prompt
        sent to the <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>import subprocess
  
def execute_command(command: str) -> str:
    """
    tool to execute a given command and output its result

      Args:
        command (str): The command to execute

      Returns:
        str: The output from the command execution
    """
    print(f'Executing the command: {command}')
    try:
        result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)
        if result.returncode != 0:
            return f'Error executing the command - {command}'
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(e)

response = client.chat(
    model=ollama_tools_model,
    messages=[{'role': 'user', 'content': 'Execute the command "docker --version" if a tool is provided and display the output'}],
    tools=[execute_command]
)

print(response['message']['tool_calls'])
if response5.message.tool_calls:
  for tool in response5.message.tool_calls:
    if tool.function.name == 'execute_command':
      print(f'Ready to call Func: {tool.function.name} with Args: {tool.function.arguments}')
      output = execute_command(**tool.function.arguments)
      print(f'Func output: {output}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code snippet generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.13</h4>
      <pre>[ToolCall(function=Function(name='execute_command', arguments={'command': 'docker --version'}))]
Ready to call Func: execute_command with Args: {'command': 'docker --version'}
Executing the command: docker --version
Func output: Docker version 27.5.1, build 27.5.1-0ubuntu3~24.04.1</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>With this, we conclude the various demonstrations on using the <span class="bold">Ollama</span> platform for running and
        working with the pre-trained LLM models locally !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://ollama.com/" target="_blank"><span class="bold">Ollama</span></a></p>
      <p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md" target="_blank"><span class="bold">Ollama API</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
