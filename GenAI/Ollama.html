<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Quick Primer on Ollama">
    <meta name="subject" content="Quick Primer on Ollama">
    <meta name="keywords" content="ollama, llm, python">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Quick Primer on Ollama</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Quick Primer on Ollama</p>
    </div>
    <br/>
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td"><span class="hi-yellow">*UPDATED*</span>08/08/2025</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr"/>
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><a href="https://ollama.com/" target="_blank"><span class="hi-yellow">Ollama</span></a> is a powerful open source platform
        that simplifies the process of running various <span class="bold">Large Language Models</span> (or <span class="bold">LLM
        </span>s for short) on a local machine. It enables one to download the various pre-trained LLM models such as, Alibaba Qwen
        3, DeepSeek-R1, Google Gemma-3, IBM Granite 3.3, Microsoft Phi-4, OpenAI GPT-OSS, etc., and run them locally.</p>
      <p>In addition, the <span class="bold">Ollama</span> platform exposes a local API endpoint, which enables developers to build
        AI applications/workflows that can interact with the local LLMs using the API endpoint.</p>
      <p>Last but not the least, the <span class="bold">Ollama</span> platform effectively leverages the underlying hardware resouces
        of the local machine, such as CPU(s) and GPU(s), to efficiently and optimally run the LLMs for better performance.</p>
      <p>In this primer, we will demonstrate how one can effectively setup and run the <span class="bold">Ollama</span> platform using
        the <span class="bold">Docker</span> image.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Installation and Setup</p>
    </div>
    <div id="para-div">
      <p>The installation and setup will can on a <span class="bold">Ubuntu 24.04 LTS</span> based Linux desktop AND a <span class=
        "bold">Apple Silicon</span> based Macbook Pro. Ensure that <span class="bold">Docker</span> is installed and setup on the
        desktop (see <a href="http://polarsparc.github.io/Docker/Docker.html" target="_blank"> instructions</a>).</p>
      <p>For Linux and MacOS, ensure that the <span class="bold">Python 3.1x</span> programming language as well as the <span class
        ="bold">Jupyter Notebook</span> packages are installed. In addition, ensure the command-line utilities <span class="bold">
        curl</span> and <span class="bold">jq</span> are installed.</p>
    </div>
    <div id="para-div">
      <p>For Linux and MacOS, we will setup two required directories by executing the following command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ mkdir -p $HOME/.ollama</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For Linux and MacOS, to pull and download the docker image for <span class="bold">Ollama</span>, execute the following
        command in a terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker pull ollama/ollama:0.11.4</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>0.11.4: Pulling from ollama/ollama
32f112e3802c: Pull complete 
e6712969789d: Pull complete 
337f48b3aa16: Pull complete 
72a9be8f1ff9: Pull complete 
Digest: sha256:be17b353bf3cfab0b6980530284e64716a57589ed753a82d9a6a2a5fa9a61a31
Status: Downloaded newer image for ollama/ollama:0.11.4
docker.io/ollama/ollama:0.11.4</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For Linux and MacOS, to install the necessary <span class="bold">Python</span> packages, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ pip install dotenv ollama pydantic</p>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>Note that by default, <span class="bold">docker</span> on MacOS is ONLY configured to use upto 8GB of RAM !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are the steps to adjust the <span class="bold">docker</span> resource usage configuration on MacOS:</p>
    </div>
    <div id="para-div">
      <p>Open the <span class="bold">Docker Desktop</span> on MacOS and click on the <span class="bold">Settings</span> gear icon as
        shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-1.png" alt="Docker MacOS Settings" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Click on the <span class="bold">Resources</span> item from the options on the left-hand side as shown in the following
        illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-2.png" alt="Docker MacOS Resources" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Choose the <span class="bold">CPU</span>, <span class="bold">Memory</span>, and <span class="bold">Disk Usage</span> limits
        and then click on the Network item under <span class="bold">Resources</span> on the left-hand side as shown in the following
        illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-3.png" alt="Docker MacOS Limits" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Choose the <span class="bold">Enable Host Networking</span> option and then click on the <span class="bold">Apply & Restart
        </span> button as shown in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img class="img-cls" src="./images/ollama-macos-4.png" alt="Docker MacOS Restart" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Finally, reboot the MacOS system for the changes to take effect.</p>
    </div>
    <div id="para-div">
      <p>This completes all the system installation and setup for the <span class="bold">Ollama</span> hands-on demonstration.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on with Ollama</p>
    </div>
    <br/>
    <div id="para-div">
      <p>In the following sections, we will show the commands for both Linux and MacOS, however, we will <span class="underbold">
        ONLY</span> show the output from Linux. Note that all the commands have been tested on both Linux and MacOS respectively.</p>
    </div>
    <div id="para-div">
      <p>Assuming that the ip address on the Linux desktop is <span class="hi-grey">192.168.1.25</span>, start the <span class="bold">
        Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.11.4</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For MacOS, start the <span class="bold">Ollama</span> platform by executing the following command in the terminal window:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama -p 11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.11.4</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>time=2025-08-07T23:39:51.567Z level=INFO source=routes.go:1304 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-08-07T23:39:51.568Z level=INFO source=images.go:477 msg="total blobs: 32"
time=2025-08-07T23:39:51.569Z level=INFO source=images.go:484 msg="total unused blobs removed: 0"
time=2025-08-07T23:39:51.569Z level=INFO source=routes.go:1357 msg="Listening on [::]:11434 (version 0.11.4)"
time=2025-08-07T23:39:51.569Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-08-07T23:39:51.571Z level=INFO source=gpu.go:377 msg="no compatible GPUs were discovered"
time=2025-08-07T23:39:51.571Z level=INFO source=types.go:130 msg="inference compute" id=0 library=cpu variant="" compute="" driver=0.0 name="" total="62.7 GiB" available="58.9 GiB"</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>If the linux desktop has <span class="hi-green">Nvidia GPU</span> with decent amount of VRAM (at least 16 GB) and has been
        enabled for use with <span class="bold">docker</span> (see <a href="https://polarsparc.github.io/Docker/DockerNVidia.html"
        target="_blank"><span class="bold">instructions</span></a>), then execute the following command instead to start <span class
        ="bold">Ollama</span>:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker run --rm --name ollama --gpus=all -p 192.168.1.25:11434:11434 -v $HOME/.ollama:/root/.ollama ollama/ollama:0.11.4</p>
    </div>
    <br/>
    <div id="para-div">
      <p>On the MacOS, currently there is <span class="underbold">NO SUPPORT</span> for the <span class="bold">Apple Silicon GPU</span>
        and the above command <span class="underbold">WILL NOT</span> work !!!</p>
    </div>
    <div id="para-div">
      <p>For the hands-on demonstration, we will download and use three different pre-trained LLM models: the <span class="hi-purple">
        OpenAI GPT-OSS 20B</span>, the <span class="hi-purple">Google Gemma-3 4B</span>, and the <span class="hi-purple">Qwen-3 4B</span>
        respectively.</p>
    </div>
    <div id="para-div">
      <p>Open a new terminal window (referred to as T-1), execute the following <span class="bold">docker</span> command to download
        the Alibaba Qwen 3 4B LLM model:</p>
    </div>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run qwen3:4b</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>pulling manifest 
pulling 163553aea1b1: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  2.6 GB                         
pulling eb4402837c78: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  1.5 KB                         
pulling d18a5cc71b84: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   11 KB                         
pulling cff3f395ef37: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   120 B                         
pulling 5efd52d6d9f2: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   487 B                         
verifying sha256 digest 
writing manifest 
success</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>In terminal window T-1, to exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Once again, in terminal window T-1, execute the following <span class="bold">docker</span> command to download the Google
        Gemma-3 4B LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run gemma3:4b</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.4</h4>
      <pre>pulling manifest 
pulling ac71e9e32c0b... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  1.5 GB                         
pulling 3da071a01bbe... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  6.6 KB                         
pulling 4a99a6dd617d... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   11 KB                         
pulling f9ed27df66e9... 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   417 B                         
verifying sha256 digest 
writing manifest 
success</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>In terminal window T-1, to exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>One final time, in terminal window T-1, execute the following <span class="bold">docker</span> command to download the OpenAI
        GPT-OSS 20B LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama run gpt-oss:20b</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>pulling manifest 
pulling b112e727c6f1: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   13 GB                         
pulling 51468a0fd901: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||  7.4 KB                         
pulling f60356777647: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   11 KB                         
pulling d8ba2f9a17b3: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||    18 B                         
pulling 8d6fddaf04b2: 100%  ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||   489 B                         
verifying sha256 digest 
writing manifest 
success</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Open another new terminal window (referred to as T-2) and execute the following <span class="bold">docker</span> command to
        list all the downloaded LLM model(s):</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama list</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.6</h4>
      <pre>NAME                     ID              SIZE      MODIFIED     
gpt-oss:20b              f2b8351c629c    13 GB     25 hours ago    
qwen3:4b                 a383baf4993b    2.6 GB    3 months ago    
gemma3:4b                c0494fe00251    3.3 GB    4 months ago</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>In the terminal window T-2, execute the following <span class="bold">docker</span> command to list the running LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama ps</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.7</h4>
      <pre>NAME           ID              SIZE     PROCESSOR    CONTEXT    UNTIL              
gpt-oss:20b    f2b8351c629c    14 GB    100% GPU     4096       4 minutes from now</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>As is evident from the Output.7 above, the OpenAI GPT-OSS 20B LLM model is fully loaded and running on the GPU.</p>
    </div>
    <div id="para-div">
      <p>In the terminal window T-2, execute the following <span class="bold">docker</span> command to display information about
        the specific LLM model:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ docker exec -it ollama ollama show gpt-oss:20b</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>  Model
    architecture        gptoss    
    parameters          20.9B     
    context length      131072    
    embedding length    2880      
    quantization        MXFP4     

  Capabilities
    completion    
    tools         
    thinking      

  Parameters
    temperature    1    

  License
    Apache License               
    Version 2.0, January 2004    
    ...</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To test the just downloaded OpenAI GPT-OSS 20B LLM model, execute the following user prompt in the terminal window T-1:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; describe a gpu in less than 50 words in json format</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.9</h4>
      <pre>```json
{
  "description": "A GPU is a specialized processor that accelerates graphics and parallel computations, enabling real-time 
rendering, AI inference, and high-performance computing. It features thousands of cores, high memory bandwidth, and 
programmable shaders."
}
```</pre>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>On MacOS, the OpenAI GPT-OSS 20B LLM model is a little bit sluggish, but works - 08/08/2025 !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>In the terminal window T-1, to exit the user input, execute the following user prompt:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>&gt;&gt;&gt; /bye</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, we will shift gears to test the local API endpoint.</p>
    </div>
    <div id="para-div">
      <p>For Linux, open a new terminal window and execute the following command to list all the LLM models that are hosted in the
        running <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ curl -s http://192.168.1.25:11434/api/tags | jq</p>
    </div>
    <br/>
    <div id="para-div">
      <p>For MacOS, open a new terminal window and execute the following command to list all the LLM models that are hosted in the
        running <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <p>$ curl -s http://127.0.0.1:11434/api/tags | jq</p>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output on Linux:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>{
  "models": [
    {
      "name": "gpt-oss:20b",
      "model": "gpt-oss:20b",
      "modified_at": "2025-08-05T23:50:42.593911502Z",
      "size": 13780173839,
      "digest": "f2b8351c629c005bd3f0a0e3046f905afcbffede19b648e4bd7c884cdfd63af6",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "gptoss",
        "families": [
          "gptoss"
        ],
        "parameter_size": "20.9B",
        "quantization_level": "MXFP4"
      }
    },
    {
      "name": "qwen3:4b",
      "model": "qwen3:4b",
      "modified_at": "2025-05-01T23:42:43.224705385Z",
      "size": 2620788019,
      "digest": "a383baf4993bed20dd7a61a68583c1066d6f839187b66eda479aa0b238d45378",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "qwen3",
        "families": [
          "qwen3"
        ],
        "parameter_size": "4.0B",
        "quantization_level": "Q4_K_M"
      }
    },
    {
      "name": "gemma3:4b",
      "model": "gemma3:4b",
      "modified_at": "2025-03-14T11:38:45.044730835Z",
      "size": 3338801718,
      "digest": "c0494fe00251c4fc844e6a1801f9cbd26c37441d034af3cb9284402f7e91989d",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "gemma3",
        "families": [
          "gemma3"
        ],
        "parameter_size": "4.3B",
        "quantization_level": "Q4_K_M"
      }
    }
  ]
}</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>From the Output.10 above, it is evident we have the three LLM models ready for use !</p>
    </div>
    <div id="para-div">
      <p>Moving along to the next task !</p>
    </div>
    <div id="para-div">
      <p>For Linux, to send a user prompt to the LLM model for a response, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <pre>$ curl -s http://192.168.1.25:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "describe a gpu in less than 50 words",
  "stream": false
}' | jq</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For MacOS, to send a user prompt to the LLM model for a response, execute the following command:</p>
    </div>
    <br/>
    <div id="cmd-div">
      <pre>$ curl -s http://127.0.0.1:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "describe a gpu in less than 50 words",
  "stream": false
}' | jq</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical trimmed output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>{
  "model": "gpt-oss:20b",
  "created_at": "2025-08-08T00:36:59.650502603Z",
  "response": "A GPU is a specialized processor built for parallel computing, handling complex graphics rendering and data‑intensive tasks. It contains many cores, high memory bandwidth, and dedicated caches, enabling rapid image processing, AI inference, and scientific simulations.",
  "thinking": "We need to describe a GPU in less than 50 words. Probably about its architecture, core functions, etc. Keep under 50 words. Let's produce about 40 words. Let's count: \"A GPU is a specialized processor designed for parallel computing, handling complex graphics rendering and data-intensive tasks. It comprises many cores, high memory bandwidth, and dedicated caches, enabling rapid image processing, AI inference, and scientific simulations.\" Count words: \"A(1) GPU(2) is(3) a(4) specialized(5) processor(6) designed(7) for(8) parallel(9) computing,(10) handling(11) complex(12) graphics(13) rendering(14) and(15) data-intensive(16) tasks.(17) It(18) comprises(19) many(20) cores,(21) high(22) memory(23) bandwidth,(24) and(25) dedicated(26) caches,(27) enabling(28) rapid(29) image(30) processing,(31) AI(32) inference,(33) and(34) scientific(35) simulations.(36)\" 36 words. Good.",
  "done": true,
  "done_reason": "stop",
  "context": [
    200006,
    17360,
    200008,
    3575,
    553,
    ... [ SNIP ] ...    
    11,
    326,
    19950,
    68830,
    13
  ],
  "total_duration": 13542134801,
  "load_duration": 5414224093,
  "prompt_eval_count": 76,
  "prompt_eval_duration": 818069952,
  "eval_count": 297,
  "eval_duration": 7308941576
}</pre>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="bold">BAM</span> - we have successfully tested the local API endpoints !</p>
    </div>
    <div id="para-div">
      <p>Now, we will test <span class="bold">Ollama</span> using <span class="bold">Python</span> code snippets.</p>
    </div>
    <div id="para-div">
      <p>Create a file called <span class="hi-yellow">.env</span> with the following environment variables defined:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>LLM_TEMPERATURE=0.2
OLLAMA_BASE_URL='http://192.168.1.25:11434'
OLLAMA_LANG_MODEL='gpt-oss:20b'
OLLAMA_STRUCT_MODEL='qwen3:4b'
OLLAMA_TOOLS_MODEL='gpt-oss:20b'
OLLAMA_VISION_MODEL='gemma3:4b'
TEST_IMAGE='./data/test-image.png'
RECEIPT_IMAGE='./data/test-receipt.jpg'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To load the environment variables and assign them to corresponding <span class="bold">Python</span> variables, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from dotenv import load_dotenv, find_dotenv

import os

load_dotenv(find_dotenv())

llm_temperature = float(os.getenv('LLM_TEMPERATURE'))
ollama_base_url = os.getenv('OLLAMA_BASE_URL')
ollama_lang_model = os.getenv('OLLAMA_LANG_MODEL')
ollama_struct_model = os.getenv('OLLAMA_STRUCT_MODEL')
ollama_tools_model = os.getenv('OLLAMA_TOOLS_MODEL')
ollama_vision_model = os.getenv('OLLAMA_VISION_MODEL')
test_image = os.getenv('TEST_IMAGE')
receipt_image = os.getenv('RECEIPT_IMAGE')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To initialize an instance of the client class for <span class="bold">Ollama</span> running on the host URL, execute the
        following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from ollama import Client

client = Client(host=ollama_base_url)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To list all the LLM models that are hosted in the running <span class="bold">Ollama</span> platform, execute the following
        code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>client.list()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.12</h4>
      <pre>ListResponse(models=[Model(model='gpt-oss:20b', modified_at=datetime.datetime(2025, 8, 5, 23, 50, 42, 593911, tzinfo=TzInfo(UTC)), digest='f2b8351c629c005bd3f0a0e3046f905afcbffede19b648e4bd7c884cdfd63af6', size=13780173839, details=ModelDetails(parent_model='', format='gguf', family='gptoss', families=['gptoss'], parameter_size='20.9B', quantization_level='MXFP4')), Model(model='qwen3:4b', modified_at=datetime.datetime(2025, 5, 1, 23, 42, 43, 224705, tzinfo=TzInfo(UTC)), digest='a383baf4993bed20dd7a61a68583c1066d6f839187b66eda479aa0b238d45378', size=2620788019, details=ModelDetails(parent_model='', format='gguf', family='qwen3', families=['qwen3'], parameter_size='4.0B', quantization_level='Q4_K_M')), Model(model='gemma3:4b', modified_at=datetime.datetime(2025, 3, 14, 11, 38, 45, 44730, tzinfo=TzInfo(UTC)), digest='c0494fe00251c4fc844e6a1801f9cbd26c37441d034af3cb9284402f7e91989d', size=3338801718, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M'))])</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>To get a text response for a user prompt from the OpenAI GPT-OSS 20B LLM model running on the <span class="bold">Ollama</span>
        platform, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>client.chat(model=ollama_lang_model,
            options={'temperature': llm_temperature},
            messages=[{'role': 'user', 'content': 'Describe ollama in less than 50 words'}])</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.13</h4>
      <pre>ChatResponse(model='gpt-oss:20b', created_at='2025-08-08T17:35:07.419790575Z', done=True, done_reason='stop', total_duration=12634388602, load_duration=5506738652, prompt_eval_count=76, prompt_eval_duration=850709350, eval_count=246, eval_duration=6275941029, message=Message(role='assistant', content='Ollama is an open‑source framework that lets developers run large language models locally via a simple CLI. It hosts a model hub, supports multiple LLMs, and focuses on speed, privacy, and easy deployment.', thinking='We need to describe Ollama in less than 50 words. Ollama is a platform for running large language models locally, open-source, with a CLI, model hub, etc. Provide concise description. Let\'s count words. We\'ll aim for maybe 30 words. Let\'s craft: "Ollama is an open‑source framework that lets developers run large language models locally via a simple CLI. It hosts a model hub, supports multiple LLMs, and focuses on speed, privacy, and easy deployment." Count words: Ollama(1) is2 an3 open‑source4 framework5 that6 lets7 developers8 run9 large10 language11 models12 locally13 via14 a15 simple16 CLI.17 It18 hosts19 a20 model21 hub,22 supports23 multiple24 LLMs,25 and26 focuses27 on28 speed,29 privacy,30 and31 easy32 deployment33. That\'s 33 words. Good.', images=None, tool_name=None, tool_calls=None))</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>For the next task, we will attempt to present the LLM model response in a structured form using a Pydantic data class. For
        that, we will first define a class object by executing the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from pydantic import BaseModel

class GpuSpecs(BaseModel):
  name: str
  vram: int
  cuda_cores: int
  tensor_cores: int</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To receive a LLM model response in the desired format for the specific user prompt from the <span class="bold">Ollama</span>
        platform, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>response = client.chat(model=ollama_struct_model,
                       options={'temperature': llm_temperature},
                       messages=[{'role': 'user', 'content': 'Extract the GPU specifications for Nvidia RTX 4060 Ti'}],
                       format=GpuSpecs.model_json_schema())</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>To display the results in the structred form, execute the following code snippet:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>rtx_4060 = (GpuSpecs.model_validate_json(response.message.content))
rtx_4060</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following should be the typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.14</h4>
      <pre>GpuSpecs(name='Nvidia RTX 4060 Ti', vram=16, cuda_cores=4864, tensor_cores=144)</pre>
    </div>
    <br/>
    <br/>
    <div id="warn-div">
      <h4>!!! ATTENTION !!!</h4>
      <pre>Structured Output (via Pydantic) *FAILS* to work in OpenAI GPT-OSS 20B LLM model at this point in time - 08/08/2025 !!!</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Moving along, we will now demonstrate the Optical Character Recognition (OCR) capabilities by processing the image of a <a
        href="https://polarsparc.github.io/GenAI/images/test-receipt.jpg" target="_blank"><span class="bold">Transaction Receipt</span>
        </a> !!!</p>
    </div>
    <div id="para-div">
      <p>Execute the following code snippet to define a method to convert a JPG image to base64 string, use it to convert the image
        of the receipt to a base64 string, and send a user prompt to the <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>from io import BytesIO
from PIL import Image

import base64

def jpg_to_base64(image):
  jpg_buffer = BytesIO()
  pil_image = Image.open(image)
  pil_image.save(jpg_buffer, format='JPEG')
  return base64.b64encode(jpg_buffer.getvalue()).decode('utf-8')

response = client.chat(
    model=ollama_vision_model,
    messages=[
        {
            'role': 'user',
            'content': 'Itemize all the transactions from this receipt image in detail',
            'images': [jpg_to_base64(receipt_image)]
        }
    ]
)

print(response['message']['content'])</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code snippet generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.15</h4>
      <pre>**Darth Vader #1234: Transactions**

*   **Feb 17:** AMAZON MKTPL*N606Z9AF3Amzn.com/billWA - $9.87
*   **Feb 17:** AMAZON MKTPL*L89WB2J1Amzn.com/billWA - $29.99
*   **Feb 20:** AMAZON RETA*C14EN8XC3WWW.AMAZON.CO.WA - $74.63

**Rey Skywalker #9876: Transactions**

*   **Feb 15:** TJMAX*0224LRAWNCEVILLENJ - $21.31
*   **Feb 15:** WEGMANS*93PRINCETONNJ - $17.75
*   **Feb 15:** PATEL BROTHERS EAST WINDSORNJ - $77.75
*   **Feb 15:** TJ MAX*B28EAST WINDSORNJ - $90.58
*   **Feb 15:** TRADER JOE S*607PRINCE TONNJ - $2.69
*   **Feb 16:** SHOPRITE LAWNCSL*S1LRAWNCEVILLENJ - $30.16
*   **Feb 18:** WEGMANS*93PRINCE TONNJ - $19.35
*   **Feb 18:** HALO FARMLAWNCEVILLENJ - $13.96

**Total:** $258.76</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>Next task, we will demonstrate the tool processing capabilities of the the <span class="bold">Ollama</span> platform.</p>
    </div>
    <div id="para-div">
      <p>Execute the following code snippet to create a custom tool for executing shell commands and reference it in the user prompt
        sent to the <span class="bold">Ollama</span> platform:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="gen-src-body">
<pre>import subprocess
  
def execute_command(command: str) -> str:
    """
    tool to execute a given command and output its result

      Args:
        command (str): The command to execute

      Returns:
        str: The output from the command execution
    """
    print(f'Executing the command: {command}')
    try:
        result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)
        if result.returncode != 0:
            return f'Error executing the command - {command}'
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(e)

response = client.chat(
    model=ollama_tools_model,
    messages=[{'role': 'user', 'content': 'Execute the command "docker --version" if a tool is provided and display the output'}],
    tools=[execute_command]
)

print(response['message']['tool_calls'])
if response5.message.tool_calls:
  for tool in response5.message.tool_calls:
    if tool.function.name == 'execute_command':
      print(f'Ready to call Func: {tool.function.name} with Args: {tool.function.arguments}')
      output = execute_command(**tool.function.arguments)
      print(f'Func output: {output}')</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>Executing the above <span class="bold">Python</span> code snippet generates the following typical output:</p>
    </div>
    <br/>
    <div id="out-div">
      <h4>Output.16</h4>
      <pre>Ready to call Func: execute_command with Args: {'command': 'docker --version'}
Executing the command: docker --version
Func output: Docker version 27.5.1, build 27.5.1-0ubuntu3~24.04.2</pre>
    </div>
    <br/>
    <div id="para-div">
      <p>With this, we conclude the various demonstrations on using the <span class="bold">Ollama</span> platform for running and
        working with the pre-trained LLM models locally !!!</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://ollama.com/" target="_blank"><span class="bold">Ollama</span></a></p>
      <p><a href="https://github.com/ollama/ollama/blob/main/docs/api.md" target="_blank"><span class="bold">Ollama API</span></a></p>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
