<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - K-Means Clustering using Scikit-Learn">
    <meta name="subject" content="Machine Learning - K-Means Clustering using Scikit-Learn">
    <meta name="keywords" content="python, machine_learning, clustering, k-means, scikit-learn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - K-Means Clustering using Scikit-Learn</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - K-Means Clustering using Scikit-Learn</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">07/24/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Clustering</span> is an unsupervised machine learning technique that attempts to group the samples
        from the unlabeled data set into meaningful clusters.</p>
      <p>In other words, the clustering algorithm divides the unlabeled data population into groups, such that the samples within a
        group are similar to each other.</p>
    </div>
    <div id="section-div">
      <p>K-Means Clustering Algorithm</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will unravel the steps behind the K-Means Clustering algorithm using a very simple data set
        related to Math and Logic scores.</p>
    </div>
    <div id="para-div">
      <p>The following illustration displays rows from the simple data set:</p>
    </div>
    <div id="img-outer-div"> <img alt="Data Set" src="./images/kmeans-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the plot of the samples from the data set:</p>
    </div>
    <div id="img-outer-div"> <img alt="Initial Plot" src="./images/kmeans-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Before we get started, the following are some of terminology referenced in this article:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span class="hi-yellow">K</span> :: the number of clusters</p>
        </li>
        <li>
          <p><span class="hi-yellow">Centroid</span> :: the average of the samples in a set.</p>
          <p>For example, if we have the following three vectors $[4, 5]$, $[6, 7]$, and $[5, 6]$ in a two-dimensional space, then
            the centroid would be computed as the average of the corresponding elements $\Large{[\frac{4+6+5}{3},\frac{5+7+6}{3}]}$
            $= [5, 6]$.</p>
          <p>Similarly, if we have the following three vectors $[4, 5, 6]$, $[6, 7, 8]$, and $[5, 6, 7]$ in a three-dimensional space,
            then the centroid would be computed as $\Large{[\frac{4+6+5}{3},\frac{5+7+6}{3},\frac{6+8+7}{3}]}$ $= [5, 6, 7]$, and so
            on</p>
        </li>
        <li>
          <p><span class="hi-yellow">Distance</span> :: the distance between two vectors.</p>
          <p>For example, for the two vectors $[4, 5]$ and $[6, 7]$ in a two-dimensional space, the L1 distance (or Manhattan distance)
            is computed as $\lvert 4 - 6 \rvert + \lvert 5 - 7 \rvert = 4$.</p>
          <p>The L2 distance (or Euclidean distance) is computed as $\sqrt{(4 - 6)^2 + (5 - 7)^2} = \sqrt{8} = 2.83$</p>
        </li>
        <li>
          <p><span class="hi-yellow">Inertia</span> :: the sum of squared errors between each of the samples in a cluster.</p>
          <p>For example, in a two-dimensional space, if the two vectors $[4, 5]$ and $[6, 7]$ are in a cluster with centrod $[5, 6]$,
            then the value for Inertia is computed as $[(4 - 5)^2 + (5 - 6)^2] + [(6 - 5)^2 + (7 - 6)^2] = [1 + 1] + [1 + 1] = 4$.</p>
          <p>As the number of clusters $K$ increases, the corresponding value of Inertia will decrease</p>
        </li>
        <li>
          <p><span class="hi-yellow">Distortion</span> :: the mean value of the Inertia.</p>
          <p>For example, in a two-dimensional space, if the two vectors $[4, 5]$ and $[6, 7]$ are in a cluster with centrod $[5, 6]$,
            then the value for Inertia is computed as $[(4 - 5)^2 + (5 - 6)^2] + [(6 - 5)^2 + (7 - 6)^2] = [1 + 1] + [1 + 1] = 4$.
            Given there are $2$ samples, the Distortion is $\Large{\frac{4}{2}}$ $= 2$</p>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>The following are the steps in the K-Means Clustering algorithm:</p>
    </div>
    <div id="para-div">
      <ol id="blue-ol">
        <li>
          <p>Choose the number of clusters $K$. For our simple data set we select $K = 2$</p>
        </li>
        <li>
          <p>Randomly choose $K$ cluster centroids $C_k$ (where $k = 1 ... K$) from the data set. For our data set, let $C_1 = [45,
            55]$ and $C_2 = [70, 70]$.</p>
          <p>The following illustration shows the initial centroids for our simple data set:</p>
          <div id="img-outer-div"> <img alt="Initial Centroids" src="./images/kmeans-03.png" class="img-cls" />
            <div class="img-cap">Figure.3</div>
          </div>
          <br/>
        </li>
        <li>
          <p>For each sample $X_i$ in the data set, assign the cluster it belongs to. To do that, compute the distance to each of the
            centroids $C_k$ (where $k = 1 ... K$) and the find the minimum of the distances. If the distance to centroid $C_c$ (where
            $c \le K$) is the minimum, then assign the sample $X_i$ to the cluster $c$.</p>
          <p>The following illustration shows the computed L1 distances and the cluster assignment for our data set:</p>
          <div id="img-outer-div"> <img alt="Cluster Assignment" src="./images/kmeans-04.png" class="img-cls" />
            <div class="img-cap">Figure.4</div>
          </div>
          <br/>
        </li>
        <li>
          <p>Compute the new cluster centroids $C_k$ (where $k = 1 ... K$) using the assigned samples $X_i$ to each cluster.</p>
          <p>For our data set, the samples $[45, 55]$, $[55, 50]$, and $[50, 60]$ are assigned to cluster $1$. The new cluster centroid
            for cluster $1$ ($C_1$) would be $\Large{[\frac{45+55+50}{3},\frac{55+50+60}{3}]}$ $= [50, 55]$. Similarly, the new cluster
            centroid for cluster $2$ ($C_2$) would be $[75, 70]$.</p>
        </li>
        <li>
          <p>Once again, assign a cluster for each of the samples $X_i$ from the data set.</p>
          <p>The following illustration shows the computed L1 distances and the cluster assignment for our data set:</p>
          <div id="img-outer-div"> <img alt="Cluster Assignment 2" src="./images/kmeans-05.png" class="img-cls" />
            <div class="img-cap">Figure.5</div>
          </div>
          <br/>
        </li>
        <li>
          <p>Continue this process of computing the new cluster centroids and assigning samples from the data set to the appropriate
            clusters until the previous cluster centroids and current cluster centroids are the same.</p>
        </li>
      </ol>
    </div>
    <div id="para-div">
      <p>The following illustration shows the plot of the samples from the data set after the final cluster assignment:</p>
    </div>
    <div id="img-outer-div"> <img alt="Final Plot" src="./images/kmeans-06.png" class="img-cls" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>For data sets that are unlabeled, how does one determine the number of clusters $K$ ???</p>
      <p>This is where the visual <span class="hi-yellow">Elbow Method</span> comes in handy. The Elbow Method is a plot of the
        cluster size $K$ vs the Distortion. From the plot, the idea is to determine the value of $K$, after which the Distortion
        starts to decrease in smaller steps.</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will demonstrate the use of the K-Means model for clustering (using scikit-learn) by leveraging
        the <a href="https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv" target="_blank">
        <span class="bold">Palmer Penguins</span></a> data set.</p>
    </div>
    <div id="para-div">
      <p>The <span class="bold">Palmer Penguins</span> includes samples with the following features for the penguin species near the
        Palmer Station, Antarctica:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-yellow">species</span> - denotes the penguin species (Adelie, Chinstrap, and Gentoo)</p></li>
        <li><p><span class="hi-yellow">island</span> - denotes the island in Palmer Archipelago, Antarctica (Biscoe, Dream, or
          Torgersen)</p></li>
        <li><p><span class="hi-yellow">bill_length_mm</span> - denotes the penguins beak length (millimeters)</p></li>
        <li><p><span class="hi-yellow">bill_depth_mm</span> - denotes the penguins beak depth (millimeters)</p></li>
        <li><p><span class="hi-yellow">flipper_length_mm</span> - denotes the penguins flipper length (millimeters)</p></li>
        <li><p><span class="hi-yellow">body_mass_g</span> - denotes the penguins body mass (grams)</p></li>
        <li><p><span class="hi-yellow">sex</span> - denotes the penguins sex (female, male)</p></li>
        <li><p><span class="hi-yellow">year</span> - denotes the study year (2007, 2008, or 2009)</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>The first step is to import all the necessary Python modules such as, pandas and scikit-learn as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to load the palmer penguins data set into a pandas dataframe, drop the index column, and then display the
        dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>url = 'https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv'
penguins_df = pd.read_csv(url)
penguins_df = penguins_df.drop(penguins_df.columns[0], axis=1)
penguins_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays few rows from the palmer penguins dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Display" src="./images/random-forest-01.png" class="img-cls" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to remediate the missing values from the palmer penguins dataframe. Note that we went through the steps
        to fix the missing values in the article <a href="https://polarsparc.github.io/MachineLearning/RandomForest.html" target=
        "_blank"><span class="bold">Random Forest using Scikit-Learn</span></a>, so we will not repeat it here.</p>
    </div>
    <div id="para-div">
      <p>For each of the missing values, we perform data analysis such as comparing the mean or (mean + std) values for the features
        and apply the following fixes as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>penguins_df = penguins_df.drop([3, 271], axis=0)
penguins_df.loc[[8, 10, 11], 'sex'] = 'female'
penguins_df.at[9, 'sex'] = 'male'
penguins_df.at[47, 'sex'] = 'female'
penguins_df.loc[[178, 218, 256, 268], 'sex'] = 'female'</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display information about the palmer penguins dataframe, such as index and column types, memory usage,
        etc., as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>penguins_df.info()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays information about the palmer penguins dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Information" src="./images/adaboost-07.png" class="img-cls" />
      <div class="img-cap">Figure.8</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Note that there are no missing values at this point in the palmer penguins dataframe.</p>
    </div>
    <div id="para-div">
      <p>The next step is to drop the target variable species from the cleansed palmer penguins dataframe to make it an unlabeled
        data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>penguins_df = penguins_df.drop('species', axis=1)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to create and display dummy binary variables for the two categorical (nominal) feature variables - island
        and sex from the cleansed palmer penguins dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>nom_features = ['island', 'sex']
nom_encoded_df = pd.get_dummies(penguins_df[nom_features], prefix_sep='.', drop_first=True, sparse=False)
nom_encoded_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the dataframe of all dummy binary variables for the two categorical (nominal) feature
        variables from the cleansed palmer penguins dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dummy Variables" src="./images/random-forest-10.png" class="img-cls" />
      <div class="img-cap">Figure.9</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to drop the two nominal categorical features and merge the dataframe of dummy binary variables into the
        cleansed palmer penguins dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>penguins_df = penguins_df.drop(penguins_df[nom_features], axis=1)
penguins_df = pd.concat([penguins_df, nom_encoded_df], axis=1)
penguins_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays few columns/rows from the merged palmer penguins dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Merged Dataframe" src="./images/random-forest-11.png" class="img-cls" />
      <div class="img-cap">Figure.10</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to initialize the K-Means model class from scikit-learn and train the model using the data set as shown
        below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = KMeans(n_clusters=3, init='random', max_iter=100, random_state=101)
model.fit(penguins_df)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are a brief description of some of the hyperparameters used by the KMeans model:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-yellow">n_clusters</span> - the number of clusters</p></li>
        <li><p><span class="hi-yellow">init</span> - the method to select the initial cluster centroid</p></li>
        <li><p><span class="hi-yellow">max_iter</span> - the maximum number of iterations</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>The next step is to display a scatter plot of the samples from the palmer penguins data set segregated into the 3 clusters
        as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>sns.scatterplot(x=penguins_df['bill_length_mm'], y=penguins_df['body_mass_g'], hue=model.labels_, palette='tab10')
plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the scatter plot of the 3 clusters:</p>
    </div>
    <div id="img-outer-div"> <img alt="Identified Clusters" src="./images/kmeans-07.png" class="img-cls" />
      <div class="img-cap">Figure.11</div>
    </div>
    <br/>
    <div id="section-div">
      <p>Demo Notebook</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this
        article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P20-KMeans-Scikit.ipynb" target="_blank">
          <span class="bold">K-Means Clustering</span></a></p></li>
      </ul>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
