<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - K Nearest Neighbors using Scikit-Learn">
    <meta name="subject" content="Machine Learning - K Nearest Neighbors using Scikit-Learn">
    <meta name="keywords" content="python, machine_learning, classification, scikit-learn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - K Nearest Neighbors using Scikit-Learn</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - K Nearest Neighbors using Scikit-Learn</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">06/17/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">K Nearest Neighbors</span> (or <span class="hi-yellow">KNN</span> for short) is one of the simplest
        algorithms for classification. It classifies a new incoming target data based on the distance to the old training data.</p>
    </div>
    <div id="section-div">
      <p>K Nearest Neighbors</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will develop an intuition for K Nearest Neighbors through visual plots.</p>
    </div>
    <div id="para-div">
      <p>Assume there are two categories (or labels) of entities that have been classified from a data set - a set of red diamonds
        and a set of blue squares as depicted in the illustration below:</p>
    </div>
    <div id="img-outer-div"> <img alt="Red and Blue" src="./images/knn-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Soon we encounter a new entity (data point) - identified by the green circle as depicted in the illustration below:</p>
    </div>
    <div id="img-outer-div"> <img alt="New Green" src="./images/knn-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>How do we label the new green data point as - is it a red diamond ? Or, a blue square ?</p>
      <p>What KNN does is that it computes the distance of the new data point (green circle) to all other training data points. It
        then selects the K nearest data points, where K is typically an odd integer, such as, 1, 3, 5, etc. Finally it assigns the
        new data point to the category (or label) to which the majority of the K data points belong to.</p>
    </div>
    <div id="para-div">
      <p>Assuming we select K to be 3. Then, the 3 entities that are nearest to the green circle are as depicted in the illustration
        below:</p>
    </div>
    <div id="img-outer-div"> <img alt="Closest Points" src="./images/knn-03.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>There are 2 red diamonds and a blue square that are nearest to the green circle. Given that the majority is red diamonds,
        we classify the green circle as the red diamond.</p>
    </div>
    <div id="para-div">
      <p>As stated above, the KNN algorithm uses distance as a measure of closeness to existing data points (from the training set)
        to determine the class label. The following are some of the commonly used distance algorithms:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span class="hi-blue">Manhattan Distance</span> - Given the two vectors $A = (x_1, y_1)$ and $B = (x_2, y_2)$, the
            Manhattan Distance between the two vectors $A$ and $B$ is computed as the sum of the absolute differences between the
            two corresponding elements of the vectors.</p>
          <p>In other words, the Manhattan Distance $D$ is computed as follows:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$D = \lvert x_1 - x_2 \rvert + \lvert y_1 - y_2 \rvert$</p>
          <p>In general terms:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$D = \sum_{i=1}^n \lvert A_i - B_i \rvert$</p>
        </li>
        <li>
          <p><span class="hi-blue">Euclidean Distance</span> - Given the two vectors $A = (x_1, y_1)$ and $B = (x_2, y_2)$, the
            Euclidean Distance between the two vectors $A$ and $B$ is computed as the square root of the sum of the squares of the
            differences between the two corresponding elements of the vectors.</p>
          <p>In other words, the Euclidean Distance $D$ is computed as follows:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$D = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$</p>
          <p>In general terms:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$D = \sqrt{\sum_{i=1}^n (A_i - B_i)^2}$</p>
        </li>
        <li>
          <p><span class="hi-blue">Minkowski Distance</span> - Given the two vectors $A = (x_1, y_1)$ and $B = (x_2, y_2)$, the
            Minkowski Distance $D$ between the two vectors $A$ and $B$ is computed as follows:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$D = [(x_1 - x_2)^p + (y_1 - y_2)^p]^{1/p}$</p>
          <p>In general terms:</p>
          <p>&nbsp;&nbsp;&nbsp;&nbsp;$D = [\sum_{i=1}^n (A_i - B_i)^p]^{1/p}$</p>
          <p>Notice that the Minowski Distance is a generalization of the Manhattan Distance ($p = 1$) and Euclidean Distance ($p =
            2$)</p>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>K Nearest Neighbors (KNN) algorithm uses lazy learning in that it only stores the training samples along with their classes
        (categories), with little or no processing. When a new test data point is provided to the model for classification, it uses
        a majority vote of the K-nearest points to determine the corresponding class (or category).</p>
    </div>
    <div id="para-div">
      <p>In this following sections, we will demonstrate the use of the KNN model for classification (using scikit-learn) by
        leveraging the <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data" target="_blank">
        <span class="bold">Glass Identification</span></a> data set.</p>
    </div>
    <div id="para-div">
      <p>The first step is to import all the necessary Python modules such as, pandas, matplotlib, seaborn, and scikit-learn as shown
        below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to load the glass identification data set into a pandas dataframe, set the column names, and then display
        the dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'
glass_df = pd.read_csv(url, header=None)
glass_df = glass_df.drop(glass_df.columns[0], axis=1)
glass_df.columns = ['r_index', 'sodium', 'magnesium', 'aluminum', 'silicon', 'potassium', 'calcium', 'barium', 'iron', 'glass_type']
glass_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays few rows from the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Display" src="./images/naive-bayes-01.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display information about the glass identification dataframe, such as index and column types, missing
        (null) values, memory usage, etc., as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>glass_df.info()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays information about the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Information" src="./images/naive-bayes-02.png" class="img-cls" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Fortunately, the data seems clean with no missing values.</p>
    </div>
    <div id="para-div">
      <p>The next step is to split the glass identification dataframe into two parts - a training data set and a test data set. The
        training data set is used to train the classification model and the test data set is used to evaluate the classification model.
        In this use case, we split 75% of the samples into the training dataset and remaining 25% into the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_train, X_test, y_train, y_test = train_test_split(glass_df, glass_df['glass_type'], test_size=0.25, random_state=101)
X_train = X_train.drop('glass_type', axis=1)
X_test = X_test.drop('glass_type', axis=1)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The next step is to create an instance of the standardization scaler to scale the desired feature (or predictor) variables
        in both the training and test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>scaler = StandardScaler()
s_X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
s_X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns, index=X_test.index)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>It is very <span class="underbold">IMPORTANT</span> to scale the feature (or predictor) variables since KNN uses distance
        to determine the target class (category).</p>
    </div>
    <div id="para-div">
      <p>The next step is to initialize the KNN model class from scikit-learn with 3 neighbors and train the model using the training
        data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = KNeighborsClassifier(n_neighbors=3)
model.fit(s_X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the trained model to predict the glass_type using the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model.predict(s_X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display the accuracy score for the model performance as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>accuracy_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the accuracy score for the model performance:</p>
    </div>
    <div id="img-outer-div"> <img alt="Accuracy Score" src="./images/knn-04.png" class="img-cls" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above, one can infer that the model seems to predict okay. Can we improve the accuracy score ???</p>
    </div>
    <div id="para-div">
      <p>One of the hyperparameters used by the K Nearest Neighbors is <span class="hi-blue">n_neighbors</span>, which controls the
        number of neighbors.</p>
      <p>The next step is to perform an extensive grid search on a list of hyperparameter values to determine the optimal value of
        the hyperparameter <span class="bold">n_neighbors</span> and display the optimal value as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>parameters = {
  'n_neighbors': range(1, 26, 2)
}
cv_model = GridSearchCV(estimator=model, param_grid=parameters, cv=5, verbose=1, scoring='accuracy')
cv_model.fit(s_X_train, y_train)
cv_model.best_params_</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the optimal value for the hyperparameter <span class="bold">n_neighbors</span>:</p>
    </div>
    <div id="img-outer-div"> <img alt="Best Estimate" src="./images/knn-05.png" class="img-cls" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to re-initialize the K Nearest Neighbors model with the hyperparameter n_neighbors set to the value of
        <span class="hi-green">1</span> and re-train the model using the training data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = KNeighborsClassifier(n_neighbors=1)
model.fit(s_X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the trained model to predict the glass_type using the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model.predict(s_X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The final step is to display the accuracy score for the model performance as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>accuracy_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the accuracy score for the model performance:</p>
    </div>
    <div id="img-outer-div"> <img alt="Accuracy Score" src="./images/knn-06.png" class="img-cls" />
      <div class="img-cap">Figure.8</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above, one can infer that the model seems to predict much better now.</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this
        article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P14-KNN-Scikit.ipynb" target="_blank">
          <span class="bold">K Nearest Neighbors</span></a></p></li>
      </ul>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
