<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Linear Regression - Part 1">
    <meta name="subject" content="Machine Learning - Linear Regression - Part 1">
    <meta name="keywords" content="python, machine_learning, regression">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Linear Regression - Part 1</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Linear Regression - Part 1</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">03/19/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br />
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Regression</span> is a statistical process, which attempts to determine the relationship between a
        dependent outcome (or a response) variable, typically denoted using the letter $y$, and one or more independent feature
        variables, typically denoted using letters $x_1, x_2, ..., x_n$, etc.</p>
      <p><span class="hi-yellow">Linear Regression</span> is applicable when the <span class="hi-vanila">Dependent</span> Outcome
        (sometimes called a Response or a Target) variable is a <span class="underbold">CONTINUOUS</span> variable and has a linear
        relationship with one or more <span class="hi-vanila">Independent</span> Feature (or Predictor) variable(s).</p>
      <p>In general, one could express the relationship between the dependent outcome variable and the independent feature variables
        in mathematical terms as $y = \beta_0 + \beta_1.x_1 + ... + \beta_n.x_n$, where coefficients $\beta_1, \beta_2, ..., \beta_n$
        are referred to as the <span class="hi-yellow">Regression Coefficients</span> (or the weights or the parameters) associated
        with the corresponding independent variables $x_1, x_2, ..., x_n$ and $\beta_0$ is a constant.</p>
      <p>In other words, $y = \beta_0 + \sum_{i=1}^n \beta_i.x_i$</p>
      <p>The process of determining the coefficients (or the weights or the parameters) of the linear regression equation is often
        referred to as the <span class="hi-yellow">Ordinary Least Squares</span> (or <span class="hi-blue">OLS</span>).</p>
    </div>
    <div id="section-div">
      <p>Simple Linear Regression</p>
    </div>
    <div id="para-div">
      <p>In the case of <span class="hi-yellow">Simple Linear Regression</span>, there is <span class="underbold">ONE</span>
        dependent outcome variable, that has a relationship with <span class="underbold">ONE</span> independent feature variable.</p>
      <p>To understand simple linear regression, let us consider the simple case of predicting (or estimating) a dependent outcome
        variable $y$ using a single independent feature variable $x$. For every value of the independent feature variable $x$, there
        is a corresponding outcome value $y$. That is, we will have a set of pairs $[(x_0, y_0), (x_1, y_1), ..., (x_n, y_n)]$.</p>
      <p>In other words, one could estimate the dependent outcome value using the model $\hat{y} = \beta_0 + \beta_1.x$. Notice that
        this equation is similar to that of a line $y = m.x + b$, where $m$ is the <span class="bold">Slope</span> of the line and
        $b$ is the <span class="bold">Intercept</span> of the line. Hence, the model for simple linear regression is often times
        referred to as the <span class="hi-yellow">Line of Best Fit</span>.</p>
    </div>
    <div id="para-div">
      <p>Consider the following plot that illustrates the line of best fit, the actual outcome values (red dots), and the estimated
        (or predicted) outcome values (blue dots):</p>
    </div>
    <div id="img-outer-div"> <img alt="Line of Best Fit" src="./images/regression-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>For the line of best fit, the idea is to minimize the sum of all the residual distances (or the errors) between the actual
        values of the outcome variable (red dots) $y_i$ compared to their predicted values (blue dots) $\hat{y_i}$.</p>
      <p>In mathematical terms, the sum of all the residual errors can be represented as $E = \sum_{i=1}^n (y_i - \hat{y_i})$.
        However, since some residuals will be <span class="bold">POSITIVE</span> and others <span class="bold">NEGATIVE</span>,
        the result of their sum will end up cancelling each other and we end up with $E = 0$, which is not correct. To avoid this,
        we need to sum the <span class="underbold">SQUARE</span> of the residual errors. In other words, $E = \sum_{i=1}^n (y_i -
        \hat{y_i})^2$. This equation is often referred to as the <span class="hi-yellow">Sum of Squared Errors</span> (or <span
        class="hi-blue">SSE</span>) or <span class="hi-yellow">Residual Sum of Squares</span> (or <span class="hi-blue">RSS</span>).</p>
      <p>For the optimal line of best fit, we need to minimize the <span class="hi-yellow">Error Function</span> (or the
        <span class="hi-yellow">Cost Function</span>) $E = \sum_{i=1}^n (y_i - \hat{y_i})^2 = \sum_{i=1}^n (y_i - (\beta_0 +
        \beta_1.x_i))^2$.</p>
    </div>
    <div id="para-div">
      <p>We know the values for $x_i$ and $y_i$, but have two unknown variables $\beta_0$ and $\beta_1$.</p>
      <p>In order to <span class="underbold">MINIMIZE</span> the error $E$, we need to take the partial derivatives of the error
        function with respect to the two unknown variables $\beta_0$ and $\beta_1$ and set their result to zero.</p>
      <p>In other words, we need to solve for $\Large{\frac{\partial{E}}{\partial{\beta_0}}}$ $= 0$ and $\Large{\frac{\partial{E}}
        {\partial{\beta_1}}}$ $= 0$.</p>
    </div>
    <div id="para-div">
      <p>First, $\Large{\frac{\partial{E}}{\partial{\beta_0}}}$ $= \sum_{i=1}^n 2(y_i - (\beta_0 + \beta_1.x_i)) (-1)$</p>
      <p>On simplification, we get, $- \sum_{i=1}^n y_i + \beta_0 \sum_{i=1}^n 1 + \beta_1 \sum_{i=1}^n x_i = 0$</p>
      <p>Or, $\sum_{i=1}^n y_i = \beta_0 n + \beta_1 \sum_{i=1}^n x_i$ ..... $\color{red} (1)$</p>
    </div>
    <div id="para-div">
      <p>Second, $\Large{\frac{\partial{E}}{\partial{\beta_1}}}$ $= \sum_{i=1}^n 2(y_i - (\beta_0 + \beta_1.x_i)) (-x_i)$</p>
      <p>On simplification, we get, $- \sum_{i=1}^n x_i.y_i + \beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2 = 0$</p>
      <p>Or, $\sum_{i=1}^n x_i.y_i = \beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2$ ..... $\color{red} (2)$</p>
    </div>
    <div id="para-div">
      <p>Let $A = \sum_{i=1}^n x_i^2$, $B = \sum_{i=1}^n x_i$, $C = \sum_{i=1}^n x_i.y_i$, and $D = \sum_{i=1}^n y_i$</p>
      <p>Therefore, equations $\color{red} (1)$ and $\color{red} (2)$ can be rewritten as follows:</p>
      <p>$D = \beta_0 n + \beta_1 B$ ..... $\color{red} (1)$</p>
      <p>$C = \beta_0 B + \beta_1 A$ ..... $\color{red} (2)$</p>
      <p>Solving equations $\color{red} (1)$ and $\color{red} (2)$, we get the following:</p>
      <p>$\beta_0 = \Large{\frac{(CB - DA)}{(B^2 - nA)}}$ $= \Large{\frac{(DA - CB)}{(nA - B^2)}}$ $= \Large{\frac{\sum y_i \sum x_i^2
        - \sum x_i \sum x_i.y_i}{n \sum x_i^2 - (\sum x_i)^2}}$ ..... $\color{red} (3)$</p>
      <p>$\beta_1 = \Large{\frac{(nC - DB)}{(nA - B^2)}}$ $= \Large{\frac{n \sum x_i.y_i - \sum x_i \sum y_i}{n \sum x_i^2 -
        (\sum x_i)^2}}$ ..... $\color{red} (4)$</p>
      <p>We know the following:</p>
      <p>$\bar{x} =$ $\Large{\frac{1}{n}}$ $\sum_{i=1}^n x_i$</p>
      <p>$\bar{y} =$ $\Large{\frac{1}{n}}$ $\sum_{i=1}^n y_i$</p>
      <p>$\overline{xy} =$ $\Large{\frac{1}{n}}$ $\sum_{i=1}^n x_i.y_i$</p>
      <p>Diving both the equations $\color{red} (3)$ and $\color{red} (4)$ by $n^2$ and simplifying, we will arrive at the following
        results:</p>
      <p>$\color{red} \boldsymbol{\beta_0}$ $= \bbox[pink,2pt]{\Large{\frac{\bar{y}.\bar{x^2} - \bar{x}.\overline{xy}}{\bar{x^2} - \bar{x}^2}}}$</p>
      <p>$\color{red} \boldsymbol{\beta_1}$ $= \bbox[pink,2pt]{\Large{\frac{\overline{xy} - \bar{x}.\bar{y}}{\bar{x^2} - \bar{x}^2}}}$</p>
    </div>
    <div id="para-div">
      <p>Let us use the <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/" target="_blank"><span class="bold">
        Auto MPG</span></a> dataset to demonstrate the relationship between the <span class="hi-vanila">Horsepower</span> and the
        <span class="hi-vanila">MPG</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the first 5 rows of the auto mpg dataset:</p>
    </div>
    <div id="img-outer-div"> <img alt="First 5 Rows of Auto Dataset" src="./images/regression-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the summary statistics of the auto horsepower feature variable):</p>
    </div>
    <div id="img-outer-div"> <img alt="Horsepower Summary Statistics" src="./images/regression-03.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the values of $\beta_0$ and $\beta_1$ computed using the equations derived above:</p>
    </div>
    <div id="img-outer-div"> <img alt="Two Coefficients" src="./images/regression-04.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the scatter plot between the auto horsepower feature variable and the auto mpg outcome
        variable, along with the line of best fit using the derivation for $\beta_0$ and $\beta_1$:</p>
    </div>
    <div id="img-outer-div"> <img alt="Horsepower vs MPG Plot" src="./images/regression-05.png" class="img-cls" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="section-div">
      <p>Multiple Linear Regression</p>
    </div>
    <div id="para-div">
      <p>In the case of <span class="hi-yellow">Multiple Linear Regression</span>, there is <span class="underbold">ONE</span>
        dependent outcome variable that has relationships with <span class="underbold">MORE</span> than <span class="bold">ONE</span>
        independent feature variables.</p>
      <p>To understand multiple linear regression, let us consider the case of predicting (or estimating) a dependent variable $y$
        using $n$ independent feature variables $x_1, x_2, ..., x_n$. In mathematical terms, one could estimate the dependent outcome
        value using the linear model $\hat{y} = \beta_0 + \beta_1.x_1 + \beta_2.x_2 + ... + \beta_n.x_n$.</p>
      <p>Using the matrix notation, one could write the above linear model as: $\hat{y} = \beta_0 + \begin{bmatrix} \beta_1 & \beta_2
        & ... & \beta_n \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_n \end{bmatrix}$</p>
      <p>That is, $\hat{y} = \begin{bmatrix} \beta_0 & \beta_1 & ... & \beta_n \end{bmatrix} \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ ...
        \\ x_n \end{bmatrix}$.</p>
    </div>
    <div id="para-div">
      <p>Now, what about extending the above linear model to predict $m$ dependent output values ???</p>
      <p>In other words, $y_m = \begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_m \end{bmatrix}$</p>
      <p>Then, $\begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_m \end{bmatrix} = \begin{bmatrix} 1 & x_{1,1} & x_{1,2} & ... & x_{1,n} \\
        1 & x_{2,1} & x_{2,2} & ... & x_{2,n} \\ ... & ... & ... & ... & ... \\ 1 & x_{m,1} & x_{m,2} & ... & x_{m,n} \end{bmatrix}
        \begin{bmatrix} \beta_0 \\ \beta_1 \\ ... \\ \beta_n \end{bmatrix}$</p>
      <p>Once again, using the matrix notation, we arrive at $\hat{y} = X\beta$, where $X$ is a matrix. Note that $\beta$ has to
        be to the right of $X$ in order to apply the coefficients (or weights) appropriately to each row (one set of feature values)
        of $X$.</p>
    </div>
    <div id="para-div">
      <p>Given that we desire to predict $m$ dependent output values, for the optimal line of best fit, we need to minimize $E =
        \sum_{i=1}^m (y_i - \hat{y_i})^2$.</p>
      <p>Using the matrix notation, $E = \begin{bmatrix} (y_1-\hat{y_1}) & (y_2-\hat{y_2}) & ... & (y_m-\hat{y_m}) \end{bmatrix}
        \begin{bmatrix} (y_1-\hat{y_1}) \\ (y_2-\hat{y_2}) \\ ... \\ (y_m-\hat{y_m}) \end{bmatrix}$</p>
      <p>If $y = \begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_m \end{bmatrix}$ and $\hat{y} = \begin{bmatrix} \hat{y_1} \\ \hat{y_2} \\
        ... \\ \hat{y_m} \end{bmatrix}$, then $\begin{bmatrix} (y_1-\hat{y_1}) \\ (y_2-\hat{y_2}) \\ ... \\ (y_m-\hat{y_m})
        \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_m \end{bmatrix} - \begin{bmatrix} \hat{y_1} \\ \hat{y_2} \\ ... \\
        \hat{y_m} \end{bmatrix} = y - \hat{y}$</p>
      <p>Therefore, $E = (y - \hat{y})^T(y - \hat{y}) = (y^T - \hat{y}^T)(y - \hat{y}) = (y^T - (X\beta)^T)(y - X\beta) = (y^T -
        \beta^TX^T)(y - X\beta)$.</p>
      <p>That is, $E = y^Ty - \beta^TX^Ty - y^TX\beta + \beta^TX^TX\beta$.</p>
    </div>
    <br/>
    <div id="warn-div">
      <h4>*** Matrix Transpose Rule ***</h4>
      <p>&nbsp;&nbsp;$(X\beta)^T = \beta^TX^T$</p>
    </div>
    <br/>
    <div id="para-div">
      <p>We know $(y^TX\beta)^T = \beta^TX^Ty$ and $y^TX\beta$ is a scalar. This implies that the transpose of the scalar is the
        scalar itself. Hence, $y^TX\beta$ can be substituted with $\beta^TX^Ty$.</p>
      <p>Therefore, $E = y^Ty - 2\beta^TX^Ty + \beta^TX^TX\beta$.</p>
    </div>
    <div id="para-div">
      <p>In order to <span class="underbold">MINIMIZE</span> the error $E$, we need to take the partial derivatives of the error
        function with respect to $\beta$ and set the result to zero.</p>
      <p>In other words, we need to solve for $\Large{\frac{\partial{E}}{\partial{\beta}}}$ $= 0$.</p>
    </div>
    <br/>
    <div id="warn-div">
      <h4>*** Matrix Calculus Rules ***</h4>
      <p>&nbsp;&nbsp;1. $\Large{\frac{\partial}{\partial{\beta}}}$ $y^Ty = 0$</p>
      <p>&nbsp;&nbsp;2. $\Large{\frac{\partial}{\partial{\beta}}}$ $\beta^TX^Ty = X^Ty$</p>
      <p>&nbsp;&nbsp;3. $\Large{\frac{\partial}{\partial{\beta}}}$ $y^TX\beta = y^TX$</p>
      <p>&nbsp;&nbsp;4. $\Large{\frac{\partial}{\partial{\beta}}}$ $\beta^TX^TX\beta = 2X^TX\beta$</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Therefore, $\Large{\frac{\partial{E}}{\partial{\beta}}}$ $= 0 - 2X^Ty + 2X^TX\beta$</p>
      <p>That is, $\Large{\frac{\partial{E}}{\partial{\beta}}}$ $= - 2X^Ty + 2X^TX\beta$</p>
      <p>To minimize the error, $\Large{\frac{\partial{E}}{\partial{\beta}}}$ $= 0$</p>
      <p>That is, $- 2X^Ty + 2X^TX\beta = 0$</p>
      <p>Or, $X^TX\beta = X^Ty$</p>
      <p>To solve for $\beta$, $(X^TX)^{-1}X^TX\beta = (X^TX)^{-1}X^Ty$</p>
      <p>Simplifying, we get the following:</p>
      <p>$\color{red} \boldsymbol{\beta}$ $= \bbox[pink,2pt]{(X^TX)^{-1}X^Ty}$</p>
    </div>
    <div id="para-div">
      <p>Once again, we use the <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/" target="_blank"><span
        class="bold">Auto MPG</span></a> dataset to demonstrate the relationship between the dependent response variable <span
        class="hi-vanila">MPG</span> and the three feature variables - <span class="hi-vanila">Horsepower</span>,
        <span class="hi-vanila">Displacement</span>, and <span class="hi-vanila">Weight</span> respectively.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the first 5 rows of the feature variable matrix:</p>
    </div>
    <div id="img-outer-div"> <img alt="First 5 Rows of Matrix" src="./images/regression-06.png" class="img-cls" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the $\beta$ values computed using the equation derived above:</p>
    </div>
    <div id="img-outer-div"> <img alt="Beta Coefficients" src="./images/regression-07.png" class="img-cls" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="section-div">
      <p>Model Evaluation Metrics</p>
    </div>
    <div id="para-div">
      <p>In the following paragraphs we will look at the different metrics that help us evaluate the effectiveness of the linear
        regression model.</p>
    </div>
    <div id="step-div">
      <p>Mean Squared Error</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Mean Squared Error</span> (or <span class="hi-blue">MSE</span>) is a metric that measures the
        amount of error from the regression model.</p>
      <p>In mathematical terms,</p>
      <p>$\color{red} \boldsymbol{MSE}$ $= \bbox[pink,2pt]{\Large{\frac{SSE}{n}}}$</p>
      <p>where</p>
      <p>$SSE = \sum_{i=1}^n (y_i - \hat{y_i})^2$ is called <span class="hi-vanila">Sum of Squared Errors</span> and represents
        the variability from the actual expected value.</p>
    </div>
    <div id="step-div">
      <p>Root Mean Squared Error</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Root Mean Squared Error</span> (or <span class="hi-blue">RMSE</span>) is a metric that represents
        the standard deviation of residuals from the regression model.</p>
      <p>In mathematical terms,</p>
      <p>$\color{red} \boldsymbol{RMSE}$ $= \bbox[pink,2pt]{\sqrt{MSE}}$</p>
    </div>
    <div id="step-div">
      <p>R-Squared</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">R-Square</span> is a metric (or measure) that evaluates the goodness (or accuracy) of prediction
        by a regression model. It is often referred to as the <span class="hi-vanila">Coefficient of Determination</span> and is  
        denoted using the symbol $R^2$.</p>
      <p>In other words, $R^2$ represents the proportion of variance of the dependent outcome variable that is influenced (or
        explained) by the independent feature variable(s) in the regression model.</p>
      <p>The $R^2$ value ranges from $0$ to $1$. For example, if $R^2$ is $0.85$, then it indicates that $85%$ of the variation in
        the response variable is explained by the predictor variables.</p>
      <p>In mathematical terms,</p>
      <p>$\color{red} \boldsymbol{R^2}$ $= \bbox[pink,2pt]{1 - \Large{\frac{SSE}{SST}}}$</p>
      <p>where</p>
      <p>$SST = \sum_{i=1}^n (y_i - \bar{y})^2$ is called <span class="hi-vanila">Sum of Squared Total</span> and represents the
        total variability from the mean</p>
      <p>$SSE = \sum_{i=1}^n (y_i - \hat{y_i})^2$ is called <span class="hi-vanila">Sum of Squared Errors</span> and represents the
        variability from the actual expected value.</p>
      <p>For a perfectly fit linear regression model, $SSE = 0$ and $R^2 = 1$.</p>
    </div>
    <div id="step-div">
      <p>Adjusted R-Squared</p>
    </div>
    <div id="para-div">
      <p>When additional independent feature (or predictor) variables that have no influence on the dependent outcome (or response)
        are added to the regression model, the value of $R^2$ tends to increase and appears to indicate the model is a better fit,
        when in reality it is <span class="underbold">NOT</span> and misleading.</p>
      <p>The <span class="hi-yellow">Adjusted R-Square</span>, denoted by $\bar{R}^2$ (or $R_{adj}^2$), addresses the issue by taking
        into account the number of independent feature variables in the model.</p>
      <p>If $p$ is the number independent feature variables, then:</p>
      <p>$\color{red} \boldsymbol{\bar{R}^2}$ $= \bbox[pink,2pt]{1 - \Large{\frac{(1 - R^2)(n - 1)}{n - p - 1}}}$</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>The following are the links to the <span class="bold">Jupyter Notebooks</span> that provides an hands-on demo for this
        article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P1-1-Simple-LR.ipynb" target="_blank"><span class="bold">Simple Linear
          Regression</span></a></p></li>
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P1-2-Multiple-LR.ipynb" target="_blank"><span class="bold">Multiple Linear
          Regression</span></a></p></li>
      </ul>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-2.html" target="_blank"><span class="bold">Introduction to Linear Algebra - Part 2</span></a></p>
      <p><a href="https://polarsparc.github.io/Mathematics/Calculus-2.html" target="_blank"><span class="bold">Introduction to Calculus - Part 2</span></a></p>
    </div>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
