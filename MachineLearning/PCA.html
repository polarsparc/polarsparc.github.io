<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Principal Component Analysis using Scikit-Learn">
    <meta name="subject" content="Machine Learning - Principal Component Analysis using Scikit-Learn">
    <meta name="keywords" content="python, machine_learning, dimensionality_reduction, pca, scikit-learn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Principal Component Analysis using Scikit-Learn</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Principal Component Analysis using Scikit-Learn</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">07/29/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p>The number of feature variables in a data set used for predicting the target is referred to as the <span class="hi-yellow">
        Dimensionality</span> of the data set.</p>
      <p>As the number of feature variables in a data set increases, it has some interesting implications, which are as follows:</p>
      <ul id="blue-sqr-ul">
        <li><p>Need more samples in the data set for better training and testing</p></li>
        <li><p>Increased complexity of the model that could lead to model overfitting</p></li>
        <li><p>Hard to explain the model behavior</p></li>
        <li><p>Increased storage and compute complexity</p></li>
      </ul>
      <p>The task of reducing the number of feature variables from a data set is referred to as <span class="hi-yellow">Dimensionality
        Reduction</span>. One of the popular techniques for Dimensionality Reduction is the <span class="hi-yellow">Principal Component
        Analysis</span> (or <span class="hi-blue">PCA</span> for short).</p>
    </div>
    <div id="section-div">
      <p>Principal Component Analysis</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will unravel the idea behind Principal Component Analysis (or PCA) from a geometric point of
        view for better intuition and understanding.</p>
      <p>Let us assume a simple hypothetical data set with two feature variables F1 and F2. The following illustration shows the plot
        of the two features in a two-dimensional space:</p>
    </div>
    <div id="img-outer-div"> <img alt="Two Features" src="./images/pca-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Looking at the plot, one can infer that there is a positive relationship between F1 and F2. One can explain this relationship
        using a vector line. There can be many vector lines in this two-dimensional space - which one do we choose ??? For simplicity,
        let us just consider the two vector lines - vector $v$ (in red) and vector $w$ (in blue) as shown in the illustration below:</p>
    </div>
    <div id="img-outer-div"> <img alt="Vector v" src="./images/pca-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, let us look at one of the points (represented as vector $a$) as shown in the illustration below:</p>
    </div>
    <div id="img-outer-div"> <img alt="Vector a" src="./images/pca-03.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From Linear Algebra, we know the vector $a$ (the point) can be projected onto the vector $v$. The new projected vector
        (dashed blue line on the vector $v$) will be a scaled version of the vector $v$. The following illustration shows the vector
        projection:</p>
    </div>
    <div id="img-outer-div"> <img alt="Vector Projection" src="./images/pca-04.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Similarly, we can project the vector $a$ (the point) onto the vector $w$. If we project all the points onto both the vectors
        $v$ and $w$, we will realize that the variation of the points on vector $v$ is larger (spread out) versus the points on vector
        $w$ (squished). This implies that with the projections onto vector $w$, there will be some information loss (some points will
        overlap). Hence, the idea is to pick the vector line with maximum variance (vector $v$ in our example) so that we can capture
        more information about the two features F1 and F2.</p>
    </div>
    <div id="para-div">
      <p>The projected vector (dashed blue line on the vector $v$) will be a scaled version of the vector $v$. If $\lambda$ is a
        scalar, then the projected vector would be $\lambda.v$. How do we find the value for the scalar $\lambda$ ???</p>
      <p>Let $d$ be the vector from the head of vector $a$ to the projected point on vector $v$.</p>
      <p>Then, $a + d = \lambda.v$</p>
      <p>Or, $d = \lambda.v - a$</p>
      <p>We know the vectors $d$ and $v$ must be orthogonal, meaning the angle between them must be $90^{\circ}$.</p>
      <p>From Linear Algebra, we know the dot product of orthogonal vectors must be zero. That is, $d.v = 0$</p>
      <p>That is, $(\lambda.v - a).v = 0$</p>
      <p>Or, $\lambda.v.v - a.v = 0$</p>
      <p>Rearranging, we get: $\lambda = \Large{\frac{a.v}{v.v}}$</p>
      <p>Once again from Linear Algebra, we know: $\lVert v \rVert = \sqrt{v.v}$</p>
      <p>Therefore, the equation for $\lambda$ can be written as: $\lambda = \Large{\frac{a.v}{{\lVert v \rVert}^2}}$</p>
      <p>Hence, the projected vector $\lambda.v = \Large{\frac{a.v}{{\lVert v \rVert}^2}}$ $.v$</p>
    </div>
    <div id="para-div">
      <p>If we continue to project all the points (or vectors) onto the vector $v$, we in effect will transform the two-dimensional
        coordinate space into a one-dimensional space (a single line). In other words, we have reduced the two features to just one.</p>
      <p>This reduction was possible since there was a clear <span class="underbold">RELATIONSHIP</span> between the two feature
        variables F1 and F2. This in essence is the idea behind Dimensionality Reduction using PCA.</p>
    </div>
    <div id="para-div">
      <p>The following sections we will use a simple example to work through the steps involved in the Principal Component Analysis
        algorithm:</p>
    </div>
    <div id="para-div">
      <ol id="blue-ol">
        <li>
          <p>Consider the following simple data set with 3 feature variables $X_1$, $X_2$, and $X_3$ as shown in the illustration
            below:</p>
          <div id="img-outer-div"> <img alt="Sample Data" src="./images/pca-05.png" class="img-cls" />
            <div class="img-cap">Figure.5</div>
          </div>
          <br/>
        </li>
        <li>
          <p>PCA uses the directional relationship between each of the feature variables (covariance) in the data set. In order to
            compute the covariance, we need to standardize the values of all the feature variables to a common scale. This is achieved
            by replacing each of the values with its corresponding Z-score.</p>
          <p>That is, $x_i = \Large{\frac{x_i - \mu}{\sigma}}$, where $\mu$ is the mean and $\sigma$ is the standard deviation.</p>
          <p>The following illustration shows the standardized data set:</p>
          <div id="img-outer-div"> <img alt="Standardized Data" src="./images/pca-06.png" class="img-cls" />
            <div class="img-cap">Figure.6</div>
          </div>
          <br/>
        </li>
        <li>
          <p>Create a covariance matrix which captures the directinal relationship between each pair of the feature variables from our
            data set.</p>
          <p>The following illustration shows the template for the covariance matrix with respect to our data set:</p>
          <div id="img-outer-div"> <img alt="Covariance Matrix" src="./images/pca-07.png" class="img-cls" />
            <div class="img-cap">Figure.7</div>
          </div>
          <br/>
          <p>That is, $cov(X, Y) = \Large{\Large{\frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{n-1}}}$, where $\bar{x}$ is the mean
            for feature X and $\bar{y}$ is the mean for feature Y, and $n$ is the number of samples.</p>
          <p>The following illustration shows the computed covariance matrix for our data set:</p>
          <div id="img-outer-div"> <img alt="Covariance Computed Matrix" src="./images/pca-08.png" class="img-cls" />
            <div class="img-cap">Figure.8</div>
          </div>
          <br/>
        </li>
        <li>
          <p>Perform an Eigen Decomposition on the covariance matrix from the previous step. For more details on this topic from Linear
            Algebra refer to the article <a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-4.html" target="_blank"><span
              class="bold">Introduction to Linear Algebra - Part 4</span></a>. The idea is to decompose the covariance matrix to a set
              of vectors called the Eigen Vectors (represents the projection of the feature relationships) and an associated set of
              scalars called the Eigen Values (represents the scale or importance of the features). Each of the eigen vectors from the
              decomposition are referred to as the <span class="hi-yellow">Principal Components</span> of the original data set.</p>
          <p>The following illustration shows the Eigen Vectors and Eigen Values for our covariance matrix:</p>
          <div id="img-outer-div"> <img alt="Eigen Decomposition" src="./images/pca-09.png" class="img-cls" />
            <div class="img-cap">Figure.9</div>
          </div>
          <br/>
        </li>
        <li>
          <p>From the Eigen Values, we can conclude that the principal components corresponding to the feature variables $X_2$ and
            $X_3$ are more important in our data set. In other words, the principal components corresponding to the feature variables
            $X_2$ and $X_3$ capture all the information from the data set after dropping the principal component associated to the
            feature variable $X_1$.</p>
        </li>
      </ol>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will demonstrate the use of Principal Component Analysis (using scikit-learn) by leveraging
        the popular <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/" target="_blank"><span class="bold">
        Auto MPG</span></a> dataset.</p>
    </div>
    <div id="para-div">
      <p>The first step is to import all the necessary Python modules such as, pandas and scikit-learn as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to load the auto mpg data set into a pandas dataframe and assign the column names as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
auto_df = pd.read_csv(url, delim_whitespace=True)
auto_df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to fix the data types, drop some unwanted features variables (like model_year, origin, and car_name), and
        drop the rows with missing values from the auto mpg data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>auto_df.horsepower = pd.to_numeric(auto_df.horsepower, errors='coerce')
auto_df.car_name = auto_df.car_name.astype('string')
auto_df = auto_df.drop(['model_year', 'origin', 'car_name'], axis=1)
auto_df = auto_df[auto_df.horsepower.notnull()]
auto_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the columns/rows of the auto mpg dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Rows" src="./images/pca-10.png" class="img-cls" />
      <div class="img-cap">Figure.10</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to create the training dataset and a test dataset with the desired feature (or predictor) variables. We
        create a 75% training dataset and a 25% test dataset with the four feature variables acceleration, cylinders, displacement,
        horsepower, and weight as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_train, X_test, y_train, y_test = train_test_split(auto_df[['acceleration', 'cylinders', 'displacement', 'horsepower', 'weight']], auto_df['mpg'], test_size=0.25, random_state=101)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to create an instance of the standardization scaler to scale the desired feature (or predictor) variables
        in both the training and test dataframes as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>scaler = StandardScaler()
s_X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
s_X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The next step is to perform the PCA in order to extract the important features from the scaled training and test dataframes
        as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5']
pca = PCA(random_state=101)
r_X_train = pd.DataFrame(pca.fit_transform(s_X_train), columns=columns, index=s_X_train.index)
r_X_test = pd.DataFrame(pca.transform(s_X_test), columns=columns, index=s_X_test.index)
r_X_train</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the columns/rows from the decomposed principal components:</p>
    </div>
    <div id="img-outer-div"> <img alt="Principal Components" src="./images/pca-11.png" class="img-cls" />
      <div class="img-cap">Figure.11</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display the variance explained by the principal components as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>pca.explained_variance_ratio_</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the variances by each principal component:</p>
    </div>
    <div id="img-outer-div"> <img alt="Components Ratio" src="./images/pca-12.png" class="img-cls" />
      <div class="img-cap">Figure.12</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Given the first two principal components explain most of the variances from the auto mpg data set, we will use them for
        in training and testing the regression model as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>drop_columns = ['PC3', 'PC4', 'PC5']
r_X_train2 = r_X_train.drop(drop_columns, axis=1)
r_X_test2 = r_X_test.drop(drop_columns, axis=1)
r_X_train2</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to initialize and train the linear regression model using the dimension reduced training dataframe as shown
        below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = LinearRegression(fit_intercept=True)
model.fit(r_X_train2, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the trained linear regression model to predict the target (mpg) using the dimension reduced test
        dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model.predict(r_X_test2)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The final step is to display the $R^2$ value for the linear regression model as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>r2_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the $R^2$ value for the model:</p>
    </div>
    <div id="img-outer-div"> <img alt="Display R-Squared" src="./images/pca-13.png" class="img-cls" />
      <div class="img-cap">Figure.13</div>
    </div>
    <br/>
    <div id="section-div">
      <p>Demo Notebook</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this
        article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P21-PCA-Scikit.ipynb" target="_blank">
          <span class="bold">Principal Component Analysis</span></a></p></li>
      </ul>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-1.html" target="_blank"><span class="bold">Introduction to Linear Algebra - Part 1</span></a></p>
      <p><a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-4.html" target="_blank"><span class="bold">Introduction to Linear Algebra - Part 4</span></a></p>
    </div>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
