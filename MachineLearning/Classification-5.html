<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Decision Trees using Scikit-Learn">
    <meta name="subject" content="Machine Learning - Decision Trees using Scikit-Learn">
    <meta name="keywords" content="python, machine_learning, classification, scikit-learn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Decision Trees using Scikit-Learn</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Decision Trees using Scikit-Learn</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">06/24/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Decision Trees</p>
    </div>
    <div id="para-div">
      <p>Whether we realize it or not, we are constantly making decisions to arrive at desired outcomes. When presented with a lot
        of options, we iteratively ask question(s) to narrow the options till we arrive at the desired outcome. This process forms
        a tree like structure called a <span class="hi-yellow">Decision Tree</span>.</p>
    </div>
    <div id="para-div">
      <p>The following is an illustration of a very simple decision tree:</p>
    </div>
    <div id="img-outer-div"> <img alt="Decision Tree" src="./images/decision-trees-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are some terminology related to decision trees:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span class="bold">Root Node</span> :: the node at the top of the decision tree</p>
        </li>
        <li>
          <p><span class="bold">Decision Node</span> :: the sub-node or child node of the decision tree where a decision is made or
            a condition is evaluated</p>
        </li>
        <li>
          <p><span class="bold">Splitting</span> :: the process of dividing the data at a decision node further into two children
            nodes, based on a condition being true or false</p>
        </li>
        <li>
          <p><span class="bold">Leaf Node</span> :: the terminal node of the decision tree, which cannot be split further, and
            represents a specific class (or category)</p>
        </li>
        <li>
          <p><span class="bold">Depth</span> :: the longest path from the root node to a leaf node</p>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>The following is an illustration of a decision tree with the various node types:</p>
    </div>
    <div id="img-outer-div"> <img alt="Terminology" src="./images/decision-trees-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>In short, a <span class="bold">Decision Tree</span> is a machine learning algorithm, in which data is classified by
        iteratively splitting the data, based on some condition(s) on the feature variable(s) from the data set.</p>
      <p>The Decision Tree machine learning algorithm can be used for either classification or regression problems. Hence, a Decision
        Tree is often referred to by another name called <span class="hi-yellow">Classification and Regression Tree</span> (or
        <span class="hi-blue">CART</span> for short).</p>
      <p>However, in reality, Decision Trees are more often used for solving classification problems.</p>
    </div>
    <div id="para-div">
      <p>Now, the question one may ask - how does the Decision Tree algorithm choose a feature variable and make the determination
        to split the node ??? This is where the <span class="hi-yellow">Gini Impurity</span> comes into play.</p>
    </div>
    <div id="para-div">
      <p>Gini Impurity is mathematically defined as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$G = \sum_{c=1}^N p_c(1 - p_c) = p_1(1 - p_1) + p_2(1 - p_2) + \ldots + p_N(1 - p_N) = (p_1 + p_2 +
        \ldots + p_N) - \sum_{c=1}^N p_c^2 = 1 - \sum_{c=1}^N p_c^2$</p>
      <p>where $N$ is the number of classes (or categories), $p_c$ is the probability of a sample with class (or category) $c$ being
        chosen, and $1 - p_c$ is the probability of mis-classifying a sample.</p>
      <p>Note that $p_1 + p_2 + \ldots + p_N = 1$</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will develop an intuition for Gini Impurity using a simple case of binary classification (two
        classes or categories or labels) - a red diamond and a blue square.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the combination of five classified samples in a node:</p>
    </div>
    <div id="img-outer-div"> <img alt="Classified Nodes" src="./images/decision-trees-03.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>For either <span class="bold">Node 1</span> OR <span class="bold">Node 6</span>, all the samples have been classified into
        one of the two categories and are considered 100% correctly classified.</p>
      <p>Next, in the spectrum, for either <span class="bold">Node 2</span> OR <span class="bold">Node 5</span>, all except one of
        the samples have been correctly classified into one of the two categories and are considered 80% classified. The remaining
        20% are mis-classified.</p>
      <p>Finally, for either <span class="bold">Node 3</span> OR <span class="bold">Node 4</span>, all except two of the samples
        have been correctly classified into one of the two categories and are considered 60% classified. And, the remaining 40% are
        mis-classified.</p>
    </div>
    <div id="para-div">
      <p>Given the above facts, the idea of Gini Impurity is then to minimize the impurity (mis-classification) at each node during
        the data splits.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the combination of five classified samples in a node, along with their Gini Impurity
        (at the bottom):</p>
    </div>
    <div id="img-outer-div"> <img alt="Gini Impurity" src="./images/decision-trees-04.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice that the Gini Impurity has a perfect score of <span class="hi-green">0</span> (zero), when all the samples in a node
        are correctly classified (<span class="bold">Node 1</span> OR <span class="bold">Node 6</span>).</p>
      <p>In other words, the decision tree algorithm performs the data split at a node with the goal of minimizing the Gini Impurity
        score. The algorithm performs the splits iteratively till the Gini Impurity score is zero, at which point the target is
        classified into one of the categories.</p>
      <p>Similarly, to choose the feature variable for the root node, the decision tree algorithm computes the Gini Impurity score
        for all the feature variables and picks the feature variable with the lowest Gini Impurity value.</p>
    </div>
    <div id="para-div">
      <p>The following are some of the advantages of decision trees:</p>
      <ul id="blue-sqr-ul">
        <li><p>Easy to explain, interpret, and visualize</p></li>
        <li><p>Can handle any type of data - be it categorical or numerical</p></li>
        <li><p>No need to perform data normalization or scaling</p></li>
        <li><p>Will automatically pick features that are important</p></li>
      </ul>
      <p>The following are some of the disadvantages of decision trees:</p>
      <ul id="blue-sqr-ul">
        <li><p>Can easily overfit if not controlled</p></li>
        <li><p>Changes in the training data impact the structure of the tree, resulting in instability</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>In this following sections, we will demonstrate the use of the Decision Tree model for classification (using scikit-learn)
        by leveraging the same <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data" target="_blank">
        <span class="bold">Glass Identification</span></a> data set we have been using until now.</p>
    </div>
    <div id="para-div">
      <p>The first step is to import all the necessary Python modules such as, pandas, matplotlib, seaborn, and scikit-learn as shown
        below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to load the glass identification data set into a pandas dataframe, set the column names, and then display
        the dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'
glass_df = pd.read_csv(url, header=None)
glass_df = glass_df.drop(glass_df.columns[0], axis=1)
glass_df.columns = ['r_index', 'sodium', 'magnesium', 'aluminum', 'silicon', 'potassium', 'calcium', 'barium', 'iron', 'glass_type']
glass_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays few rows from the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Display" src="./images/naive-bayes-01.png" class="img-cls" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display information about the glass identification dataframe, such as index and column types, missing
        (null) values, memory usage, etc., as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>glass_df.info()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays information about the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Information" src="./images/naive-bayes-02.png" class="img-cls" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Fortunately, the data seems clean with no missing values.</p>
    </div>
    <div id="para-div">
      <p>The next step is to display the count of each of the target classes (or categories) in the glass identification dataframe
        as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>glass_df['glass_type'].value_counts()</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The following illustration displays the count of each of the target classes from the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Categories Counts" src="./images/decision-trees-05.png" class="img-cls" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice that there are values for <span class="underbold">SIX (6)</span> types of glasses.</p>
    </div>
    <div id="para-div">
      <p>The next step is to split the glass identification dataframe into two parts - a training data set and a test data set. The
        training data set is used to train the classification model and the test data set is used to evaluate the classification model.
        In this use case, we split 75% of the samples into the training dataset and remaining 25% into the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_train, X_test, y_train, y_test = train_test_split(glass_df, glass_df['glass_type'], test_size=0.25, random_state=101)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>With Decision Trees, one does *<span class="underbold">NOT</span>* have to scale the feature (or predictor) variables.</p>
    </div>
    <div id="para-div">
      <p>The next step is to display the correlation matrix of the feature (or predictor) variables with the target variable as
        shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>sns.heatmap(X_train.corr(), annot=True, cmap='coolwarm', fmt='0.2f', linewidth=0.5)
plt.show()</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The following illustration displays the correlation matrix of the feature from the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Correlation Matrix Annotated" src="./images/decision-trees-06.png" class="img-cls" />
      <div class="img-cap">Figure.8</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the correlation matrix above, notice that some of the features (annotated in red) have a strong relation with the
        target variable.</p>
    </div>
    <div id="para-div">
      <p>The next step is to drop the target variable from the training and test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_train = X_train.drop('glass_type', axis=1)
X_test = X_test.drop('glass_type', axis=1)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The next step is to initialize the Decision Tree model class from scikit-learn and train the model using the training data
        set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model1 = DecisionTreeClassifier(random_state=101)
model1.fit(X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the trained model to predict the glass_type using the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model1.predict(X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display the accuracy score for the model performance as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>accuracy_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the accuracy score for the model performance:</p>
    </div>
    <div id="img-outer-div"> <img alt="Accuracy Score" src="./images/decision-trees-07.png" class="img-cls" />
      <div class="img-cap">Figure.9</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above, one can infer that the model seems to predict okay.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the visual representation of the decision tree:</p>
    </div>
    <div id="img-outer-div"> <img alt="Show Decision Tree" src="./images/decision-trees-08.png" class="img-cls" />
      <div class="img-cap">Figure.10</div>
    </div>
    <br/>
    <div id="para-div">
      <p>One of the challenges with a Decision Tree model is that it can overfit using the training model to create a deeply nested
        (large depth) tree.</p>
      <p>One of the hyperparameters used by the Decision Tree classifier is <span class="hi-blue">max_depth</span>, which controls
        the maximum depth of the tree.</p>
    </div>
    <div id="para-div">
      <p>The next step is to re-initialize the Decision Tree class with the hyperparameter max_depth set to the value of
        <span class="hi-green">5</span> and re-train the model using the training data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model2 = DecisionTreeClassifier(max_depth=5, random_state=101)
model2.fit(X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the re-trained model to predict the glass_type using the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model2.predict(X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display the accuracy score for the model performance as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>accuracy_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the accuracy score for the model performance:</p>
    </div>
    <div id="img-outer-div"> <img alt="Accuracy Score" src="./images/decision-trees-09.png" class="img-cls" />
      <div class="img-cap">Figure.11</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above, one can infer that the model seems to predict much better now.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the visual representation of the improved decision tree:</p>
    </div>
    <div id="img-outer-div"> <img alt="Show Decision Tree" src="./images/decision-trees-10.png" class="img-cls" />
      <div class="img-cap">Figure.12</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Let us look at the following simpler visual representation of the decision tree to explain how to interpret the contents
        of a node in the decision tree:</p>
    </div>
    <div id="img-outer-div"> <img alt="Interpret Decision Tree" src="./images/decision-trees-11.png" class="img-cls" />
      <div class="img-cap">Figure.13</div>
    </div>
    <br/>
    <div id="para-div">
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-yellow">Feature Criteria</span> :: represents the decision criteria using a feature variable. In this
          example, the node is testing for <span class="hi-grey">barium &le 0.335</span></p></li>
        <li><p><span class="hi-yellow">Gini Impurity Score</span> :: represents the gini impurity value at the node. In this example,
          the node has a gini impurity score of <span class="hi-grey">0.718</span></p></li>
        <li><p><span class="hi-yellow">Sample Size</span> :: represents the size of the sample (number of rows from the training data
          set). In this example, the node is dealing with a sample size of <span class="hi-grey">160</span></p></li>
        <li><p><span class="hi-yellow">Class Distribution</span> :: represents how many samples fall into the different classes (or
          categories). In this example, the node has classified <span class="hi-grey">49</span> samples into class 1,
          <span class="hi-grey">63</span> samples into class 2, and so on</p></li>
      </ul>
    </div>
    <br/>
    <div id="para-div">
      <p><span class="underbold">IMPORTANT</span> - One may be wondering what was the purpose of the correlation heatmap above
        (Figure.8). While the feature <span class="bold">silicon</span> may have had a poor correlation with the target
        <span class="bold">glass_type</span>, take look at the decision tree(s) from above (Figure.12 or Figure.13) - the feature
        <span class="bold">silicon</span> seems to have an influence on the target classification.</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this
        article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P15-DecisionTrees-Scikit.ipynb" target="_blank">
          <span class="bold">Decision Trees</span></a></p></li>
      </ul>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
