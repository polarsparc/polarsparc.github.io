<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Understanding Bias and Variance - Part 4">
    <meta name="subject" content="Machine Learning - Understanding Bias and Variance - Part 4">
    <meta name="keywords" content="python, machine_learning, regression">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Understanding Bias and Variance - Part 4</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Understanding Bias and Variance - Part 4</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">04/02/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/MachineLearning/LinearRegression-3.html" target="_blank"><span class="bold">Part 3</span></a>
        of this series, we demonstrated the Polynomial Regression models using <span class="bold">Scikit-Learn</span> on the Auto
        MPG dataset. One of the concepts we touched towards the end was related to <span class="hi-grey">overfitting</span>.</p>
      <p><span class="hi-yellow">Bias</span> and <span class="hi-yellow">Variance</span> are related to underfitting and overfitting
        and are essential concepts for developing optimal machine learning models.</p>
      <p>We will leverage the dataset <a href="https://people.sc.fsu.edu/~jburkardt/datasets/regression/x03.txt" target="_blank">
        <span class="bold">Age vs BP</span></a> to understand bias and variance through practical demonstration.</p>
    </div>
    <div id="section-div">
      <p>Bias and Variance</p>
    </div>
    <div id="para-div">
      <p>One of the steps in the process of model development involves splitting the known dataset into the training set and the test
        set. The training set is used to train the model and the test set is used to evaluate the model. In the training phase, the
        model is learning by finding patterns from the training data, so that it can be used for predicting in the future. There will
        always be some deviation between what the model predicts versus what is the actual. This is error of estimation from the
        model. The <span class="hi-yellow">Bias</span> and <span class="hi-yellow">Variance</span> are two measures of the error of
        estimation.</p>
    </div>
    <div id="step-div">
      <p>Bias</p>
    </div>
    <div id="para-div">
      <p><span class="bold">Bias</span> measures the difference between the actual value versus the estimated value. When the model
        is simple (as in the case of Simple Linear Regression), the model makes simplistic assumptions and does not attempt to
        identify all the patterns from the training set, and as a result, the differences (between the estimated and the actual) are
        large (meaning that the Bias is <span class="underbold">HIGH</span>). However, when the model is complex (as in the case of
        Polynomial Regression), the model attempts to capture as many patterns as possible from the training set, and as a result, the
        differences (between the estimated and the actual) are small (meaning that the Bias is <span class="underbold">LOW</span>).</p>
      <p>In mathematical terms, $Bias = E[(y - \hat{y})]$, where $y$ is the actual value and $\hat{y}$ is the estimated value.</p>
    </div>
    <div id="para-div">
      <p>Consider the following plot of the Age vs BP with the line of best fit using Simple Linear Regression:</p>
    </div>
    <div id="img-outer-div"> <img alt="Age vs BP (Simple)" src="./images/regression-24.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Contrast the above plot with the following plot of the Age vs BP with the polynomial line of best fit using Polynomial
        Regression:</p>
    </div>
    <div id="img-outer-div"> <img alt="Age vs BP (Polynomial)" src="./images/regression-25.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>As is evident comparing the above two plots, the Simple Linear Regression model in Figure.1 has a high bias, while the
        Polynomial Regression model in Figure.2 has a low bias.</p>
    </div>
    <div id="step-div">
      <p>Variance</p>
    </div>
    <div id="para-div">
      <p><span class="bold">Variance</span> measures the variability in the model estimates between the different training data sets.
        When the model is simple (as in the case of Simple Linear Regression), the model estimates are consistent between the different
        training data sets, and as a result, the Variance is <span class="underbold">LOW</span>. However, when the model is complex
        (as in the case with Polynomial Regression), the model estimates vary greatly between the different training data sets, and
        as a result, the Variance is <span class="underbold">HIGH</span>.</p>
      <p>In mathematical terms, $Var(\hat{y}) = E[(\hat{y} - \bar{\hat{y}})^2]$, where $\hat{y}$ is the estimated value and
        $\bar{\hat{y}}$ is the mean of all the estimated values from different data sets.</p>
    </div>
    <div id="para-div">
      <p>Consider the following plot of the Age vs BP with the line of best fit, along with the training set (in grey) and test set
        (in red), using Simple Linear Regression:</p>
    </div>
    <div id="img-outer-div"> <img alt="Age vs BP (Simple)" src="./images/regression-26.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Contrast the above plot with the following plot of the Age vs BP with the polynomial line of best fit, along with the
        training set (in grey) and test set (in red), using Polynomial Regression:</p>
    </div>
    <div id="img-outer-div"> <img alt="Age vs BP (Polynomial)" src="./images/regression-27.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>As is evident comparing the above two plots, the Simple Linear Regression model in Figure.3 has a low variance, while the
        Polynomial Regression model in Figure.4 has a high variance.</p>
    </div>
    <div id="section-div">
      <p>Bias-Variance Decomposition</p>
    </div>
    <div id="para-div">
      <p>The total error from a model is the sum of two errors - <span class="bold">Reducible-Error</span> + <span class="bold">
        Irreducible-Error</span>.</p>
      <p><span class="bold">Irreducible-Error</span> is due to various noise and cannot be eliminated, while <span class="bold">
        Reducible-Error</span> is contributed by the bias and the variance components of the model and can be reduced by tuning
        the model.</p>
      <p>In general, the model error is represented using the equation: $Err = Bias^2 + Variance + \varepsilon$.</p>
    </div>
    <div id="para-div">
      <p>Let $f(X)$ denote the function that produces the actual target values $Y$ for a given data set $X$. Then, $Y = f(X) +
        \epsilon$, where $\epsilon$ is the noise.</p>
      <p>Also, let $g(X)$ denote the model function that estimates the target values $\hat{Y}$ for the given data set $X$. Then,
        $\hat{Y} = g(X)$.</p>
      <p>From <a href="https://polarsparc.github.io/Mathematics/Statistics-2.html" target="_blank"><span class="bold">Introduction to
        Statistics - Part 2</span></a>, we have learnt the following facts about discrete random variable $X$:</p>
      <ul id="blue-sqr-ul">
        <li><p>The expected value $E(X) = \bar{X} = \sum_{i=1}^n x_i P(x_i)$ ..... $\color{red} (1)$</p></li>
        <li>
          <p>The variance $Var(X) = E[(X - \bar{X})^2] = E(X^2) - \bar{X}^2$.</p>
          <p>OR, $E(X^2) = \bar{X} + Var(X)$ ..... $\color{red} (2)$</p>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>The model error can be expressed as $Err(X) = E[(Y - \hat{Y})^2]$</p>
      <p>That is, $Err(X) = E[Y^2 + \hat{Y}^2 - 2 Y \hat{Y}]$</p>
      <p>Or, $Err(X) = E[Y^2] + E[\hat{Y}^2] - 2 E[Y] E[\hat{Y}]$ ..... $\color{red} (3)$</p>
      <p>From $\color{red} (1)$ and $\color{red} (2)$, we can write $E[Y^2] = \bar{Y}^2 + Var(Y) = E[Y]^2 + Var(Y)$ ..... $\color{red}
        (4)$</p>
      <p>From $\color{red} (1)$ and $\color{red} (2)$, we can write $E[\hat{Y}^2] = \bar{\hat{Y}}^2 + Var(\hat{Y}) = E[\hat{Y}]^2 +
        Var(\hat{Y})$ ..... $\color{red} (5)$</p>
      <p>Replacing $\color{red} (4)$ and $\color{red} (5)$ into $\color{red} (3)$, we get $Err(X) = E[Y]^2 + Var(Y) + E[\hat{Y}]^2
        + Var(\hat{Y}) - 2 E[Y] E[\hat{Y}]$</p>
      <p>Rearranging the terms, we get $(\color{blue}E[Y]^2 + E[\hat{Y}]^2 - 2 E[Y] E[\hat{Y}]$ $) + Var(\hat{Y}) + Var(Y)$</p>
      <p>That is, $Err(X) = E[(Y - \hat{Y})]^2 + Var(\hat{Y})  + Var(f(X) + \epsilon)$ ..... $\color{red} (6)$</p>
      <p>For the actual model $f(X)$, there is <span class="underbold">NO</span> variance and hence $Var(f(X)) = 0$</p>
      <p>The variance on noise is a constant. That is, $Var(\epsilon) = \varepsilon$</p>
      <p>Rewriting $\color{red} (6)$, we get the following:</p>
      <p>$\color{red} \boldsymbol{Err(X)}$ $= \bbox[pink,2pt]{Bias^2 + Var(\hat{Y}) + \varepsilon}$</p>
    </div>
    <div id="section-div">
      <p>Bias-Variance Tradeoff</p>
    </div>
    <div id="para-div">
      <p>For a given model, there is a tension between the bias and the variance. As the bias increases, the variance decreases and
        vice-versa. If the model has fewer number of independent feature variables, then the model is simple, resulting in high bias
        and low variance. On the other hand, if the model has a larger number of independent feature variables, then the model is
        complex, resulting in low bias and high variance. The goal of <span class="hi-yellow">Bias-Variance Tradeoff</span> is to
        find the right balance between the bias and the variance in the model.</p>
    </div>
    <div id="para-div">
      <p>The following illustration depicts the bias-variance relationship:</p>
    </div>
    <div id="img-outer-div"> <img alt="Bias-Variance" src="./images/regression-28.png" class="img-cls" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P4-1-Bias-Variance.ipynb" target="_blank"><span class="bold">Bias and Variance</span></a></p></li>
      </ul>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/MachineLearning/LinearRegression-3.html" target="_blank"><span class="bold">Machine Learning - Polynomial Regression using Scikit-Learn - Part 3</span></a></p>
      <p><a href="https://polarsparc.github.io/MachineLearning/LinearRegression-2.html" target="_blank"><span class="bold">Machine Learning - Linear Regression using Scikit-Learn - Part 2</span></a></p>
      <p><a href="https://polarsparc.github.io/MachineLearning/LinearRegression-1.html" target="_blank"><span class="bold">Machine Learning - Linear Regression - Part 1</span></a></p>
    </div>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
