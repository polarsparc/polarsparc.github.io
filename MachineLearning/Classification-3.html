<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Naive Bayes using Scikit-Learn">
    <meta name="subject" content="Machine Learning - Naive Bayes using Scikit-Learn">
    <meta name="keywords" content="python, machine_learning, classification, scikit-learn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Naive Bayes using Scikit-Learn</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Naive Bayes using Scikit-Learn</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">06/11/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Naive Bayes</span> is a probabilistic classification model, that is based on
        <a href="https://polarsparc.github.io/Mathematics/BayesTheorem.html" target="_blank"><span class="bold">Bayes Theorem</span></a>.
        It is fast, explainable, and good for handling high-dimensional data set. It is called <span class="hi-blue">Naive</span> since
        it makes the simple assumption that all the features in the data set are independent of each other, which is typically not
        the case in the real world scenarios.</p>
    </div>
    <div id="section-div">
      <p>Naive Bayes</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will develop a mathematical intuition for Naive Bayes.</p>
    </div>
    <div id="para-div">
      <p>From the article on <a href="https://polarsparc.github.io/Mathematics/BayesTheorem.html" target="_blank"><span class="bold">
        Bayes Theorem</span></a>, we can express the relationship between the dependent target variable $y$ and the independent feature
        variables $x_1, x_2, ..., x_n$ as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(y \mid x_1, x_2, ..., x_n) = \Large{\frac{P(x_1, x_2, ..., x_n \mid y) . P(y)}{P(x_1, x_2, ...,
        x_n)}}$ ..... $\color{red} (1)$</p>
      <p>Using the chain rule for conditional probability, we know the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(a,b,c) = P(a \mid b,c) . P(b,c) = P(a \mid b,c) . P(b \mid c) . P(c)$</p>
      <p>Therefore,</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(x_1, x_2, ..., x_n \mid y) = P(x_1 \mid x_2, ..., x_n \mid y) . P(x_2 \mid x_3, ..., x_n \mid y)
        ... P(x_n \mid y)$</p>
      <p>We know from Naive Bayes that each feature variable $x_i$ is independent of the other.</p>
      <p>Therefore,</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(x_1, x_2, ..., x_n \mid y) = P(x_1 \mid y) . P(x_2 \mid y) ... P(x_n \mid y)$ ..... $\color{red}
        (2)$</p>
      <p>Also,</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(x_1, x_2, ..., x_n) = P(x_1) . P(x_2) ... P(x_n)$ ..... $\color{red} (3)$</p>
      <p>Substituting equations $\color{red} (2)$ and $\color{red} (3)$ into equation $\color{red} (1)$, we get:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(y \mid x_1, x_2, ..., x_n) = \Large{\frac{P(x_1 \mid y) . P(x_2 \mid y) ... P(x_n \mid y) . P(y)}
        {P(x_1) . P(x_2) ... P(x_n)}}$ ..... $\color{red} (4)$</p>
      <p>For all samples in the data set, the value of equation $\color{red} (3)$ will be a constant.</p>
      <p>Therefore, left-hand side of the equation $\color{red} (4)$ will be proportional to the right-hand side of the equation
        $\color{red} (4)$ and can be rewriten as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(y \mid x_1, x_2, ..., x_n) \propto P(y) . P(x_1 \mid y) . P(x_2 \mid y) ... P(x_n \mid y)$</p>
      <p>That is,</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$P(y \mid x_1, x_2, ..., x_n) \propto \bbox[pink,2pt]{P(y) . \prod_{i=1}^n P(x_i \mid y)}$ .....
        $\color{red} (5)$</p>
      <p>The goal of Naive Bayes is then to classify the target variable $y$ into the appropriate category (or class) by finding the
        maximum likelihood (the right-side of the equation $\color{red} (5)$) that the given set of feature variables $x_1, x_2, ...,
        x_n$ belong to the appropriate category (or class).</p>
    </div>
    <div id="para-div">
      <p>The following are some of the commonly used Naive Bayes algorithms:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-blue">Bernoulli Naive Bayes</span> - Used when all the feature variables $x_1, x_2, ..., x_n$ in the
          data set can only take <span class="underbold">BINARY</span> values (two values encoded as either a 0 or a 1)</p></li>
        <li><p><span class="hi-blue">Gaussian Naive Bayes</span> - Used when all the feature variables $x_1, x_2, ..., x_n$ in the
          data set are <span class="underbold">CONTINUOUS</span> values that are normally distributed</p></li>
        <li><p><span class="hi-blue">Multinomial Naive Bayes</span> - Used when all the feature variables $x_1, x_2, ..., x_n$ in
          the data set are <span class="underbold">DISCRETE</span> values (like counts or frequencies)</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>In this following sections, we will demonstrate the use of the Naive Bayes model for classification (using scikit-learn)
        by leveraging the <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data" target="_blank">
        <span class="bold">Glass Identification</span></a> data set.</p>
    </div>
    <div id="para-div">
      <p>The first step is to import all the necessary Python modules such as, pandas, matplotlib, seaborn, and scikit-learn as shown
        below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to load the glass identification data set into a pandas dataframe, set the column names, and then display
        the dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'
glass_df = pd.read_csv(url, header=None)
glass_df = glass_df.drop(glass_df.columns[0], axis=1)
glass_df.columns = ['r_index', 'sodium', 'magnesium', 'aluminum', 'silicon', 'potassium', 'calcium', 'barium', 'iron', 'glass_type']
glass_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays few rows from the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Display" src="./images/naive-bayes-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display information about the glass identification dataframe, such as index and column types, missing
        (null) values, memory usage, etc., as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>glass_df.info()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays information about the glass identification dataframe:</p>
    </div>
    <div id="img-outer-div"> <img alt="Dataframe Information" src="./images/naive-bayes-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Fortunately, the data seems clean with no missing values.</p>
    </div>
    <div id="para-div">
      <p>The next step is to split the glass identification dataframe into two parts - a training data set and a test data set. The
        training data set is used to train the classification model and the test data set is used to evaluate the classification model.
        In this use case, we split 75% of the samples into the training dataset and remaining 25% into the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_train, X_test, y_train, y_test = train_test_split(glass_df, glass_df['glass_type'], test_size=0.25, random_state=101)
X_train = X_train.drop('glass_type', axis=1)
X_test = X_test.drop('glass_type', axis=1)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The next step is to create an instance of the standardization scaler to scale the desired feature (or predictor) variables
        in both the training and test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>scaler = StandardScaler()
s_X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
s_X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns, index=X_test.index)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The next few steps is to visualize the distribution of the feature variables to determine the type of Naive Bayes algorithm
        to use.</p>
    </div>
    <div id="para-div">
      <p>The next step is to display a distribution plot for the feature r_index (refractive index) using the glass identification
        training data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>sns.displot(X_train, x='r_index', color='salmon', kind='kde')
plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the distribution plot for the feature r_index from the glass identification training data
        set:</p>
    </div>
    <div id="img-outer-div"> <img alt="Refractive Index" src="./images/naive-bayes-03.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the distribution plot above, we can infer that the feature r_index seems to be normally distributed.</p>
    </div>
    <div id="para-div">
      <p>The next step is to display a distribution plot for the feature aluminum using the glass identification training data set
        as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>sns.displot(X_train, x='aluminum', color='limegreen', kind='kde')
plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the distribution plot for the feature aluminum from the glass identification training data
        set:</p>
    </div>
    <div id="img-outer-div"> <img alt="Aluminum" src="./images/naive-bayes-04.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the distribution plot above, we can infer that the feature aluminum seems to be normally distributed.</p>
    </div>
    <div id="para-div">
      <p>The next step is to display a distribution plot for the feature barium using the glass identification training data set as
        shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>sns.displot(X_train, x='barium', color='dodgerblue', kind='kde')
plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the distribution plot for the feature barium from the glass identification training data
        set:</p>
    </div>
    <div id="img-outer-div"> <img alt="Barium" src="./images/naive-bayes-05.png" class="img-cls" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the distribution plot above, we can infer that the feature barium seems almost normally distributed.</p>
    </div>
    <div id="para-div">
      <p>The next step is to display the distribution plot (along with the scatter plot) for the remaining features using the glass
        identification training data set as shown below</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>sns.pairplot(glass_df[['calcium', 'iron', 'magnesium', 'silicon', 'sodium']], height=1.5, diag_kind='kde')
plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the distribution plot (along with the scatter plot) for the remaining features using the
        glass identification training data:</p>
    </div>
    <div id="img-outer-div"> <img alt="Remaining Features" src="./images/naive-bayes-06.png" class="img-cls" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the pair plot above, we can infer that the remaining features seems normally distributed.</p>
    </div>
    <div id="para-div">
      <p>Given that all the feature variables from the glass identification data set seem normally distributed, the next step is to
        initialize the Gaussian Naive Bayes model class from scikit-learn and train the model using the training data set as shown
        below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = GaussianNB()
model.fit(s_X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the trained model to predict the glass_type using the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model.predict(s_X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display the accuracy score for the model performance as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>accuracy_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the accuracy score for the model performance:</p>
    </div>
    <div id="img-outer-div"> <img alt="Accuracy Score" src="./images/naive-bayes-07.png" class="img-cls" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above, one can infer that the model seems to predict very poorly. One possible reason could be that the model is
        not using an optimal hyperparameter value.</p>
    </div>
    <div id="para-div">
      <p>One of the hyperparameters used by the Gaussian Naive Bayes is <span class="hi-blue">var_smoothing</span>, which controls
        the variance of all the feature variables such that the distribution curve could be widen to include more samples away from
        the mean.</p>
      <p>The next step is to perform an extensive grid search on a list of hyperparameter values to determine the optimal value of
        the hyperparameter <span class="bold">var_smoothing</span> and display the optimal value as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>parameters = {
  'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]
}
cv_model = GridSearchCV(estimator=model, param_grid=parameters, cv=5, verbose=1, scoring='accuracy')
cv_model.fit(s_X_train, y_train)
cv_model.best_params_</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the optimal value for the hyperparameter <span class="bold">var_smoothing</span>:</p>
    </div>
    <div id="img-outer-div"> <img alt="Best Estimate" src="./images/naive-bayes-08.png" class="img-cls" />
      <div class="img-cap">Figure.8</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to re-initialize the Gaussian Naive Bayes model with the hyperparameter var_smoothing set to the value of
        <span class="hi-green">0.01</span> and re-train the model using the training data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = GaussianNB(var_smoothing=0.01)
model.fit(s_X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the trained model to predict the glass_type using the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model.predict(s_X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The final step is to display the accuracy score for the model performance as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>accuracy_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the accuracy score for the model performance:</p>
    </div>
    <div id="img-outer-div"> <img alt="Accuracy Score" src="./images/naive-bayes-09.png" class="img-cls" />
      <div class="img-cap">Figure.9</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above, one can infer that the model seems to predict much better now (not great).</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this
        article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P13-NaiveBayes-Scikit.ipynb" target="_blank">
          <span class="bold">Naive Bayes</span></a></p></li>
      </ul>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/Mathematics/Probability.html" target="_blank"><span class="bold">Introduction to Probability</span></a></p>
      <p><a href="https://polarsparc.github.io/Mathematics/BayesTheorem.html" target="_blank"><span class="bold">Introduction to Bayes Theorem</span></a></p>
    </div>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
