<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Logistic Regression - Part 1">
    <meta name="subject" content="Machine Learning - Logistic Regression - Part 1">
    <meta name="keywords" content="python, machine_learning, classification">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Logistic Regression - Part 1</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Logistic Regression - Part 1</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">05/01/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br />
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p>Often times in the real world, the dependent outcome (or target) variable is <span class="underbold">NOT</span> always a
        continuous random variable (quantitative), but a discrete random variable (qualitative or categorical). In such cases, the
        Linear Regression models do not work well and we will need a different approach. This is where the techniques of
        <span class="hi-yellow">Classification</span> come in handy. Classification is a statistical process, which attempts to
        segregate the observations from the dependent target (or a response) variable into one of the pre-determined set of
        categorical classes.</p>
      <p><span class="hi-yellow">Logistic Regression</span> is applicable when the <span class="hi-vanila">Dependent</span> Outcome
        (sometimes called a Response or a Target) variable $y$ is a dichotomous (or binary) <span class="underbold">CATEGORICAL</span>
        variable and has some linear relationship with one or more of <span class="hi-vanila">Independent</span> Feature (or
        Predictor) variable(s) $x_1, x_2, ..., x_n$.</p>
      <p>The value of the outcome (or target) variable $y$ is a binary - either a $0$ (referred to as a class $0$, a Failure, or a
        Negative) OR a $1$ (referred to as a class $1$, a Success, or a Positive).</p>
    </div>
    <div id="section-div">
      <p>Logistic Regression</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will use mathematical intuition to arrive at the <span class="hi-yellow">Logistic Regression
        </span> model.</p>
      <p>We know there exists a linear relationship between the dependent target variable $y$ and the independent feature variables
        $x_1, x_2, ..., x_n$. This relationship can be expressed (using the matrix notation) as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$y = \beta_0 + \sum_{i=1}^n \beta_i x_i = X\beta$ ..... $\color{red} (1)$</p>
      <p>where $X$ is the matrix of features and $\beta$ is the vector of coefficients.</p>
    </div>
    <div id="para-div">
      <p>Also, for Logistic Regression, the dependent outcome (or target) variable $y$ predicts the probability of a $1$ (referred
        to as a class $1$, a Success, or a Positive) or a $0$ (referred to as a class $0$, a Failure, or a Negative), based on the
        linear relationship with one or more independent feature variables $x_1, x_2, ..., x_n$. This implies that the range of the
        dependent target (or response) variable $y$ is in the range of $0$ to $1$.</p>
    </div>
    <div id="para-div">
      <p>We can rewrite the equation $\color{red} (1)$ from above as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$p(X) = X\beta$ ..... $\color{red} (2)$</p>
      <p>where $p(X)$ is the probability for the dependent outcome (or target) variable $y$.</p>
    </div>
    <div id="para-div">
      <p>From the equation $\color{red} (2)$ above, it is clear that there is a mismatch between the right hand side and the left
        hand side of the equation. The right hand side is unbounded in the range $(-\infty, \infty)$, while the left hand side is
        bounded in the range $(0, 1)$.</p>
    </div>
    <div id="para-div">
      <p>From the concepts in probability, we know that if $p(X)$ is the probability of predicting a $1$ (or success), then the
        probability of predicting a $0$ (or failure) is $(1 - p(X))$.</p>
      <p>Also, from probability, <span class="underbold">odds</span> is expressed as a ratio of the probability of the event
        happening over the probability the event not happening. In other words:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$odds = \Large{\frac{p(X)}{1-p(X)}}$ ..... $\color{red} (3)$</p>
    </div>
    <div id="para-div">
      <p>Let us now rewrite the equation $\color{red} (2)$ from above in terms of the equation $\color{red} (3)$ as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{p(X)}{1-p(X)}}$ $= X\beta$ ..... $\color{red} (4)$</p>
    </div>
    <div id="para-div">
      <p>From the equation $\color{red} (4)$ above, we still have a mismatch between the right hand side and the left hand side of
        the equation. The right hand side is unbounded in the range $(-\infty, \infty)$, while the left hand side is half-unbounded
        in the range $(0, \infty)$.</p>
    </div>
    <div id="para-div">
      <p>Let us now rewrite the equation $\color{red} (4)$ from above in terms of natural logarithm as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln({\Large{\frac{p(X)}{1-p(X)}}})$ $= X\beta$ ..... $\color{red} (5)$</p>
    </div>
    <div id="para-div">
      <p>From the equation $\color{red} (5)$ above, as the probability $p(X)$ tends towards $1$, the natural log will tend towards
        $\infty$ and similarly as the probability $p(X)$ tends towards $0$, the natural log will tend towards $-\infty$. This means
        the left hand side is unbounded in the range $(-\infty, \infty)$ just like the right hand side of the equation.</p>
    </div>
    <div id="para-div">
      <p>Now that the left hand side and the right hand side match, let us now try to simplify the equation $\color{red} (5)$ from
        above.</p>
    </div>
    <br/>
    <div id="warn-div">
      <h4>*** Basic Rule of Natural Logarithm ***</h4>
      <p>&nbsp;&nbsp;If $\ln(a) = b$, then $a = e^{b}$</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Using the basic rule of logarithms, we can rewrite the equation $\color{red} (5)$ from above as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{p(X)}{1-p(X)}}$ $= e^{X\beta}$ ..... $\color{red} (6)$</p>
      <p>Simplifying the the equation $\color{red} (6)$ from above, we get:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$p(X) = (1 - p(X)).e^{X\beta}$</p>
      <p>Expanding the right hand side, we get:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$p(X) = e^{X\beta} - p(X)).e^{X\beta}$</p>
      <p>Moving all the $p(X)$ terms together, we get:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$p(X) + p(X)).e^{X\beta} = e^{X\beta}$</p>
      <p>Pulling the common $p(X)$ term out, we get:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$p(X).(1 + e^{X\beta}) = e^{X\beta}$</p>
      <p>Simplifying the equation further, we get:</p>
      <p>$\color{red} \boldsymbol{p(X)}$ $=$ $\bbox[pink,2pt]{\Large{\frac{e^{X\beta}}{1 + e^{X\beta}}}}$ $=$ $\bbox[pink,2pt]
        {\Large{\frac{1}{1 + e^{-X\beta}}}}$ ..... $\color{blue} (A)$</p>
    </div>
    <div id="para-div">
      <p>The equation $\color{blue} (A)$ from above is the mathematical model for Logistic Regression. The above function for $p(X)$
        is called as a <span class="bold">Sigmoid</span> (or <span class="bold">Logit</span>) function.</p>
    </div>
    <div id="para-div">
      <p>The following illustration displays the plot of a sigmoid function from the equation $\color{blue} (A)$ shown above:</p>
    </div>
    <div id="img-outer-div"> <img alt="Sigmoid Function" src="./images/classification-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>By applying a threshold to the sigmoid function, such as $p(X) \ge 0.5$ to predict class $1$, one can leverage it for binary
        classification.</p>
    </div>
    <div id="para-div">
      <p>Realize the equation $\color{blue}(A)$ from above is non-linear in nature, meaning, we cannot relate how a unit increase
        in $\beta$ will effect the predicted target value.</p>
      <p>We can gain some insights from the sign of the $\beta$ coefficients, which are as follows:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-green">Positive Coefficient</span> - Increase in the likelihood of the predicted target value belonging
          to class $1$ (or success)</p></li>
        <li><p><span class="hi-red">Negative Coefficient</span> - Decrease in the likelihood of the predicted target value belonging
          to class $1$ (or success)</p></li>
      </ul>
      <p>As for the magnitude of the $\beta$ coefficients, one can compare one against the other to gain insights as to which of the
        features have a stronger influence on the predicted target value. For example, a positive $\beta_1$ that is larger in value
        (magnitude) than $\beta_2$ has more effect on the predicted target value.</p>
    </div>
    <div id="para-div">
      <p>To find the optimum values for the $\beta$ coefficients, the optimization goal of Logistic Regression is to find the sigmoid
        line of best fit. For that, Logistic Regression uses the idea of <span class="hi-yellow">Maximum Likelihood</span>, which tries
        to predict the target values as close as possible to $1$ for all the samples that belong to class $1$ (or success) and as close
        as possible to $0$ for the samples that belong to class $0$ (or failure).</p>
      <p>In mathematical terms, the equation for the maximum likelihood estimate (or cost function) can be represented as follows:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$L(\beta) = \prod_{i=1}^m p(X_i)^{y_i} (1 - p(X_i))^{(1 -  y_i)}$ ..... $\color{red} (7)$</p>
      <p>where there are $m$ samples and $i$ is index to represent the $i_{th}$ sample.</p>
    </div>
    <br/>
    <div id="warn-div">
      <h4>*** Basic Rules Logarithms ***</h4>
      <p>&nbsp;&nbsp;1. $log_e = \ln$</p>
      <p>&nbsp;&nbsp;2. $\ln(1) = 0$</p>
      <p>&nbsp;&nbsp;3. $\ln(a \times b) = \ln(a) + \ln(b)$</p>
      <p>&nbsp;&nbsp;4. $\ln(a \div b) = \ln(a) - \ln(b)$</p>
      <p>&nbsp;&nbsp;5. $\ln(a^b) = b.\ln(a)$</p>
      <p>&nbsp;&nbsp;6. $\ln(e^a) = a$</p>
    </div>
    <br/>
    <div id="para-div">
      <p>Applying natural logarithm to both side of the equation $\color{red} (7)$ from above, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln(L(\beta)) = \ln(\prod_{i=1}^m p(X_i)^{y_i} (1 - p(X_i))^{(1 -  y_i)})$</p>
      <p>Applying the rules $3$ and $5$ of logarithms to the right hand side, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln(L(\beta)) = \sum_{i=1}^m [y_i.\ln(p(X_i)) + (1 - y_i).\ln(1 - p(X_i))]$</p>
      <p>Expanding and re-arranging, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln(L(\beta)) = \sum_{i=1}^m [y_i.\ln(p(X_i)) - y_i.\ln(1 - p(X_i)) + \ln(1 - p(X_i))]$</p>
      <p>Pulling the common $y_i$ term out, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln(L(\beta)) = \sum_{i=1}^m [y_i.(\ln(p(X_i)) - \ln(1 - p(X_i))) + \ln(1 - p(X_i))]$</p>
      <p>Applying the rule $4$ of logarithms to the right hand side, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln(L(\beta)) = \sum_{i=1}^m [y_i.\ln({\Large{\frac{p(X_i)}{1 - p(X_i)}}})$ $+ \ln(1 - p(X_i))]$</p>
      <p>Using the equation $\color{red} (6)$ from above, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln(L(\beta)) = \sum_{i=1}^m [y_i.\ln(X_i\beta) + \ln(1 - p(X_i))]$</p>
      <p>Applying the rule $6$ of logarithms to the right hand side, we get the following:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\ln(L(\beta)) = \sum_{i=1}^m [y_iX_i\beta + \ln(1 - p(X_i))]$</p>
      <p>Using the equation $\color{blue} (A)$ from above, we get the following:</p>
      <p>$\color{red} \boldsymbol{\ln(L(\beta))}$ $=$ $\bbox[pink,2pt]{\sum_{i=1}^m [y_iX_i\beta - \ln(1 + e^{X_i\beta})]}$ .....
        $\color{blue} (B)$</p>
    </div>
    <div id="para-div">
      <p>For simplicity, let $l(\beta) = \ln(L(\beta))$.</p>
      <p>In order to find the maximum likelihood estimates (or <span class="hi-blue">MLE</span>), one needs to take the partial
        derivative of the equation $\color{blue} (B)$ from above, which results in the following optimization equation:</p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;$\Large{\frac{\partial{l(\beta)}}{\partial{\beta}}}$ $= \sum_{i=1}^m [$ $y_iX_i - \Large{\frac{
        X_ie^{X_i\beta}}{1 + e^{X_i\beta}}}$ $]$</p>
      <p>Rearranging and simplifying, we get the following:</p>
      <p>$\color{red} \boldsymbol{\Large{\frac{\partial{l(\beta)}}{\partial{\beta}}}}$ $=$ $\bbox[pink,2pt]{\sum_{i=1}^m X_i[y_i -
        p(X_i)]}$ ..... $\color{blue} (C)$</p>
    </div>
    <div id="para-div">
      <p>In order to find an approximation for the $\beta$ coefficients, one needs to solve the equation $\color{blue} (C)$ from
        above using numerical methods (such as gradient descent).</p>
    </div>
    <div id="section-div">
      <p>Model Evaluation Metrics</p>
    </div>
    <div id="para-div">
      <p>In the following paragraphs we will look at the different metrics that help us evaluate the effectiveness of the logistic
        regression model.</p>
    </div>
    <div id="step-div">
      <p>Confusion Matrix</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Confusion Matrix</span> (or <span class="hi-yellow">Error Matrix</span>) is a summary matrix that
        can be used to evaluate the performance of a classification model.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the confusion matrix for the logistic regression (binary classification) model:</p>
    </div>
    <div id="img-outer-div"> <img alt="Confusion Matrix" src="./images/classification-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>In the confusion matrix above, class $1$ means success or positive, while class $0$ means failure or negative. Also, the
        columns (top) represents the actual classification, while the rows (left) represents the predicted classification.</p>
      <p>The following are the four possible outcomes:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-green">True Positive</span> - <span class="hi-green">TP</span> for short, it indicates the predicted
          value of $1$ and the actual value is also $1$</p></li>
        <li><p><span class="hi-red">False Positive</span> - <span class="hi-red">FP</span> for short, it indicates the predicted
          value of $1$, while the actual value is $0$. This is also known as the <span class="hi-yellow">Type 1</span> error</p></li>
        <li><p><span class="hi-red">False Negative</span> - <span class="hi-red">FN</span> for short, it indicates the predicted
          value of $0$, while the actual value is $1$. This is also known as the <span class="hi-yellow">Type 2</span> error</p></li>
        <li><p><span class="hi-green">True Negative</span> - <span class="hi-green">TN</span> for short, it indicates the predicted
          value of $0$ and the actual value is also $0$</p></li>
      </ul>
    </div>
    <div id="step-div">
      <p>Accuracy</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Accuracy</span> is a metric that represents the number of correctly classified data samples over
        the total number of data samples.</p>
      <p>In mathematical terms,</p>
      <p>$\color{red} \boldsymbol{Accuracy}$ $= \bbox[pink,2pt]{\Large{\frac{TP + TN}{TP + FP + FN + TN}}}$</p>
    </div>
    <div id="step-div">
      <p>Precision</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Precision</span> is a metric that represents the number of correctly classified class $1$ (positive)
        data samples over the total number of actual classified data samples.</p>
      <p>In mathematical terms,</p>
      <p>$\color{red} \boldsymbol{Precision}$ $= \bbox[pink,2pt]{\Large{\frac{TP}{TP + FP}}}$</p>
    </div>
    <div id="step-div">
      <p>Recall</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Recall</span> (also known as <span class="hi-yellow">Sensitivity</span>) is a metric that represents
        the number of correctly classified class $1$ (positive) data samples over the total number of actual class $1$ (positive)
        data samples.</p>
      <p>In mathematical terms,</p>
      <p>$\color{red} \boldsymbol{Recall}$ $= \bbox[pink,2pt]{\Large{\frac{TP}{TP + FN}}}$</p>
    </div>
    <div id="step-div">
      <p>F1 - Score</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">F1 - Score</span> is a metric that represents the weighted average of precision and recall.</p>
      <p>In mathematical terms,</p>
      <p>$\color{red} \boldsymbol{F1-Score}$ $= \bbox[pink,2pt]{2 \times \Large{\frac{Precision \times Recall}{Precision + Recall}}}$</p>
    </div>
    <div id="step-div">
      <p>Receiver Operating Characteristics Curve</p>
    </div>
    <div id="para-div">
      <p>The <span class="hi-yellow">Receiver Operating Characteristics</span> (also known as the <span class="hi-yellow">ROC</span>)
        curve is used to visualize the performance of a classification model (such as the logistic regression model) for all possible
        classification thresholds.</p>
      <p>The plot uses the following two terms:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="bold">True Positive Rate</span> - <span class="hi-yellow">TPR</span> for short. When the actual
          classification is a $1$, how often did the model correctly predict a $1$. In mathematical terms, it is defined as:
          $TPR = \Large{\frac{TP}{TP + FN}}$</p></li>
        <li><p><span class="bold">False Positive Rate</span> - <span class="hi-yellow">FPR</span> for short. When the actual
          classification is a $0$, how often did the model incorrectly predict a $1$. In mathematical terms, it is defined as:
          $FPR = \Large{\frac{FP}{FP + TN}}$</p></li>
      </ul>
      <p>ROC curve is a plot of the False Positive Rate (FPR) along the x-axis versus the True Positive Rate (TPR) along the y-axis
        for the different classification thresholds.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows a hypothetical ROC curve:</p>
    </div>
    <div id="img-outer-div"> <img alt="ROC Curve" src="./images/classification-03.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The red dotted line that divides the graph into two halfs is where $TPR = FPR$. Any value on or below this red dotted line
        is BAD (worse case). The desire is for the ROC curve to be as close as possible to the red star at the top left corner -
        this is when the classification model is predicting correctly all the time (ideal case).</p>
    </div>
    <div id="para-div">
      <p>As we increase the classification threshold, we predict more $0$ values. This implies a lower TPR and FPR. Similarly, as
        we decrease the classification threshold, we predict more $1$ values. This implies a higher TPR and FPR.</p>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/Mathematics/Probability.html" target="_blank"><span class="bold">Introduction to Probability</span></a></p>
    </div>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
