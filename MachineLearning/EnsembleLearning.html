<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Understanding Ensemble Learning">
    <meta name="subject" content="Machine Learning - Understanding Ensemble Learning">
    <meta name="keywords" content="python, machine_learning, ensemble_learning">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Understanding Ensemble Learning</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Understanding Ensemble Learning</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">07/03/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br />
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Ensemble</span> means a group of <span class="underbold">ENTITIES</span> viewed as a whole versus
        viewed individually.</p>
      <p>So, what is this got to do with machine learning ???</p>
      <p>We know that some machine learning algorithms perform better than the others for a given data set. The natural question then
        is - why can't we combine these different machine learning algorithms together to create a better performing meta model ???
        This in essence is the core idea behind <span class="hi-yellow">Ensemble Learning</span>.</p>
    </div>
    <div id="section-div">
      <p>Ensemble Learning</p>
    </div>
    <div id="para-div">
      <p>An <span class="bold">Ensemble Learning</span> model combines multiple weak (poorly performing) machine learning models to
        create an enhanced machine learning model that has better predicting accuracy.</p>
      <p>In other words, an Ensemble Learning model aims to optimize the bias/variance trade-off by combining multiple base models.</p>
    </div>
    <div id="para-div">
      <p>Now, the next question to pop in ones mind - how does one combine the various machine learning models ???</p>
      <p>The following are the two most popular ways of combining the machine learning models:</p>
    </div>
    <div id="para-div">
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-yellow">Bagging</span> - the models are evaluated in parallel</p></li>
        <li><p><span class="hi-yellow">Boosting</span> - the models are evaluated in sequence</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>The <span class="bold">Ensemble Learning</span> methods are typically used with Decision Trees, since trees are applicable
        to both classification and regression problems.</p>
    </div>
    <div id="step-div">
      <p>Bagging</p>
    </div>
    <div id="para-div">
      <p>Bagging is the short for <span class="hi-yellow">Bootstrap AGGregatING</span>.</p>
      <p>The following are the details behind the bagging technique:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p>Train each model in the bagging ensemble with random samples of data from the training data set. The method of taking
            random samples from a data set with replacement is referred to as <span class="hi-yellow">Bootstrapping</span>.</p>
          <p>In addition, each model is trained with a random subset of feature variables from the training data set. If $N$ is the
            total number of feature variables in the data set, then as a general rule of thumb, one would randomly select $\sqrt{N}$
            number of features for each model in the ensemble for classification problems AND $\large{\frac{N}{3}}$ number of features
            for each model in the ensemble for regression problems.</p>
          <p>The illustration below depicts this step:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Train Bagging" src="./images/ensemble-learning-01.png" class="img-cls" />
            <div class="img-cap">Figure.1</div>
          </div>
          <br/>
          <p>Note that with bootstrapping, it is possible that some samples appear more than once and some samples are never chosen.
            The unselected (or unseen) samples are referred to as <span class="hi-yellow">Out of Bag</span> samples.</p>
          <p>The illustration below depicts an example of Out of Bag samples:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Out of Bag" src="./images/ensemble-learning-02.png" class="img-cls" />
            <div class="img-cap">Figure.2</div>
          </div>
          <br/>
          <p>The Bagging technique can internally use the out of bag samples to perform validation on the ensemble model.</p>
        </li>
        <li>
          <p>To predict the outcome for a new data sample, evaluate the sample through each of the trained models in the ensemble
            in parallel.</p>
          <p>The illustration below depicts this step:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Evaluate Bagging" src="./images/ensemble-learning-03.png" class="img-cls" />
            <div class="img-cap">Figure.3</div>
          </div>
          <br/>
        </li>
        <li>
          <p>Aggregate the individual predictions from each of the trained models in the ensemble. For classification problems, the
            aggregation method is mode (majority vote) and for regression problems, the aggregation method is mean (average).</p>
          <p>The illustration below depicts this step:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Aggregate Bagging" src="./images/ensemble-learning-04.png" class="img-cls" />
            <div class="img-cap">Figure.4</div>
          </div>
          <br/>
        </li>
      </ul>
      <p>One of the popular machine learning algorithms using the bagging method is <span class="hi-yellow">Random Forest</span>.</p>
    </div>
    <div id="step-div">
      <p>Boosting</p>
    </div>
    <div id="para-div">
      <p>The following are the details behind the boosting technique:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p>Add an additional feature to the training data set which represents the <span class="hi-yellow">weight</span> of each
            of the samples in the training data set. During training of the model, these weights influence which samples the model
            should focus on more - more the weight, more the focus. Initialize the weights to the same value for all the samples in
            the training data set.</p>
        </li>
        <li>
          <p>Pass the training data set through the first <span class="hi-yellow">Weak Model</span> to fit. Think of the Weak Model
            as a very simple Decision Tree of depth one, meaning, the tree has a decision node and two leaves. This is often referred
            to as a <span class="hi-yellow">Decision Stump</span>.</p>
          <p>The illustration below depicts this step:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Train Boosting" src="./images/ensemble-learning-05.png" class="img-cls" />
            <div class="img-cap">Figure.5</div>
          </div>
          <br/>
          <p>The weak model may accurately predict for some samples (the blue square in the green region) and poorly for the other
            remaining samples (other shapes in the blue region).</p>
        </li>
        <li>
          <p>Adjust the weights of the samples in the training data set based on the predictions from the previous weak model. The
            weights of the samples predicted corrected is <span class="underbold">DECREASED</span> and the weights of the samples
            with errors is <span class="underbold">INCREASED</span>.</p>
          <p>The illustration below depicts this step:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Adjust Weights" src="./images/ensemble-learning-06.png" class="img-cls" />
            <div class="img-cap">Figure.6</div>
          </div>
          <br/>
          <p>Notice the size of the blue square in reduced since it was predicted correctly.</p>
        </li>
        <li>
          <p>Pass the weight adjusted training data set through the next Weak Model to fit. Based on the prediction accuracy, once
            again adjust the weights of the samples in the training data set.</p>
          <p>The illustration below depicts this step:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Train and Adjust" src="./images/ensemble-learning-07.png" class="img-cls" />
            <div class="img-cap">Figure.7</div>
          </div>
          <br/>
          <p>The general idea is that the next weak model will learn from the mistakes of the previous weak model.</p>
        </li>
        <li>
          <p>Continue this process of adjusting the weights and training the next weak model until the errors are reduced or have
            reached a threshold.</p>
          <p>The illustration below depicts this step:</p>
          <br/>
          <div id="img-outer-div"> <img alt="Final State" src="./images/ensemble-learning-08.png" class="img-cls" />
            <div class="img-cap">Figure.8</div>
          </div>
          <br/>
        </li>
        <li>
          <p>To predict the outcome for a new data sample, pass the sample through each of the trained weak models in the ensemble
            (in sequence) for the target outcome.</p>
        </li>
      </ul>
      <p>Some of the popular machine learning algorithms using the boosting method are as follows:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-yellow">AdaBoost</span></p></li>
        <li><p><span class="hi-yellow">Gradient Boosting Machine</span></p></li>
        <li><p><span class="hi-yellow">XGBoost</span></p></li>
      </ul>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
