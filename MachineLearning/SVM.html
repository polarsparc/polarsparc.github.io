<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Support Vector Machines using Scikit-Learn">
    <meta name="subject" content="Machine Learning - Support Vector Machines using Scikit-Learn">
    <meta name="keywords" content="python, machine_learning, classification, ensemble, svm, scikit-learn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Support Vector Machines using Scikit-Learn</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br />
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Support Vector Machines using Scikit-Learn</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">08/10/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p><span class="hi-yellow">Support Vector Machines</span> (or <span class="hi-blue">SVM</span> for short) is another machine
        learning model that is widely used for classification problems, although it can also be used for regression problems.</p>
    </div>
    <div id="section-div">
      <p>Support Vector Machines</p>
    </div>
    <div id="para-div">
      <p>Before we jump into the intuition behind SVM, let us understand the concept of <span class="hi-yellow">Hyperplanes</span>
        in the context of the Geometrical space.</p>
      <p>A Hyperplane is a $N-1$ subspace in an $N$ dimensional geometric space. For example, a <span class="underbold">DOT</span>
        is a subspace on a one dimensional <span class="underbold">LINE</span>, a <span class="underbold">LINE</span> is a subspace
        on a two dimensional <span class="underbold">PLANE</span>, a <span class="underbold">PLANE</span> is a subspace on a three
        dimensional geometric space, and so on. For the $4^{th}$ dimension and beyond, since it is difficult to visuialize, we refer
        to the subspace as a <span class="underbold">HYPERPLANE</span>.</p>
      <p>In the context of SVM, a Hyperplane separates the target class(es) based on the given features.</p>
    </div>
    <div id="para-div">
      <p>In the following sections, we will develop an intuition on the working of the SVM classification model.</p>
      <p>Consider that we have a simple data set with one feature the cholesterol LDL levels and the outcome of whether someone has
        a Heart disease.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the plot of the LDL levels on a one-dimensional line, with the green points indicating no
        heart disease and the red points indicating heart disease:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="LDL on Line" src="./images/svm-01.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the illustration in Figure.1 above, we can see a clear separation between the two classes - Disease and No Disease.</p>
      <p>The question then is, where on the line, we can put the <span class="hi-yellow">Decision Boundary</span> to segregate the
        classes ???</p>
      <p>Intuitively, looking at the illustration in the Figure.1 above, the decision boundary should be mid-way between the region
        separating the green points from the red points. The space between the decision boundary and the class point (green or red)
        farthest from the decision boundary is referred to as the <span class="hi-yellow">Margin</span>.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the decision boundary hyperplane (golden line) between the green dotted margin line (that
        separates the green points) and the red dotted margin line (that separates the red points):</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Threshold Line" src="./images/svm-02.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>With this decision boundary in place, any new data points with LDL level below 135 is classfied as 'No Heart Disease' and
        anything above 135 is classified as having 'Heart Disease'.</p>
    </div>
    <div id="para-div">
      <p>Now, consider the same simple data set with two features - the cholesterol LDL and Triglyceride (TG) levels and the outcome
        of whether someone has a Heart disease.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the plot of the LDL vs TG levels on a two-dimensional plot, with the green points indicating
        no heart disease and the red point indicating heart disease:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="LDL vs TG" src="./images/svm-05.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the decision boundary hyperplane (golden line) between the green dotted margin line (that
        separates the green points) and the red dotted margin line (that separates the red points) in the two-dimensional space:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Threshold Line in 2D" src="./images/svm-06.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The dotted margin lines (green and red), that correspond to the two classes, touch the appropriate class vectors (the data
        points) that are farthest from the decision boundary. Those vectors (touching the class margin lines) are referred to as the
        <span class="hi-yellow">Support Vectors</span>.</p>
    </div>
    <div id="para-div">
      <p>With this decision boundary in place, any new data points that fall below the decision boundary (green region) classfied as
        'No Heart Disease' and anything above the decision boundary is classified as having 'Heart Disease'.</p>
    </div>
    <div id="para-div">
      <p>The classifier in the above two cases (one and two dimensional) is referred to as the <span class="hi-yellow">Maximal Margin
        Classifier</span>. In other words, we want a decision boundary that maximizes the margins between the classes.</p>
    </div>
    <div id="para-div">
      <p>Now that we have a geometrical intuition on the Maximal Margin Classifier, let us look at it from a mathematical point of
        view.</p>
    </div>
    <div id="para-div">
      <p>We will refer to the following illustration for the mathematical intuition:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Soft Margins" src="./images/svm-13.png" class="img-cls" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The equation for a line in a two dimensional space is: $y = m.x + c$, where $m$ is the slope and $c$ is the y-intercept.</p>
      <p>For the two features $x_1$ (x-axis) and $x_2$ (y-axis), the equation of the decision boundary (line) is: $x_2 = \beta_1.x_1
        + \beta_0$</p>
      <p>That is, $\beta_1.x_1 - x_2 + \beta_0 = 0$</p>
      <p>Or, $\begin{bmatrix} \beta_1 & -1\end{bmatrix}$.$\begin{bmatrix} x_1 \\ x_2\end{bmatrix} + \beta_0 = 0$</p>
      <p>Or, $\beta^T.X + \beta_0 = 0$ ..... $\color{red} (1)$</p>
    </div>
    <div id="para-div">
      <p>Consider the two points $A$ and $B$ on the decision boundary line.</p>
      <p>Then, using the equation (1) from above, we get the following:</p>
      <p>$\beta^T.A + \beta_0 = 0$ ..... $\color{red} (2)$</p>
      <p>$\beta^T.B + \beta_0 = 0$ ..... $\color{red} (3)$</p>
      <p>Subtracting (2) from (3), we get the following:</p>
      <p>$\beta^T.A + \beta_0 - \beta^T.B - \beta_0 = \beta^T.(A - B) = 0$ ..... $\color{red} (4)$</p>
      <p>From the equation (4) above, we know $\beta^T$ and $(A - B)$ are both vectors in the two dimensional space and from Linear
        Algebra (<a href="https://polarsparc.github.io/Mathematics/LinearAlgebra-1.html" target="_blank"><span class="bold">Part 1
        </span></a>), we know if the dot product of two vectors is <span class="underbold">ZERO</span>, then they have to be ORTHOGONAL.
        That is, the vector $\beta^T$ is orthogonal (or perpendicular) to the vector $(A - B)$.</p>
      <p>For any new points <span class="underbold">BELOW</span> the decision boundary line (represented using $-1$), we classify
        them as 'no heart disease' (green points). Then, the corresponding equation would be as follows:</p>
      <p>$\beta^T.X + \beta_0 \le -1$ ..... $\color{red} (5)$</p>
      <p>For any new points <span class="underbold">ABOVE</span> the decision boundary line (represented using $1$), we classify
        them as 'heart disease' (red points). Then, the corresponding equation would be as follows:</p>
      <p>$\beta^T.X + \beta_0 \ge 1$ ..... $\color{red} (6)$</p>
      <p>In other words, the target $y$ prediction is a $-1$ (green point - no heart disease) OR $1$ (red point - heart disease).</p>
      <p>This implies, equations $\color{red}(5)$ and $\color{red}(6)$ can be expressed using the following compact form:</p>
      <p>$\bbox[pink,2pt]{y.(\beta^T.X + \beta_0) \ge 1}$ ..... $\color{red} (7)$</p>
      <p>Let $C_1$ be the nearest green point to the decision boundary. Then, using equation (5), we get: $\beta^T.C_1 + \beta_0 =
        -1$ ..... $\color{red} (8)$</p>
      <p>Similarly, let $C_2$ be the nearest red point to the decision boundary. Then, using equation (6), we get: $\beta^T.C_2 +
        \beta_0 = 1$ ..... $\color{red} (9)$</p>
      <p>Subtracting (9) from (8), we get the following:</p>
      <p>$\beta^T.C_2 + \beta_0 - \beta^T.C_1 - \beta_0 = 1 - (-1)$</p>
      <p>That is, $\beta^T.(C_2 - C_1) = 2$</p>
      <p>To eliminate $\beta^T$ from the left-hand side, we make it a unit vector.</p>
      <p>That is, $\Large{\frac{\beta^T}{\lVert \beta^T \rVert}}$.$(C_2 - C_1) = \Large{\frac{2}{\lVert \beta^T \rVert}}$</p>
      <p>Given $\Large{\frac{\beta^T}{\lVert \beta^T \rVert}}$ is a unit vector of magnitude $1$, we can drop it from the left-hand
        side.</p>
      <p>That is, $(C_2 - C_1) = \Large{\frac{2}{\lVert \beta^T \rVert}}$ ..... $\color{red} (10)$</p>
      <p>The goal of Maximal Margin Classifier is to maximize the equation $\color{red}(10)$ from above.</p>
      <p>Conversely, the goal can also be to minimize $\Large{\frac{\lVert \beta^T \rVert}{2}}$ ..... $\color{red} (11)$</p>
      <p>Note that maximization of equation $\color{red}(10)$ (or minimization of equation $\color{red}(11)$) is bounded by the
        constraints of equation $\color{red}(7)$.</p>
    </div>
    <div id="para-div">
      <p>Now that we have a mathematical intuition, let us add a little twist. What if we encounter an outlier data point (with heart
        disease) as depicted in the following illustration:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Outlier Point" src="./images/svm-03.png" class="img-cls" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The Maximal Margin Classifier (with its hard margins) would classify the outlier as 'No Heart Disease', which is not true.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the same outlier point in the two-dimensional space:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Outlier in 2D" src="./images/svm-14.png" class="img-cls" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>What if we have classifier with <span class="hi-grey">SOFT</span> margins that allows for some misclassifications ??? That
        is exactly what the <span class="hi-yellow">Support Vector Classifier</span> is for.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the visual representation of the Support Vector Classifier with the misclassified outlier:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Soft Margins" src="./images/svm-04.png" class="img-cls" />
      <div class="img-cap">Figure.8</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the visual representation of the Support Vector Classifier with a misclassified outlier
        in the two-dimensional space:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Soft Margins in 2D" src="./images/svm-07.png" class="img-cls" />
      <div class="img-cap">Figure.9</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, consider another simple data set with one feature - the number of credit cards and the outcome of whether someone has
        a good credit score or not. If someone has less than $3$ or greater than $6$ credit cards, they have a bad credit score.</p>
    </div>
    <div id="para-div">
      <p>The following illustration shows the plot of the number of credit cards on a one-dimensional line, with the green points
        indicating good credit score and the red points indicating bad credit score:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Credit Cards on Line" src="./images/svm-08.png" class="img-cls" />
      <div class="img-cap">Figure.10</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the illustration in Figure.8 above, it is obvious that there is no easy way to classify the data points into good and
        bad credit scores.</p>
      <p>How do we classify the data points in this situations ???</p>
      <p>Before we proceed to tackle this situation, let us understand the concept of <span class="hi-yellow">Kernels</span>.</p>
      <p>A Kernel is a type of transformation that projects the features of a data set into a higher dimension. One of the commonly
        used kernels is a <span class="hi-yellow">Polynomial Kernel</span>. For example, given a feature $X_i$, a second degree
        Polynomial Kernel could be $X_i^2$.</p>
    </div>
    <div id="para-div">
      <p>For situations (as in the Figure.8 above) where there is no easy way to classify the data points, such a task falls in the
        realm of the <span class="hi-yellow">Support Vectore Machines</span>, which makes use of a kernel to transform the features
        into a higher dimension.</p>
      <p>For our usecase, we will use a $2^{nd}$ degree Polynomial Kernel to square the number of credit cards and use it as the
        second feature. In other words we are moving a one-dimensional space (line) to a two-dimensional space (plane). </p>
      <p>The following illustration depicts the visual representation of the Support Vector Machine, which uses the $2^{nd}$ degree
        Polynomial Kernel, to project the single feature into the two-dimensional space:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="SVM using Kernel" src="./images/svm-09.png" class="img-cls" />
      <div class="img-cap">Figure.11</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Now, we are in a much better position to segregate the two classes and the following illustration shows the hyperplane
        (golden line) between the green points and the red points in the two-dimensional space:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Hyperplane Line in 2D" src="./images/svm-10.png" class="img-cls" />
      <div class="img-cap">Figure.12</div>
    </div>
    <br/>
    <div id="para-div">
      <p>This in essence is the idea behind the Support Vector Machine model, that is, to find a higher dimension to separate the
        data samples into the different target classes.</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>For the hands-on demonstration of the SVM classification model, we will predict the event of death due to heart failure,
        using the dataset that contains the medical records of some heart patients.</p>
    </div>
    <div id="para-div">
      <p>The first step is to import all the necessary Python modules such as, matplotlib, pandas, and scikit-learn as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to load the heart failure dataset into a pandas dataframe, adjust some column names, and display the
        dataframe as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv'
heart_failure_df = pd.read_csv(url)
heart_failure_df.rename(columns={'DEATH_EVENT': 'death_event', 'creatinine_phosphokinase': 'cpk_enzyme', 'high_blood_pressure': 'high_bp'}, inplace=True)
heart_failure_df</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays few rows/columns from the heart failure dataframe:</p>
    </div>
    <br/>
    <div id="img-outer-div"> <img alt="Dataframe Rows/Columns" src="./images/svm-11.png" class="img-cls" />
      <div class="img-cap">Figure.13</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to split the heart failure dataframe into two parts - a training data set and a test data set. The training
        data set is used to train the SVM model and the test data set is used to evaluate the SVM model. In this use case, we split
        75% of the samples into the training dataset and remaining 25% into the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_train, X_test, y_train, y_test = train_test_split(heart_failure_df, heart_failure_df['death_event'], test_size=0.25, random_state=101)
X_train = X_train.drop('death_event', axis=1)
X_test = X_test.drop('death_event', axis=1)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to create an instance of the standardization scaler to scale the desired feature (or predictor) variables
        in both the training and test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>scaler = StandardScaler()
s_X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
s_X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns, index=X_test.index)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to initialize the SVM classification model class from scikit-learn and train the model using the scaled
        training data set as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = SVC(kernel='linear', C=100, random_state=101)
model.fit(s_X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following are a brief description of some of the hyperparameters used by the SVM classification model:</p>
      <ul id="blue-sqr-ul">
        <li><p><span class="hi-yellow">kernel</span> - the SVM kernel transformation to use. The commonly used options are
          <span class="hi-blue">linear</span> and <span class="hi-blue">poly</span> (for polynomial)</p></li>
        <li><p><span class="hi-yellow">C</span> - the regularization parameter that is inversely proportional to the number of
          misclassifications allowed by the SVM model. The lower the value, the higher the misclassifications. The higher the value,
          the lower the misclassifications</p></li>
      </ul>
    </div>
    <div id="para-div">
      <p>The next step is to use the trained model to predict the death_event using the scaled test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model.predict(s_X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display the accuracy score for the model performance as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>accuracy_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the accuracy score for the model performance:</p>
    </div>
    <div id="img-outer-div"> <img alt="Accuracy Score" src="./images/svm-12.png" class="img-cls" />
      <div class="img-cap">Figure.14</div>
    </div>
    <br/>
    <div id="para-div">
      <p>From the above, one can infer that the model seems to predict with good accuracy.</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this
        article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P22-SVM-Scikit.ipynb" target="_blank">
          <span class="bold">Support Vector Machines</span></a></p></li>
      </ul>
    </div>
    <br/>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
