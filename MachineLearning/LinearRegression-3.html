<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="application/xhtml+xml; charset=windows-1252" />
    <meta name="author" content="Bhaskar.S">
    <meta name="description" content="Machine Learning - Polynomial Regression using Scikit-Learn - Part 3">
    <meta name="subject" content="Machine Learning - Polynomial Regression using Scikit-Learn - Part 3">
    <meta name="keywords" content="python, machine_learning, regression, scikit-learn">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <title>Machine Learning - Polynomial Regression using Scikit-Learn - Part 3</title>
    <link href="../css/polarsparc-v2.4.css" type="text/css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <body>
    <br/>
    <table borber="0">
      <tr>
        <td valign="bottom"><span id="ps-home"></span></td>
        <td valign="bottom"><span id="home-a"><a id="home-a" href="https://polarsparc.github.io/">PolarSPARC</a></span></td>
      </tr>
    </table>
    <br/>
    <div id="title-div">
      <p>Machine Learning - Polynomial Regression using Scikit-Learn - Part 3</p>
    </div>
    <br />
    <table id="ad-table">
      <tbody>
        <tr>
          <td class="author-td">Bhaskar S</td>
          <td class="date-td">03/27/2022</td>
        </tr>
      </tbody>
    </table>
    <hr class="line-hr" />
    <br/>
    <div id="section-div">
      <p>Overview</p>
    </div>
    <div id="para-div">
      <p>In <a href="https://polarsparc.github.io/MachineLearning/LinearRegression-2.html" target="_blank"><span class="bold">Part 2</span></a>
        of this series, we demonstrated the Linear Regression models using <span class="bold">Scikit-Learn</span>. For the Simple
        Linear Regression of Displacement vs MPG, our model achieved an $R^2$ score of about $65\%$, which is not that great. Also,
        observe the scatter plot that depicts the relationship between displacement and mpg from the auto mpg test dataset as shown
        below:</p>
    </div>
    <div id="img-outer-div"> <img alt="Displacement vs MPG" src="./images/regression-13.png" class="img-cls" />
      <div class="img-cap">Figure.1</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice that the relationship follows a <span class="underbold">NON-LINEAR</span> (curved) path from top-left to the
        bottom-right.</p>
    </div>
    <div id="section-div">
      <p>Basics of Polynomials</p>
    </div>
    <div id="para-div">
      <p>The word <span class="hi-yellow">Polynomial</span> is in fact a composition of two terms - <span class="hi-grey">Poly</span>
        which means <span class="underbold">MANY</span>, and <span class="hi-grey">Nomial</span> which means <span class="underbold">
        TERMS</span>. In Algebra, a polynomial is function that consists of constants, coefficients, variables, and exponents.</p>
      <p>For any polynomial equation, the variable with the largest exponent determines the <span class="hi-yellow">Degree</span> of
        the polynomial. For example, $y = x^3 - 4x + 5$ is a third degree (cubic) polynomial equation, which has the constant $5$,
        the variable $x$ with coefficient $4$ and an exponent $x^3$ with coefficient $1$. This is a third-degree polynomial or also
        known as a <span class="hi-yellow">Cubic</span> polynomial.</p>
      <p>To get an intuition on the shapes of the various polynomial equations, let us look at some of the polynomial plots.</p>
    </div>
    <div id="para-div">
      <p>The following plot illustrates a 2nd-degree polynomial equation:</p>
    </div>
    <div id="img-outer-div"> <img alt="2nd Degree Polynomial" src="./images/regression-29.png" class="img-cls" />
      <div class="img-cap">Figure.2</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following plot illustrates a 3nd-degree polynomial equation:</p>
    </div>
    <div id="img-outer-div"> <img alt="3rd Degree Polynomial" src="./images/regression-30.png" class="img-cls" />
      <div class="img-cap">Figure.3</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following plot illustrates a 4th-degree polynomial equation:</p>
    </div>
    <div id="img-outer-div"> <img alt="4th Degree Polynomial" src="./images/regression-31.png" class="img-cls" />
      <div class="img-cap">Figure.4</div>
    </div>
    <br/>
    <div id="section-div">
      <p>Polynomial Regression</p>
    </div>
    <div id="para-div">
      <p>The purpose of the <span class="hi-yellow">Polynomial Regression</span> is to find a polynomial equation of the form $y =
        \beta_0 + \beta_1.x + \beta_2.x^2 + ... + \beta_n.x^n$ (of degree $n$) such that, it estimates the non-linear relationship
        between the dependent outcome (or target) variable $y$ and one or more independent feature (or predictor) variables $x_1,
        x_2, ..., x_n$.</p>
      <p>In other words, one can think of the Polynomial Regression as an extension of the Linear Regression, with the difference
        that it includes polynomial terms to deal with the non-linear relationship.</p>
    </div>
    <div id="para-div">
      <p>Assuming $m$ dependent output values and a $n$ degree polynomial, the following is the matrix representation:</p>
      <p>$\begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_m \end{bmatrix} = \begin{bmatrix} 1 & x_1 & x_1^2 & ... & x_1^n \\ 1 & x_2 & x_2^2
        & ... & x_2^n \\ ... & ... & ... & ... & ... \\ 1 & x_m & x_m^2 & ... & x_m^n \end{bmatrix} \begin{bmatrix} \beta_0 \\
        \beta_1 \\ ... \\ \beta_n \end{bmatrix}$</p>
      <p>Using the matrix notation, we arrive at $\hat{y} = X\beta$, where $X$ is a matrix. Notice that this form of mathematical
        representation is very similar to that of the Multiple Linear Regression.</p>
    </div>
    <div id="para-div">
      <p>The mathematical derivation follows the same set of steps as was the case with the Multiple Linear Regression we explored
        in <a href="https://polarsparc.github.io/MachineLearning/LinearRegression-1.html" target="_blank"><span class="bold">Part
          1</span></a> of this series, so we will skip it here.</p>
    </div>
    <div id="para-div">
      <p>For the demonstration of the polynomial regression, we will cover the model that estimates mpg using displacement.</p>
    </div>
    <div id="para-div">
      <p>The first step is to import all the necessary Python modules such as, matplotlib, pandas, and scikit-learn as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to load the auto mpg dataset into a pandas dataframe and assign the column names as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
auto_df = pd.read_csv(url, delim_whitespace=True)
auto_df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next set of steps, related to exploratory data analysis, help cleanse and prepare the data for regression. We will skip
        those, since we covered them in detail in <a href="https://polarsparc.github.io/MachineLearning/LinearRegression-2.html"
        target="_blank"><span class="bold">Part 2</span></a> of this series.</p>
    </div>
    <div id="para-div">
      <p>The next step is to create the training dataset and a test dataset with the desired feature (or predictor) variable. We
        create a 75% training dataset and a 25% test dataset with the feature variable displacement as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_train, X_test, y_train, y_test = train_test_split(auto_df[['displacement']], auto_df['mpg'], test_size=0.25, random_state=101)</pre>
      </div>
    </div>
    <div id="para-div">
      <p>The next step is to initialize the polynomial feature class from scikit-learn. Think of the polynomial feature object as a
        feature transformer that takes one-dimensional features to generate features of the higher dimension based on the specified
        degree of the polynomial. For our demonstration, we will initialize a quadratic polynomial feature transformer (degree=2)
        with the line-intercept (include_bias=True) as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>transformer = PolynomialFeatures(degree=2, include_bias=True)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to transform the training dataset to include $2^{nd}$ degree features as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_p_train = transformer.fit_transform(X_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to initialize the linear regression model class as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model = LinearRegression()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to train the model using the training dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model.fit(X_train, y_train)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display the values for the intercept and the polynomial coefficients as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>model.intercept_, model.coef_</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the values for the intercept and the polynomial coefficient from the model:</p>
    </div>
    <div id="img-outer-div"> <img alt="Display Intercept and Coefficients" src="./images/regression-21.png" class="img-cls" />
      <div class="img-cap">Figure.5</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice that the coefficients for the other polynomial terms are included in the output above.</p>
    </div>
    <div id="para-div">
      <p>The next step is to transform the test dataset to include $2^{nd}$ degree features as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>X_p_test = transformer.fit_transform(X_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to use the trained model to predict the mpg using the test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>y_predict = model.predict(X_p_test)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The next step is to display a scatter plot (along with the polynomial curve of best fit) that shows the visual relationship
        between the data from the columns displacement and mpg from the auto mpg test dataset as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>plt.scatter(X_test, y_test, color='dodgerblue')
sorted_Xy = sorted(zip(X_test.displacement, y_predict))
X_s, y_s = zip(*sorted_Xy)
plt.plot(X_s, y_s, color='red')
plt.xlabel('Displacement')
plt.ylabel('MPG')
plt.show()</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration shows the scatter plot (along with the polynomial curve of best fit) between displacement and mpg
        from the auto mpg test dataset:</p>
    </div>
    <div id="img-outer-div"> <img alt="Displacement vs MPG with Regression Line" src="./images/regression-22.png" class="img-cls" />
      <div class="img-cap">Figure.6</div>
    </div>
    <br/>
    <div id="para-div">
      <p>The final step is to display the $R^2$ value for the model as shown below:</p>
    </div>
    <br/>
    <div id="src-outer-div-1">
      <div class="src-body-1">
<pre>r2_score(y_test, y_predict)</pre>
      </div>
    </div>
    <br/>
    <div id="para-div">
      <p>The following illustration displays the $R^2$ value for the model:</p>
    </div>
    <div id="img-outer-div"> <img alt="Display R-Squared" src="./images/regression-23.png" class="img-cls" />
      <div class="img-cap">Figure.7</div>
    </div>
    <br/>
    <div id="para-div">
      <p>Notice our model has achieved an $R^2$ score of about $70\%$ which is a little better.</p>
    </div>
    <div id="section-div">
      <p>Overfitting</p>
    </div>
    <div id="para-div">
      <p>One of the challenges with the polynomial regression, especially with the higher degree polynomials, is that the model tends
        to learn and fit every data point from the training set, including outliers. In other words, the regression model starts to
        include noise into model, rather than generalizing the genuine underlying relationship between the dependent and independent
        variables. This behavior of the polynomial regression model is known as <span class="hi-yellow">Overfitting</span>.</p>
      <p>The result of overfitting is as though the regression model was custom made for the given training dataset, which reduces
        the models ability to be generalized for future predictions.</p>
    </div>
    <div id="section-div">
      <p>Hands-on Demo</p>
    </div>
    <div id="para-div">
      <p>The following is the link to the <span class="bold">Jupyter Notebook</span> that provides an hands-on demo for this article:</p>
      <ul id="blue-sqr-ul">
        <li><p><a href="https://github.com/bhaskars-repo/MachineLearning/blob/main/P3-1-Poly-Auto-Scikit.ipynb" target="_blank">
          <span class="bold">Polynomial Regression</span></a></p></li>
      </ul>
    </div>
    <br/>
    <div id="section-div">
      <p>References</p>
    </div>
    <div id="para-div">
      <p><a href="https://polarsparc.github.io/MachineLearning/LinearRegression-2.html" target="_blank"><span class="bold">Machine Learning - Linear Regression using Scikit-Learn - Part 2</span></a></p>
      <p><a href="https://polarsparc.github.io/MachineLearning/LinearRegression-1.html" target="_blank"><span class="bold">Machine Learning - Linear Regression - Part 1</span></a></p>
    </div>
    <hr class="line-hr" />
    <div>
      <a id="footer-a" href="https://polarsparc.github.io/">&copy;&nbsp;PolarSPARC</a>
    </div>
  </body>
</html>
